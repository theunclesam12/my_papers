@article{abdollahiDeepLearningApproaches2020,
  title = {Deep {{Learning Approaches Applied}} to {{Remote Sensing Datasets}} for {{Road Extraction}}: {{A State-Of-The-Art Review}}},
  shorttitle = {Deep {{Learning Approaches Applied}} to {{Remote Sensing Datasets}} for {{Road Extraction}}},
  author = {Abdollahi, Abolfazl and Pradhan, Biswajeet and Shukla, Nagesh and Chakraborty, Subrata and Alamri, Abdullah},
  year = 2020,
  month = jan,
  journal = {Remote Sensing},
  volume = {12},
  number = {9},
  pages = {1444},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2072-4292},
  doi = {10.3390/rs12091444},
  urldate = {2025-01-03},
  abstract = {One of the most challenging research subjects in remote sensing is feature extraction, such as road features, from remote sensing images. Such an extraction influences multiple scenes, including map updating, traffic management, emergency tasks, road monitoring, and others. Therefore, a systematic review of deep learning techniques applied to common remote sensing benchmarks for road extraction is conducted in this study. The research is conducted based on four main types of deep learning methods, namely, the GANs model, deconvolutional networks, FCNs, and patch-based CNNs models. We also compare these various deep learning models applied to remote sensing datasets to show which method performs well in extracting road parts from high-resolution remote sensing images. Moreover, we describe future research directions and research gaps. Results indicate that the largest reported performance record is related to the deconvolutional nets applied to remote sensing images, and the F1 score metric of the generative adversarial network model, DenseNet method, and FCN-32 applied to UAV and Google Earth images are high: 96.08\%, 95.72\%, and 94.59\%, respectively.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {/unread,common benchmarks,deep learning,machine learning,remote sensing,road extraction},
  file = {C:\Users\theun\Zotero\storage\W5R5PI8A\Abdollahi et al. - 2020 - Deep Learning Approaches Applied to Remote Sensing Datasets for Road Extraction A State-Of-The-Art.pdf}
}

@misc{abrarAnomalyBehaviorAnalysis2024,
  title = {An {{Anomaly Behavior Analysis Framework}} for {{Securing Autonomous Vehicle Perception}}},
  author = {Abrar, Murad Mehrab and Hariri, Salim},
  year = 2024,
  month = apr,
  number = {arXiv:2310.05041},
  eprint = {2310.05041},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.05041},
  urldate = {2024-12-04},
  abstract = {As a rapidly growing cyber-physical platform, Autonomous Vehicles (AVs) are encountering more security challenges as their capabilities continue to expand. In recent years, adversaries are actively targeting the perception sensors of autonomous vehicles with sophisticated attacks that are not easily detected by the vehicles' control systems. This paper proposes an Anomaly Behavior Analysis framework to detect perception system anomalies and sensor attacks against an autonomous vehicle. The framework relies on temporal features extracted from a physics-based autonomous vehicle behavior model to capture the normal behavior of vehicular perception in autonomous driving. By employing a combination of model-based techniques and machine learning algorithms, the proposed framework distinguishes between normal and abnormal vehicular perception behavior. As part of our experimental evaluation of the framework, a depth camera blinding attack experiment was performed on an autonomous vehicle testbed and an extensive dataset was generated. The effectiveness of the proposed framework has been validated using this real-world data and the dataset has been released for public access. To our knowledge, this dataset is the first of its kind and will serve as a valuable resource for the research community in evaluating their intrusion detection techniques effectively.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control},
  file = {C:\Users\theun\Zotero\storage\UBTGTISQ\Abrar and Hariri - 2024 - An Anomaly Behavior Analysis Framework for Securing Autonomous Vehicle Perception.pdf}
}

@inproceedings{afifiLearningMultiScalePhoto2021,
  title = {Learning {{Multi-Scale Photo Exposure Correction}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Afifi, Mahmoud and Derpanis, Konstantinos G. and Ommer, Bjorn and Brown, Michael S.},
  year = 2021,
  pages = {9157--9167},
  urldate = {2025-03-10},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\296TM69Y\Afifi et al. - 2021 - Learning Multi-Scale Photo Exposure Correction.pdf}
}

@article{alabaDeepLearningBasedImage2023,
  title = {Deep {{Learning-Based Image}} 3-{{D Object Detection}} for {{Autonomous Driving}}: {{Review}}},
  shorttitle = {Deep {{Learning-Based Image}} 3-{{D Object Detection}} for {{Autonomous Driving}}},
  author = {Alaba, Simegnew Yihunie and Ball, John E.},
  year = 2023,
  month = feb,
  journal = {IEEE Sensors Journal},
  volume = {23},
  number = {4},
  pages = {3378--3394},
  issn = {1558-1748},
  doi = {10.1109/JSEN.2023.3235830},
  urldate = {2025-03-19},
  abstract = {An accurate and robust perception system is key to understanding the driving environment of autonomous driving and robots. Autonomous driving needs 3-D information about objects, including the object's location and pose, to understand the driving environment clearly. A camera sensor is widely used in autonomous driving because of its richness in color and texture, and low price. The major problem with the camera is the lack of 3-D information, which is necessary to understand the 3-D driving environment. In addition, the object's scale change and occlusion make 3-D object detection more challenging. Many deep learning-based methods, such as depth estimation, have been developed to solve the lack of 3-D information. This survey presents the image 3-D object detection 3-D bounding box encoding techniques and evaluation metrics. The image-based methods are categorized based on the technique used to estimate an image's depth information, and insights are added to each method. Then, state-of-the-art (SOTA) monocular and stereo camera-based methods are summarized. We also compare the performance of the selected 3-D object detection models and present challenges and future directions in 3-D object detection.},
  keywords = {/unread,3-D object detection,autonomous driving,Autonomous vehicles,camera,deep learning (DL),Encoding,Feature extraction,Object detection,Proposals,Sensors,Three-dimensional displays},
  file = {C\:\\Users\\theun\\Zotero\\storage\\79VM4MNB\\Alaba and Ball - 2023 - Deep Learning-Based Image 3-D Object Detection for Autonomous Driving Review.pdf;C\:\\Users\\theun\\Zotero\\storage\\PCWFNPPL\\10017184.html}
}

@misc{AlternativeTechniqueComputation,
  title = {An Alternative Technique for the Computation of the Designator in the Retinex Theory of Color Vision.},
  doi = {10.1073/pnas.83.10.3078},
  urldate = {2025-03-06},
  howpublished = {https://www.pnas.org/doi/epdf/10.1073/pnas.83.10.3078},
  langid = {english},
  file = {C\:\\Users\\theun\\Zotero\\storage\\JQN9F2XQ\\An alternative technique for the computation of the designator in the retinex theory of color vision.pdf;C\:\\Users\\theun\\Zotero\\storage\\YL8TVV8R\\pnas.83.10.html}
}

@article{ashrafCatastrophicFactorsInvolved2019,
  title = {Catastrophic Factors Involved in Road Accidents: {{Underlying}} Causes and Descriptive Analysis},
  shorttitle = {Catastrophic Factors Involved in Road Accidents},
  author = {Ashraf, Imran and Hur, Soojung and Shafiq, Muhammad and Park, Yongwan},
  year = 2019,
  month = oct,
  journal = {PLOS ONE},
  volume = {14},
  number = {10},
  pages = {e0223473},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0223473},
  urldate = {2025-01-18},
  abstract = {South Korea is ranked as 4th among 34 nations of the Organization for Economic Cooperation and Development with 102 deaths in road accidents per one million population. This paper aims to investigate the factors associated with road accidents in South Korea. The rainfall data of the Korea Meteorological Administration and road accidents data of Traffic Accident Analysis System of Korea Road Traffic Authority is analyzed for this purpose. In this connection, multivariate regression analysis and ratio analysis with the descriptive analysis are performed to uncover the catastrophic factors involved. In turn, the results reveal that traffic volume is the leading factor in road accidents. The limited road extension of 1.47\% compared to the 4.14\% per annum growth of the vehicles is resulting in road accidents at such a large scale. The increasing proportion of passenger cars accelerate road accidents as well. 56\% of accidents occur by the infringement of safety driving violations. The drivers with higher driving experience tend to have a higher accident ratio. The collected data is analyzed in terms of gender, driver experience, type of violations and accidents as well as the associated time of the accidents when they happen. The results indicate that 36.29\% and 53.01\% of accidents happen by male drivers in the day and night time, respectively. 29.15\% of crashes happen due to safety infringement and violations of 41 to 60 years old drivers. The results demonstrate that population density is associated with the accidents frequency and lower density results in an increased number of accidents. The necessity of the state-of-the-art regulations to govern the urban road traffic is beyond dispute, and it becomes even more crucial for citizens' relief since in our daily lives road accidents are getting more diverse.},
  langid = {english},
  keywords = {Alcohol consumption,Korea,Population density,Regression analysis,Road traffic collisions,Roads,South Korea,Traffic safety},
  file = {C:\Users\theun\Zotero\storage\9388MFXW\Ashraf et al. - 2019 - Catastrophic factors involved in road accidents Underlying causes and descriptive analysis.pdf}
}

@misc{azadLossFunctionsEra2023,
  title = {Loss {{Functions}} in the {{Era}} of {{Semantic Segmentation}}: {{A Survey}} and {{Outlook}}},
  shorttitle = {Loss {{Functions}} in the {{Era}} of {{Semantic Segmentation}}},
  author = {Azad, Reza and Heidary, Moein and Yilmaz, Kadir and H{\"u}ttemann, Michael and Karimijafarbigloo, Sanaz and Wu, Yuli and Schmeink, Anke and Merhof, Dorit},
  year = 2023,
  month = dec,
  number = {arXiv:2312.05391},
  eprint = {2312.05391},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.05391},
  urldate = {2025-10-17},
  abstract = {Semantic image segmentation, the process of classifying each pixel in an image into a particular class, plays an important role in many visual understanding systems. As the predominant criterion for evaluating the performance of statistical models, loss functions are crucial for shaping the development of deep learning-based segmentation algorithms and improving their overall performance. To aid researchers in identifying the optimal loss function for their particular application, this survey provides a comprehensive and unified review of \$25\$ loss functions utilized in image segmentation. We provide a novel taxonomy and thorough review of how these loss functions are customized and leveraged in image segmentation, with a systematic categorization emphasizing their significant features and applications. Furthermore, to evaluate the efficacy of these methods in real-world scenarios, we propose unbiased evaluations of some distinct and renowned loss functions on established medical and natural image datasets. We conclude this review by identifying current challenges and unveiling future research opportunities. Finally, we have compiled the reviewed studies that have open-source implementations on our GitHub page.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\2NJP6KEA\\Azad et al. - 2023 - Loss Functions in the Era of Semantic Segmentation A Survey and Outlook.pdf;C\:\\Users\\theun\\Zotero\\storage\\JJPVWD27\\2312.html}
}

@article{badrinarayananSegNetDeepConvolutional2017,
  title = {{{SegNet}}: {{A Deep Convolutional Encoder-Decoder Architecture}} for {{Image Segmentation}}},
  shorttitle = {{{SegNet}}},
  author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  year = 2017,
  month = dec,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {39},
  number = {12},
  pages = {2481--2495},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2016.2644615},
  urldate = {2025-06-09},
  abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
  keywords = {Computer architecture,Convolutional codes,decoder,Decoding,Deep convolutional neural networks,encoder,Image segmentation,indoor scenes,Neural networks,pooling,road scenes,semantic pixel-wise segmentation,Semantics,Training,upsampling},
  file = {C:\Users\theun\Zotero\storage\ZC7LCV6D\Badrinarayanan et al. - 2017 - SegNet A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation.pdf}
}

@inproceedings{bahetiEffUNetNovelArchitecture2020,
  title = {Eff-{{UNet}}: {{A Novel Architecture}} for {{Semantic Segmentation}} in {{Unstructured Environment}}},
  shorttitle = {Eff-{{UNet}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Baheti, Bhakti and Innani, Shubham and Gajre, Suhas and Talbar, Sanjay},
  year = 2020,
  month = jun,
  pages = {1473--1481},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPRW50498.2020.00187},
  urldate = {2025-01-03},
  abstract = {Since the last few decades, the number of road causalities has seen continuous growth across the globe. Nowadays intelligent transportation systems are being developed to enable safe and relaxed driving and scene understanding of the surrounding environment is an integral part of it. While several approaches are being developed for semantic scene segmentation based on deep learning and Convolutional Neural Network (CNN), these approaches assume well structured road infrastructure and driving environment. We focus our work on recent India Driving Lite Dataset (IDD), which contains data from unstructured driving environment and was hosted as an online challenge in NCVPRIPG 2019. We propose a novel architecture named as Eff-UNet which combines the effectiveness of compound scaled EfficientNet as the encoder for feature extraction with UNet decoder for reconstructing the fine-grained segmentation map. High level feature information as well as low level spatial information useful for precise segmentation are combined. The proposed architecture achieved 0.7376 and 0.6276 mean Intersection over Union (mIoU) on validation and test dataset respectively and won first prize in IDD lite segmentation challenge outperforming other approaches in the literature.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-7281-9360-1},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\INWRST6X\Baheti et al. - 2020 - Eff-UNet A Novel Architecture for Semantic Segmentation in Unstructured Environment.pdf}
}

@inproceedings{baiBasedImprovedZeroDCE2025,
  title = {Based on an Improved {{Zero-DCE}}++ Low-Light Enhanced Object Detection Algorithm},
  booktitle = {Proc {{SPIE Int Soc Opt Eng}}},
  author = {Bai, X. and Gao, H. and Zhao, Z. and Yan, Z.},
  editor = {{Hu L.}},
  year = 2025,
  volume = {13555},
  publisher = {SPIE},
  doi = {10.1117/12.3064933},
  abstract = {Low-light image quality often suffers from noise, color distortion, and reduced contrast, challenging accurate object detection. Traditional enhancement methods can lead to over-processing and semantic information loss. This study presents an enhanced low-light image detection model based on an improved Zero-DCE++ algorithm, integrated with YOLOv8 for high-speed, accurate detection. Key improvements include the CBAM attention mechanism to enhance feature extraction in low-light areas, DCNv2 for capturing complex object details, and a Local Region Brightness Curve Enhancement Module to prevent overexposure in well-lit areas. The proposed model was tested on the Night Drone dataset, where it achieved superior performance in image quality and detection accuracy compared to state-of-the-art methods. This approach demonstrates significant potential for applications requiring reliable object detection in challenging low-light conditions. \copyright{} 2025 SPIE.},
  isbn = {0277786X (ISSN); 978-151068917-6 (ISBN)},
  langid = {english},
  keywords = {CBAM,Color distortions,DCNv2,Image detection,Image enhancement,Low light,low-light image detection,Low-light image detection,Low-light images,Object detection algorithms,Objects detection,Semantic Segmentation,Semantics Information},
  file = {C:\Users\theun\Zotero\storage\U9BDD7S9\Bai et al. - 2025 - Based on an improved Zero-DCE++ low-light enhanced object detection algorithm.pdf}
}

@article{banicLightRandomSprays2013,
  title = {Light {{Random Sprays Retinex}}: {{Exploiting}} the {{Noisy Illumination Estimation}}},
  shorttitle = {Light {{Random Sprays Retinex}}},
  author = {Bani{\'c}, Nikola and Lon{\v c}ari{\'c}, Sven},
  year = 2013,
  month = dec,
  journal = {IEEE Signal Processing Letters},
  volume = {20},
  number = {12},
  pages = {1240--1243},
  issn = {1558-2361},
  doi = {10.1109/LSP.2013.2285960},
  urldate = {2025-05-29},
  abstract = {In this letter, Light Random Sprays Retinex (LRSR), an improvement of the Random Sprays Retinex (RSR) algorithm is proposed. RSR is a white balancing algorithm for achieving local color constancy and image enhancement by using random sprays of the same size. The main problem of the original RSR is that the lower the number and size of the sprays, the greater the noise in the resulting image, which means that the number and size of sprays have to be relatively high in order to reduce the noise leading to a higher computation cost. The proposed improved algorithm is based on a new method to remove the noise in the resulting image thereby allowing only one spray of a smaller size to be used resulting in lower computation cost. By using interpolation the computation cost is reduced even further without a noticeable perceptual difference. The improvement is tested on a public database and is shown to outperform the original RSR in image quality and computation cost. The source code is available at http://www.fer.unizg.hr/ipg/resources/color\_constancy/.},
  keywords = {Algorithm design and analysis,Color constancy,Image color analysis,image enhancement,Image enhancement,Image quality,interpolation,Interpolation,Noise,noise removal,random sprays retinex,retinex,white balance},
  file = {C:\Users\theun\Zotero\storage\UCSJ4FPU\Banić and Lončarić - 2013 - Light Random Sprays Retinex Exploiting the Noisy Illumination Estimation.pdf}
}

@article{banicSmartLightRandom2015,
  title = {Smart Light Random Memory Sprays {{Retinex}}: A Fast {{Retinex}} Implementation for High-Quality Brightness Adjustment and Color Correction},
  shorttitle = {Smart Light Random Memory Sprays {{Retinex}}},
  author = {Bani{\'c}, Nikola and Lon{\v c}ari{\'c}, Sven},
  year = 2015,
  month = nov,
  journal = {Journal of the Optical Society of America A},
  volume = {32},
  number = {11},
  pages = {2136},
  issn = {1084-7529, 1520-8532},
  doi = {10.1364/JOSAA.32.002136},
  urldate = {2025-05-29},
  copyright = {https://doi.org/10.1364/OA\_License\_v1\#VOR},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\R2G26BHD\Banić and Lončarić - 2015 - Smart light random memory sprays Retinex a fast Retinex implementation for high-quality brightness.pdf}
}

@misc{barbosaCameraRadarPerceptionAutonomous2023,
  title = {Camera-{{Radar Perception}} for {{Autonomous Vehicles}} and {{ADAS}}: {{Concepts}}, {{Datasets}} and {{Metrics}}},
  shorttitle = {Camera-{{Radar Perception}} for {{Autonomous Vehicles}} and {{ADAS}}},
  author = {Barbosa, Felipe Manfio and Os{\'o}rio, Fernando Santos},
  year = 2023,
  month = mar,
  number = {arXiv:2303.04302},
  eprint = {2303.04302},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.04302},
  urldate = {2025-01-02},
  abstract = {One of the main paths towards the reduction of traffic accidents is the increase in vehicle safety through driver assistance systems or even systems with a complete level of autonomy. In these types of systems, tasks such as obstacle detection and segmentation, especially the Deep Learning-based ones, play a fundamental role in scene understanding for correct and safe navigation. Besides that, the wide variety of sensors in vehicles nowadays provides a rich set of alternatives for improvement in the robustness of perception in challenging situations, such as navigation under lighting and weather adverse conditions. Despite the current focus given to the subject, the literature lacks studies on radar-based and radar-camera fusion-based perception. Hence, this work aims to carry out a study on the current scenario of camera and radar-based perception for ADAS and autonomous vehicles. Concepts and characteristics related to both sensors, as well as to their fusion, are presented. Additionally, we give an overview of the Deep Learning-based detection and segmentation tasks, and the main datasets, metrics, challenges, and open questions in vehicle perception.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C\:\\Users\\theun\\Zotero\\storage\\VKV5YFXN\\Barbosa and Osório - 2023 - Camera-Radar Perception for Autonomous Vehicles and ADAS Concepts, Datasets and Metrics.pdf;C\:\\Users\\theun\\Zotero\\storage\\3BD9QEH8\\2303.html}
}

@article{bertalmioIssuesRetinexTheory2009,
  title = {Issues {{About Retinex Theory}} and {{Contrast Enhancement}}},
  author = {Bertalm{\'i}o, Marcelo and Caselles, Vicent and Provenzi, Edoardo},
  year = 2009,
  month = jun,
  journal = {International Journal of Computer Vision},
  volume = {83},
  number = {1},
  pages = {101--119},
  issn = {1573-1405},
  doi = {10.1007/s11263-009-0221-5},
  urldate = {2025-05-29},
  abstract = {We present an interpretation of Land's Retinex theory that we show to be consistent with the original formulation. The proposed model relies on the computation of the expectation value of a suitable random variable weighted with a kernel function, thus the name Kernel-Based Retinex (KBR) for the corresponding algorithm. KBR shares the same intrinsic characteristics of the original Retinex: it can reduce the effect of a color cast and enhance details in low-key images but, since it can only increase pixel intensities, it is not able to enhance over-exposed pictures. Comparing the analytical structure of KBR with that of a recent variational model of color image enhancement, we are able to perform an analysis of the action of KBR on contrast, showing the need to anti-symmetrize its equation in order to produce a two-sided contrast modification, able to enhance both under and over-exposed pictures. The anti-symmetrized KBR equations show clear correspondences with other existing color correction models, in particular ACE, whose relationship with Retinex has always been difficult to clarify. Finally, from an image processing point of view, we mention that both KBR and its antisymmetric version are free from the chromatic noise due to the use of paths in the original Retinex implementation and that they can be suitably approximated in order to reduce their computational complexity from \$\textbackslash mathcal\textbraceleft O\textbraceright (N\textasciicircum\textbraceleft 2\textbraceright )\$to \$\textbackslash mathcal\textbraceleft O\textbraceright (N\textbackslash log N)\$, being N the number of input pixels.},
  langid = {english},
  keywords = {Color image processing,Color Perception,Contrast enhancement,Film Theory,Image Processing,Matrix Theory,Retinex,Technical Languages,Valence-Bond Theory,Variational methods},
  file = {C:\Users\theun\Zotero\storage\XT7PMBKB\Bertalmío et al. - 2009 - Issues About Retinex Theory and Contrast Enhancement.pdf}
}

@misc{bhartiAdvancingAutonomousDriving2024,
  title = {Advancing {{Autonomous Driving Perception}}: {{Analysis}} of {{Sensor Fusion}} and {{Computer Vision Techniques}}},
  shorttitle = {Advancing {{Autonomous Driving Perception}}},
  author = {Bharti, Urvishkumar and Shahapur, Vikram},
  year = 2024,
  month = nov,
  number = {arXiv:2411.10535},
  eprint = {2411.10535},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.10535},
  urldate = {2024-12-04},
  abstract = {In autonomous driving, perception systems are pivotal as they interpret sensory data to understand the environment, which is essential for decision-making and planning. Ensuring the safety of these perception systems is fundamental for achieving high-level autonomy, allowing us to confidently delegate driving and monitoring tasks to machines. This report aims to enhance the safety of perception systems by examining and summarizing the latest advancements in vision based systems, and metrics for perception tasks in autonomous driving. The report also underscores significant achievements and recognized challenges faced by current research in this field. This project focuses on enhancing the understanding and navigation capabilities of self-driving robots through depth based perception and computer vision techniques. Specifically, it explores how we can perform better navigation into unknown map 2D map with existing detection and tracking algorithms and on top of that how depth based perception can enhance the navigation capabilities of the wheel based bots to improve autonomous driving perception.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C:\Users\theun\Zotero\storage\5JK9GDDC\Bharti and Shahapur - 2024 - Advancing Autonomous Driving Perception Analysis of Sensor Fusion and Computer Vision Techniques.pdf}
}

@misc{bhatZoeDepthZeroshotTransfer2023,
  title = {{{ZoeDepth}}: {{Zero-shot Transfer}} by {{Combining Relative}} and {{Metric Depth}}},
  shorttitle = {{{ZoeDepth}}},
  author = {Bhat, Shariq Farooq and Birkl, Reiner and Wofk, Diana and Wonka, Peter and M{\"u}ller, Matthias},
  year = 2023,
  month = feb,
  number = {arXiv:2302.12288},
  eprint = {2302.12288},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.12288},
  urldate = {2025-07-03},
  abstract = {This paper tackles the problem of depth estimation from a single image. Existing work either focuses on generalization performance disregarding metric scale, i.e. relative depth estimation, or state-of-the-art results on specific datasets, i.e. metric depth estimation. We propose the first approach that combines both worlds, leading to a model with excellent generalization performance while maintaining metric scale. Our flagship model, ZoeD-M12-NK, is pre-trained on 12 datasets using relative depth and fine-tuned on two datasets using metric depth. We use a lightweight head with a novel bin adjustment design called metric bins module for each domain. During inference, each input image is automatically routed to the appropriate head using a latent classifier. Our framework admits multiple configurations depending on the datasets used for relative depth pre-training and metric fine-tuning. Without pre-training, we can already significantly improve the state of the art (SOTA) on the NYU Depth v2 indoor dataset. Pre-training on twelve datasets and fine-tuning on the NYU Depth v2 indoor dataset, we can further improve SOTA for a total of 21\% in terms of relative absolute error (REL). Finally, ZoeD-M12-NK is the first model that can jointly train on multiple datasets (NYU Depth v2 and KITTI) without a significant drop in performance and achieve unprecedented zero-shot generalization performance to eight unseen datasets from both indoor and outdoor domains. The code and pre-trained models are publicly available at https://github.com/isl-org/ZoeDepth .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\VE34DN8J\\Bhat et al. - 2023 - ZoeDepth Zero-shot Transfer by Combining Relative and Metric Depth.pdf;C\:\\Users\\theun\\Zotero\\storage\\GIWGV4IX\\2302.html}
}

@misc{bhoiMonocularDepthEstimation2019,
  title = {Monocular {{Depth Estimation}}: {{A Survey}}},
  shorttitle = {Monocular {{Depth Estimation}}},
  author = {Bhoi, Amlaan},
  year = 2019,
  month = jan,
  number = {arXiv:1901.09402},
  eprint = {1901.09402},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1901.09402},
  urldate = {2025-06-29},
  abstract = {Monocular depth estimation is often described as an ill-posed and inherently ambiguous problem. Estimating depth from 2D images is a crucial step in scene reconstruction, 3Dobject recognition, segmentation, and detection. The problem can be framed as: given a single RGB image as input, predict a dense depth map for each pixel. This problem is worsened by the fact that most scenes have large texture and structural variations, object occlusions, and rich geometric detailing. All these factors contribute to difficulty in accurate depth estimation. In this paper, we review five papers that attempt to solve the depth estimation problem with various techniques including supervised, weakly-supervised, and unsupervised learning techniques. We then compare these papers and understand the improvements made over one another. Finally, we explore potential improvements that can aid to better solve this problem.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\SK4TNTAK\\Bhoi - 2019 - Monocular Depth Estimation A Survey.pdf;C\:\\Users\\theun\\Zotero\\storage\\JMFRFKFI\\1901.html}
}

@article{bilodeauThermalVisibleRegistration2014,
  title = {Thermal--Visible Registration of Human Silhouettes: {{A}} Similarity Measure Performance Evaluation},
  shorttitle = {Thermal--Visible Registration of Human Silhouettes},
  author = {Bilodeau, Guillaume-Alexandre and Torabi, Atousa and {St-Charles}, Pierre-Luc and Riahi, Dorra},
  year = 2014,
  month = may,
  journal = {Infrared Physics \& Technology},
  volume = {64},
  pages = {79--86},
  issn = {1350-4495},
  doi = {10.1016/j.infrared.2014.02.005},
  urldate = {2025-05-15},
  abstract = {When dealing with the registration of information from different image sources, the de facto similarity measure used is Mutual Information (MI). Although MI gives good performance in many image registration applications, recent works in thermal--visible registration have shown that other similarity measures can give results that are as accurate, if not more than MI. Furthermore, some of these measures also have the advantage of being calculated independently from each image to register, which allows them to be integrated more easily in energy minimization frameworks. In this article, we investigate the accuracy of similarity measures for thermal--visible image registration of human silhouettes, including MI, Sum of Squared Differences (SSD), Normalized Cross-Correlation (NCC), Histograms of Oriented Gradients (HOG), Local Self-Similarity (LSS), Scale-Invariant Feature Transform (SIFT), Speeded-Up Robust Features (SURF), Census, Fast Retina Keypoint (FREAK), and Binary Robust Independent Elementary Feature (BRIEF). We tested the various similarity measures in dense stereo matching tasks over 25,000 windows to have statistically significant results. To do so, we created a new dataset in which one to five humans are walking in a scene in various depth planes. Results show that even if MI is a very strong performer, particularly for large regions of interest (ROI), LSS gives better accuracies when ROI are small or segmented into small fragments because of its ability to capture shape. The other tested similarity measures did not give consistently accurate results.},
  keywords = {/unread,Dense stereo matching,Multispectral imagery,Similarity measures,Thermal camera,Thermal-visible registration,Visible camera},
  file = {C\:\\Users\\theun\\Zotero\\storage\\7JIGC9EQ\\Bilodeau et al. - 2014 - Thermal–visible registration of human silhouettes A similarity measure performance evaluation.pdf;C\:\\Users\\theun\\Zotero\\storage\\TXCCP29E\\S1350449514000334.html}
}

@article{biNighttimeDrivingSceneSegmentation2024,
  title = {A {{Nighttime Driving-Scene Segmentation Method Based}} on {{Light-Enhanced Network}}},
  author = {Bi, Lihua and Zhang, Wenjiao and Zhang, Xiangfei and Li, Canlin},
  year = 2024,
  month = nov,
  journal = {World Electric Vehicle Journal},
  volume = {15},
  number = {11},
  pages = {490},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2032-6653},
  doi = {10.3390/wevj15110490},
  urldate = {2025-06-18},
  abstract = {To solve the semantic segmentation problem of night driving-scene images, which often have low brightness, low contrast, and uneven illumination, a nighttime driving-scene segmentation method based on a light-enhanced network was proposed. Firstly, we designed a light enhancement network, which comprises two parts: a color correction module and a parameter predictor. The color correction module mitigates the impact of illumination variations on the segmentation network by adjusting the color information of the image. Meanwhile, the parameter predictor accurately predicts the parameters of the image filter through the analysis of global content, including factors such as brightness, contrast, hue, and exposure level, thereby effectively enhancing the image quality. Subsequently, the output of the light enhancement network is input into the segmentation network to obtain the final segmentation prediction. Experimental results show that the proposed method achieves mean Intersection over Union (mIoU) values of 59.4\% on the Dark Zurich-test dataset, outperforming other segmentation algorithms for nighttime driving-scenes.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Colour corrections,Daylighting,deep learning,Deep learning,image enhancement,Image enhancement,Light enhancement,Luminance,Night driving,Night time driving,nighttime driving-scene,Nighttime driving-scene,Scene image,Scene segmentation,Segmentation methods,semantic segmentation,Semantic segmentation,Semantic Segmentation},
  file = {C:\Users\theun\Zotero\storage\MU74JLAD\Bi et al. - 2024 - A Nighttime Driving-Scene Segmentation Method Based on Light-Enhanced Network.pdf}
}

@misc{bochkovskiiDepthProSharp2025,
  title = {Depth {{Pro}}: {{Sharp Monocular Metric Depth}} in {{Less Than}} a {{Second}}},
  shorttitle = {Depth {{Pro}}},
  author = {Bochkovskii, Aleksei and Delaunoy, Ama{\"e}l and Germain, Hugo and Santos, Marcel and Zhou, Yichao and Richter, Stephan R. and Koltun, Vladlen},
  year = 2025,
  month = apr,
  number = {arXiv:2410.02073},
  eprint = {2410.02073},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.02073},
  urldate = {2025-07-04},
  abstract = {We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. Extensive experiments analyze specific design choices and demonstrate that Depth Pro outperforms prior work along multiple dimensions. We release code and weights at https://github.com/apple/ml-depth-pro},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\theun\\Zotero\\storage\\NHYAZ5F5\\Bochkovskii et al. - 2025 - Depth Pro Sharp Monocular Metric Depth in Less Than a Second.pdf;C\:\\Users\\theun\\Zotero\\storage\\QYX8VGKE\\2410.html}
}

@misc{bojarskiEndEndLearning2016,
  title = {End to {{End Learning}} for {{Self-Driving Cars}}},
  author = {Bojarski, Mariusz and Testa, Davide Del and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D. and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake and Zieba, Karol},
  year = 2016,
  month = apr,
  number = {arXiv:1604.07316},
  eprint = {1604.07316},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1604.07316},
  urldate = {2025-01-06},
  abstract = {We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\theun\\Zotero\\storage\\BHGL9ASG\\Bojarski et al. - 2016 - End to End Learning for Self-Driving Cars.pdf;C\:\\Users\\theun\\Zotero\\storage\\DGXCYZ9H\\1604.html}
}

@article{boykovFastApproximateEnergy2001,
  title = {Fast Approximate Energy Minimization via Graph Cuts},
  author = {Boykov, Y. and Veksler, O. and Zabih, R.},
  year = 2001,
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {23},
  number = {11},
  pages = {1222--1239},
  issn = {1939-3539},
  doi = {10.1109/34.969114},
  urldate = {2025-06-06},
  abstract = {Many tasks in computer vision involve assigning a label (such as disparity) to every pixel. A common constraint is that the labels should vary smoothly almost everywhere while preserving sharp discontinuities that may exist, e.g., at object boundaries. These tasks are naturally stated in terms of energy minimization. The authors consider a wide class of energies with various smoothness constraints. Global minimization of these energy functions is NP-hard even in the simplest discontinuity-preserving case. Therefore, our focus is on efficient approximation algorithms. We present two algorithms based on graph cuts that efficiently find a local minimum with respect to two types of large moves, namely expansion moves and swap moves. These moves can simultaneously change the labels of arbitrarily large sets of pixels. In contrast, many standard algorithms (including simulated annealing) use small moves where only one pixel changes its label at a time. Our expansion algorithm finds a labeling within a known factor of the global minimum, while our swap algorithm handles more general energy functions. Both of these algorithms allow important cases of discontinuity preserving energies. We experimentally demonstrate the effectiveness of our approach for image restoration, stereo and motion. On real data with ground truth, we achieve 98 percent accuracy.},
  keywords = {Approximation algorithms,Computer vision,Energy measurement,Image restoration,Labeling,Markov random fields,Minimization methods,Motion estimation,Simulated annealing,Stereo vision},
  file = {C:\Users\theun\Zotero\storage\C4ARUEUJ\Boykov et al. - 2001 - Fast approximate energy minimization via graph cuts.pdf}
}

@article{brarImageSegmentationReview2025,
  title = {Image Segmentation Review: {{Theoretical}} Background and Recent Advances},
  shorttitle = {Image Segmentation Review},
  author = {Brar, Khushmeen Kaur and Goyal, Bhawna and Dogra, Ayush and Mustafa, Mohammed Ahmed and Majumdar, Rana and Alkhayyat, Ahmed and Kukreja, Vinay},
  year = 2025,
  month = feb,
  journal = {Information Fusion},
  volume = {114},
  pages = {102608},
  issn = {15662535},
  doi = {10.1016/j.inffus.2024.102608},
  urldate = {2024-12-06},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\P5TWGNFV\Brar et al. - 2025 - Image segmentation review Theoretical background and recent advances.pdf}
}

@misc{brasoNativeSegmentationVision2025,
  title = {Native {{Segmentation Vision Transformers}}},
  author = {Bras{\'o}, Guillem and O{\v s}ep, Aljo{\v s}a and {Leal-Taix{\'e}}, Laura},
  year = 2025,
  month = may,
  number = {arXiv:2505.16993},
  eprint = {2505.16993},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.16993},
  urldate = {2025-06-04},
  abstract = {Uniform downsampling remains the de facto standard for reducing spatial resolution in vision backbones. In this work, we propose an alternative design built around a content-aware spatial grouping layer, that dynamically assigns tokens to a reduced set based on image boundaries and their semantic content. Stacking our grouping layer across consecutive backbone stages results in hierarchical segmentation that arises natively in the feature extraction process, resulting in our coined Native Segmentation Vision Transformer. We show that a careful design of our architecture enables the emergence of strong segmentation masks solely from grouping layers, that is, without additional segmentation-specific heads. This sets the foundation for a new paradigm of native, backbone-level segmentation, which enables strong zero-shot results without mask supervision, as well as a minimal and efficient standalone model design for downstream segmentation tasks. Our project page is https://research.nvidia.com/labs/dvl/projects/native-segmentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\theun\\Zotero\\storage\\X9WK57E5\\Brasó et al. - 2025 - Native Segmentation Vision Transformers.pdf;C\:\\Users\\theun\\Zotero\\storage\\WA8TIUJD\\2505.html}
}

@inproceedings{brooksUnprocessingImagesLearned2019,
  title = {Unprocessing {{Images}} for {{Learned Raw Denoising}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Brooks, Tim and Mildenhall, Ben and Xue, Tianfan and Chen, Jiawen and Sharlet, Dillon and Barron, Jonathan T.},
  year = 2019,
  month = jun,
  pages = {11028--11037},
  publisher = {IEEE},
  address = {Long Beach, CA, USA},
  doi = {10.1109/CVPR.2019.01129},
  urldate = {2025-03-17},
  abstract = {Machine learning techniques work best when the data used for training resembles the data used for evaluation. This holds true for learned single-image denoising algorithms, which are applied to real raw camera sensor readings but, due to practical constraints, are often trained on synthetic image data. Though it is understood that generalizing from synthetic to real images requires careful consideration of the noise properties of camera sensors, the other aspects of an image processing pipeline (such as gain, color correction, and tone mapping) are often overlooked, despite their significant effect on how raw measurements are transformed into finished images. To address this, we present a technique to ``unprocess'' images by inverting each step of an image processing pipeline, thereby allowing us to synthesize realistic raw sensor measurements from commonly available Internet photos. We additionally model the relevant components of an image processing pipeline when evaluating our loss function, which allows training to be aware of all relevant photometric processing that will occur after denoising. By unprocessing and processing training data and model outputs in this way, we are able to train a simple convolutional neural network that has 14\%-38\% lower error rates and is 9\texttimes -18\texttimes{} faster than the previous state of the art on the Darmstadt Noise Dataset [31], and generalizes to sensors outside of that dataset as well.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-7281-3293-8},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\WG3YJ5WH\Brooks et al. - 2019 - Unprocessing Images for Learned Raw Denoising.pdf}
}

@article{brostowSemanticObjectClasses2009,
  title = {Semantic Object Classes in Video: {{A}} High-Definition Ground Truth Database},
  shorttitle = {Semantic Object Classes in Video},
  author = {Brostow, Gabriel J. and Fauqueur, Julien and Cipolla, Roberto},
  year = 2009,
  month = jan,
  journal = {Pattern Recognition Letters},
  series = {Video-Based {{Object}} and {{Event Analysis}}},
  volume = {30},
  number = {2},
  pages = {88--97},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2008.04.005},
  urldate = {2025-06-15},
  abstract = {Visual object analysis researchers are increasingly experimenting with video, because it is expected that motion cues should help with detection, recognition, and other analysis tasks. This paper presents the Cambridge-driving Labeled Video Database (CamVid) as the first collection of videos with object class semantic labels, complete with metadata. The database provides ground truth labels that associate each pixel with one of 32 semantic classes. The database addresses the need for experimental data to quantitatively evaluate emerging algorithms. While most videos are filmed with fixed-position CCTV-style cameras, our data was captured from the perspective of a driving automobile. The driving scenario increases the number and heterogeneity of the observed object classes. Over 10min of high quality 30Hz footage is being provided, with corresponding semantically labeled images at 1Hz and in part, 15Hz. The CamVid Database offers four contributions that are relevant to object analysis researchers. First, the per-pixel semantic segmentation of over 700 images was specified manually, and was then inspected and confirmed by a second person for accuracy. Second, the high-quality and large resolution color video images in the database represent valuable extended duration digitized footage to those interested in driving scenarios or ego-motion. Third, we filmed calibration sequences for the camera color response and intrinsics, and computed a 3D camera pose for each frame in the sequences. Finally, in support of expanding this or other databases, we present custom-made labeling software for assisting users who wish to paint precise class-labels for other images and videos. We evaluate the relevance of the database by measuring the performance of an algorithm from each of three distinct domains: multi-class object recognition, pedestrian detection, and label propagation.},
  keywords = {Label propagation,Object recognition,Semantic segmentation,Video database,Video understanding},
  file = {C\:\\Users\\theun\\Zotero\\storage\\V3NZH6TM\\Brostow et al. - 2009 - Semantic object classes in video A high-definition ground truth database.pdf;C\:\\Users\\theun\\Zotero\\storage\\85APA9VT\\S0167865508001220.html}
}

@inproceedings{bruggemannRefignAlignRefine2023,
  title = {Refign: {{Align}} and {{Refine}} for {{Adaptation}} of {{Semantic Segmentation}} to {{Adverse Conditions}}},
  shorttitle = {Refign},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}}},
  author = {Br{\"u}ggemann, David and Sakaridis, Christos and Truong, Prune and Van Gool, Luc},
  year = 2023,
  pages = {3174--3184},
  urldate = {2025-06-15},
  langid = {english},
  keywords = {Algorithm: image recognition and understanding (object detection categorization segmentation),Algorithms: Image recognition and understanding (object detection categorization segmentation),And algorithm (including transfer low-shot semi- self- and un-supervised learning),and algorithms (including transfer low-shot semi- self- and un-supervised learning),Condition,Domain adaptation,Formulation,formulations,Image recognition,Learning algorithms,Learning architectures,Machine learning,Machine learning architecture,Machine learning architectures,Machine-learning,Object detection,Objects detection,Robotics,Semantic Segmentation,Semantics,Un-supervised learning},
  file = {C:\Users\theun\Zotero\storage\US6JYJED\Brüggemann et al. - 2023 - Refign Align and Refine for Adaptation of Semantic Segmentation to Adverse Conditions.pdf}
}

@inproceedings{butlerNaturalisticOpenSource2012,
  title = {A {{Naturalistic Open Source Movie}} for {{Optical Flow Evaluation}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2012},
  author = {Butler, Daniel J. and Wulff, Jonas and Stanley, Garrett B. and Black, Michael J.},
  year = 2012,
  pages = {611--625},
  publisher = {Springer, Berlin, Heidelberg},
  issn = {1611-3349},
  doi = {10.1007/978-3-642-33783-3_44},
  urldate = {2025-05-15},
  abstract = {Ground truth optical flow is difficult to measure in real scenes with natural motion. As a result, optical flow data sets are restricted in terms of size, complexity, and diversity, making optical flow algorithms difficult to train and test on realistic data. We...},
  isbn = {978-3-642-33783-3},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\P933QR2G\Butler et al. - 2012 - A Naturalistic Open Source Movie for Optical Flow Evaluation.pdf}
}

@inproceedings{bychkovskyLearningPhotographicGlobal2011,
  title = {Learning Photographic Global Tonal Adjustment with a Database of Input/Output Image Pairs},
  booktitle = {{{CVPR}} 2011},
  author = {Bychkovsky, Vladimir and Paris, Sylvain and Chan, Eric and Durand, Fr{\'e}do},
  year = 2011,
  month = jun,
  pages = {97--104},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2011.5995413},
  urldate = {2025-02-20},
  abstract = {Adjusting photographs to obtain compelling renditions requires skill and time. Even contrast and brightness adjustments are challenging because they require taking into account the image content. Photographers are also known for having different retouching preferences. As the result of this complexity, rule-based, one-size-fits-all automatic techniques often fail. This problem can greatly benefit from supervised machine learning but the lack of training data has impeded work in this area. Our first contribution is the creation of a high-quality reference dataset. We collected 5,000 photos, manually annotated them, and hired 5 trained photographers to retouch each picture. The result is a collection of 5 sets of 5,000 example input-output pairs that enable supervised learning. We first use this dataset to predict a user's adjustment from a large training set. We then show that our dataset and features enable the accurate adjustment personalization using a carefully chosen set of training photos. Finally, we introduce difference learning: this method models and predicts difference between users. It frees the user from using predetermined photos for training. We show that difference learning enables accurate prediction using only a handful of examples.},
  keywords = {Cameras,Ground penetrating radar,Histograms,Machine learning,Measurement,Supervised learning,Training},
  file = {C\:\\Users\\theun\\Zotero\\storage\\G3DN2SGQ\\Bychkovsky et al. - 2011 - Learning photographic global tonal adjustment with a database of inputoutput image pairs.pdf;C\:\\Users\\theun\\Zotero\\storage\\8QYDQJXK\\5995413.html}
}

@inproceedings{caesarNuScenesMultimodalDataset2020,
  title = {{{nuScenes}}: {{A Multimodal Dataset}} for {{Autonomous Driving}}},
  shorttitle = {{{nuScenes}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Caesar, Holger and Bankiti, Varun and Lang, Alex H. and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
  year = 2020,
  pages = {11621--11631},
  urldate = {2025-05-29},
  file = {C:\Users\theun\Zotero\storage\9V8BSPYZ\Caesar et al. - 2020 - nuScenes A Multimodal Dataset for Autonomous Driving.pdf}
}

@article{caiLearningDeepSingle2018,
  title = {Learning a {{Deep Single Image Contrast Enhancer}} from {{Multi-Exposure Images}}},
  author = {Cai, Jianrui and Gu, Shuhang and Zhang, Lei},
  year = 2018,
  month = apr,
  journal = {IEEE Transactions on Image Processing},
  volume = {27},
  number = {4},
  pages = {2049--2062},
  issn = {1941-0042},
  doi = {10.1109/TIP.2018.2794218},
  urldate = {2025-03-16},
  abstract = {Due to the poor lighting condition and limited dynamic range of digital imaging devices, the recorded images are often under-/over-exposed and with low contrast. Most of previous single image contrast enhancement (SICE) methods adjust the tone curve to correct the contrast of an input image. Those methods, however, often fail in revealing image details because of the limited information in a single image. On the other hand, the SICE task can be better accomplished if we can learn extra information from appropriately collected training data. In this paper, we propose to use the convolutional neural network (CNN) to train a SICE enhancer. One key issue is how to construct a training data set of low-contrast and high-contrast image pairs for end-to-end CNN learning. To this end, we build a large-scale multi-exposure image data set, which contains 589 elaborately selected high-resolution multi-exposure sequences with 4,413 images. Thirteen representative multi-exposure image fusion and stack-based high dynamic range imaging algorithms are employed to generate the contrast enhanced images for each sequence, and subjective experiments are conducted to screen the best quality one as the reference image of each scene. With the constructed data set, a CNN can be easily trained as the SICE enhancer to improve the contrast of an under-/over-exposure image. Experimental results demonstrate the advantages of our method over existing SICE methods with a significant margin.},
  keywords = {convolutional neural network,Dynamic range,Heuristic algorithms,Image sequences,Imaging,Lighting,multi-exposure image fusion,Single image contrast enhancement,Training,Training data},
  file = {C\:\\Users\\theun\\Zotero\\storage\\DYZKDWEK\\Cai et al. - 2018 - Learning a Deep Single Image Contrast Enhancer from Multi-Exposure Images.pdf;C\:\\Users\\theun\\Zotero\\storage\\TDXBDHGN\\8259342.html}
}

@article{caiMFFNetMultifeatureFusion2022,
  title = {{{MFF-Net}}: {{A}} Multi-Feature Fusion Network for Community Detection in Complex Network},
  shorttitle = {{{MFF-Net}}},
  author = {Cai, Biao and Wang, Mingyue and Chen, Yongkeng and Hu, Yanmei and Liu, Mingzhe},
  year = 2022,
  month = sep,
  journal = {Knowledge-Based Systems},
  volume = {252},
  pages = {109408},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2022.109408},
  urldate = {2025-01-03},
  abstract = {Community detection is a crucial research orientation in complex network owing to its practical applications. Recently, the convolutional neural network (CNN) based edge classification community detection approach achieved impressive performance as a representative graph embedding method. However, existing methods rely heavily on the manually defined relationship between nodes and local feature representations of the edges, leading to the potential limitation of feature representation in complex real-world networks. To tackle this issue, in this paper, we propose a novel multi-feature fusion network for community detection, namely MFF-Net, which can embed edges of the network into comprehensive feature representations according to the intrinsic property of the network. Specifically, instead of utilizing local features of the edges only, we first propose to simultaneously consider local and non-local relationships of the edges through the local neighbors of the nodes and random node sequences sampled from a customized random walk. Then, we propose to convert local and non-local feature representations of the edges into grayscale images for edge classification via introducing a quantitative relationship between nodes. More importantly, those obtained feature representations are fused with an implicit manner in a latent space, which can leverage more comprehensive relationships for complex real-world networks. Extensive experiments on the computer-generated networks, small-scale and large-scale real-world networks, show that the proposed MFF-Net can achieve better performance compared with existing methods, in terms of quantitative and visualized results.},
  keywords = {/unread,Community detection,Convolutional neural network,Local feature,Non-local feature},
  file = {C\:\\Users\\theun\\Zotero\\storage\\DUQ2TM6H\\Cai et al. - 2022 - MFF-Net A multi-feature fusion network for community detection in complex network.pdf;C\:\\Users\\theun\\Zotero\\storage\\FNNUYMIY\\S0950705122007055.html}
}

@inproceedings{caiRetinexformerOnestageRetinexbased2023,
  title = {Retinexformer: {{One-stage Retinex-based Transformer}} for {{Low-light Image Enhancement}}},
  shorttitle = {Retinexformer},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Cai, Yuanhao and Bian, Hao and Lin, Jing and Wang, Haoqian and Timofte, Radu and Zhang, Yulun},
  year = 2023,
  pages = {12504--12513},
  urldate = {2025-08-08},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\2F3R8F7A\Cai et al. - 2023 - Retinexformer One-stage Retinex-based Transformer for Low-light Image Enhancement.pdf}
}

@misc{calayzhouCalayZhouMultispectralPedestrianDetectionResource2025,
  title = {{{CalayZhou}}/{{Multispectral-Pedestrian-Detection-Resource}}},
  author = {CalayZhou},
  year = 2025,
  month = may,
  urldate = {2025-05-15},
  abstract = {A list of resouces for multispectral pedestrian detection,including the datasets, methods, annotations  and tools.},
  keywords = {/unread}
}

@inproceedings{caoImageSegmentationNightvision2022,
  title = {Image Segmentation for Night-Vision Surveillance Camera Based on Deep Learning},
  booktitle = {Proc {{SPIE Int Soc Opt Eng}}},
  author = {Cao, Y. and Wang, J. and Qi, L. and Xu, M.},
  editor = {{Yang Y.}},
  year = 2022,
  volume = {12478},
  publisher = {SPIE},
  doi = {10.1117/12.2654811},
  abstract = {Under low illumination environments, the insufficient visible light and the existence of near-infrared light will cause photon noise and color distortion for the imaging of night-vision CMOS sensor. The light source highly affects the imaging of surveillance camera and declines the accuracy of semantic segmentation. In this work, we report a modified convolutional neural network based on DeepLabV3+. We modify the backbone of the network from Xception to MobileNetV2 to deal with the real-time vision task of night-vision surveillance camera. Linear bottleneck and inverted residuals are adopted in MobileNetV2, and they greatly reduce parameters of the network. A real-world low-light dataset with fine annotations for night-vision surveillance camera is proposed to train and evaluated the new framework. Aiming at the problem of insufficient training samples, transfer learning and a new image enhancement strategy are carried out to complete the training. We also change the loss function to a joint loss function to further improve the results of segmentation. Comparing with other existing state-of-the-art algorithms, the modified neural network shows competitive performance on both subjective and objective assessments. The ablation study comparing with the baseline model proves the effectiveness of the modifications. \copyright{} 2022 SPIE.},
  isbn = {0277786X (ISSN); 978-151066063-2 (ISBN)},
  langid = {english},
  keywords = {Camera-based,Cameras,Convolution,convolutional neural network,Convolutional neural network,Convolutional neural networks,Deep learning,Image enhancement,Images segmentations,Infrared devices,Loss functions,low illumination,Low illuminations,Near Infrared,near-infrared,Near-infrared,Night vision,Security systems,semantic segmentation,Semantic segmentation,Semantic Segmentation,Semantic Web,Semantics,Surveillance cameras,Vision},
  file = {C:\Users\theun\Zotero\storage\XSYJYENZ\Cao et al. - 2022 - Image segmentation for night-vision surveillance camera based on deep learning.pdf}
}

@article{caoZeroReferenceLowLightImageEnhancement2024,
  title = {A {{Zero-Reference Low-Light Image-Enhancement Approach Based}} on {{Noise Estimation}}},
  author = {Cao, Pingping and Niu, Qiang and Zhu, Yanping and Li, Tao},
  year = 2024,
  month = jan,
  journal = {Applied Sciences},
  volume = {14},
  number = {7},
  pages = {2846},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  doi = {10.3390/app14072846},
  urldate = {2025-04-02},
  abstract = {A novel zero-reference low-light image-enhancement approach based on noise estimation (ZLEN) is proposed to mitigate noise interference in image-enhancement processes, while the tenets of zero-reference and lightweight network architecture are maintained. ZLEN improves the high-order curve expression governing the mapping of low-light images to their enhanced counterparts, addressing image noise through a meticulously designed noise-estimation module and a zero-reference noise loss function. First, the higher-order curve expression with a noise term is defined, and then the noise map undergoes feature extraction through the semantic-aware attention module; following this, the resulting features are integrated with the low-light image. Ultimately, a lightweight convolutional neural network is adjusted to estimate higher-order curve parameters that link the low-light image to its enhanced version. Notably, ZLEN achieves luminance enhancement and noise reduction without paired or unpaired training data. Rigorous qualitative and quantitative evaluations were conducted on diverse benchmark datasets, demonstrating that ZLEN attained state-of-the-art (SOAT) status among existing zero-reference and unpaired-reference image-enhancement methodologies, while it exhibited comparable performance to full-reference image-enhancement methods. To confirm the practicality and robustness of ZLEN, the luminance enhancement was applied to mine images, which yielded satisfactory results.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {higher-order curve parameters,mine image enhancement,noise estimation,zero-reference image enhancement},
  file = {C:\Users\theun\Zotero\storage\EFAPT3TW\Cao et al. - 2024 - A Zero-Reference Low-Light Image-Enhancement Approach Based on Noise Estimation.pdf}
}

@inproceedings{caronEmergingPropertiesSelfSupervised2021,
  title = {Emerging {{Properties}} in {{Self-Supervised Vision Transformers}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  year = 2021,
  pages = {9650--9660},
  urldate = {2025-05-13},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\ZQCCYK8P\Caron et al. - 2021 - Emerging Properties in Self-Supervised Vision Transformers.pdf}
}

@inproceedings{changPyramidStereoMatching2018,
  title = {Pyramid {{Stereo Matching Network}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Chang, Jia-Ren and Chen, Yong-Sheng},
  year = 2018,
  pages = {5410--5418},
  urldate = {2025-05-13},
  file = {C:\Users\theun\Zotero\storage\3NP7GFZP\Chang and Chen - 2018 - Pyramid Stereo Matching Network.pdf}
}

@inproceedings{charbonnierTwoDeterministicHalfquadratic1994,
  title = {Two Deterministic Half-Quadratic Regularization Algorithms for Computed Imaging},
  booktitle = {Proceedings of 1st {{International Conference}} on {{Image Processing}}},
  author = {Charbonnier, P. and {Blanc-Feraud}, L. and Aubert, G. and Barlaud, M.},
  year = 1994,
  month = nov,
  volume = {2},
  pages = {168-172 vol.2},
  doi = {10.1109/ICIP.1994.413553},
  urldate = {2025-06-03},
  abstract = {Many image processing problems are ill-posed and must be regularized. Usually, a roughness penalty is imposed on the solution. The difficulty is to avoid the smoothing of edges, which are very important attributes of the image. The authors first give sufficient conditions for the design of such an edge-preserving regularization. Under these conditions, it is possible to introduce an auxiliary variable whose role is twofold. Firstly, it marks the discontinuities and ensures their preservation from smoothing. Secondly, it makes the criterion half-quadratic. The optimization is then easier. The authors propose a deterministic strategy, based on alternate minimizations on the image and the auxiliary variable. This yields two algorithms, ARTUR and LEGEND. The authors apply these algorithms to the problem of SPECT reconstruction.{$<>$}},
  keywords = {Computer applications,Gaussian noise,Image processing,Image reconstruction,Markov random fields,Minimization methods,Signal processing,Smoothing methods,Sufficient conditions,Tomography},
  file = {C\:\\Users\\theun\\Zotero\\storage\\8LANPD4V\\Charbonnier et al. - 1994 - Two deterministic half-quadratic regularization algorithms for computed imaging.pdf;C\:\\Users\\theun\\Zotero\\storage\\GA2AMG2E\\413553.html}
}

@inproceedings{chenCooperCooperativePerception2019,
  title = {Cooper: {{Cooperative Perception}} for {{Connected Autonomous Vehicles Based}} on {{3D Point Clouds}}},
  shorttitle = {Cooper},
  booktitle = {2019 {{IEEE}} 39th {{International Conference}} on {{Distributed Computing Systems}} ({{ICDCS}})},
  author = {Chen, Qi and Tang, Sihai and Yang, Qing and Fu, Song},
  year = 2019,
  month = jul,
  pages = {514--524},
  issn = {2575-8411},
  doi = {10.1109/ICDCS.2019.00058},
  urldate = {2025-01-02},
  abstract = {Autonomous vehicles may make wrong decisions due to inaccurate detection and recognition. Therefore, an intelligent vehicle can combine its own data with that of other vehicles to enhance perceptive ability, and thus improve detection accuracy and driving safety. However, multi-vehicle cooperative perception requires the integration of real world scenes and the traffic of raw sensor data exchange far exceeds the bandwidth of existing vehicular networks. To the best our knowledge, we are the first to conduct a study on raw-data level cooperative perception for enhancing the detection ability of self-driving systems. In this work, relying on LiDAR 3D point clouds, we fuse the sensor data collected from different positions and angles of connected vehicles. A point cloud based 3D object detection method is proposed to work on a diversity of aligned point clouds. Experimental results on KITTI and our collected dataset show that the proposed system outperforms perception by extending sensing area, improving detection accuracy and promoting augmented results. Most importantly, we demonstrate it is possible to transmit point clouds data for cooperative perception via existing vehicular network technologies.},
  keywords = {/unread,3D object detection,Accidents,Automobiles,Autonomous vehicles,Connected autonomous vehicles,Cooperative perception,Laser radar,Object detection,point clouds,Sensors,Three-dimensional displays},
  file = {C\:\\Users\\theun\\Zotero\\storage\\PW5H5XE4\\Chen et al. - 2019 - Cooper Cooperative Perception for Connected Autonomous Vehicles Based on 3D Point Clouds.pdf;C\:\\Users\\theun\\Zotero\\storage\\E52MFIFN\\8885377.html}
}

@article{chenCrossParallaxAttention2022,
  title = {Cross {{Parallax Attention Network}} for {{Stereo Image Super-Resolution}}},
  author = {Chen, Canqiang and Qing, Chunmei and Xu, Xiangmin and Dickinson, Patrick},
  year = 2022,
  journal = {IEEE Transactions on Multimedia},
  volume = {24},
  pages = {202--216},
  issn = {1941-0077},
  doi = {10.1109/TMM.2021.3050092},
  urldate = {2025-03-24},
  abstract = {Stereo super-resolution (SR) aims to enhance the spatial resolution of one camera view using additional information from the other. Previous deep-learning-based stereo SR methods indeed improved the SR performance effectively by employing additional information, but they are unable to super-resolve stereo images where there are large disparities, or different types of epipolar lines. Moreover, in these methods, one model can only super-solve images of a particular view, and for one specific scale factor. This paper proposes a cross parallax attention stereo super-resolution network (CPASSRnet) which can perform stereo SR of multiple scale factors for both views, with a single model. To overcome the difficulties of large disparity and different types of epipolar lines, a cross parallax attention module (CPAM) is presented, which captures the global correspondence of additional information for each view, relative to the other. CPAM allows the two views to exchange additional information with each other according to the generated attention maps. Quantitative and qualitative results compared with the state of the arts illustrate the superiority of CPASSRnet. Ablation experiments demonstrate that the proposed components are effective and noise tests verify the robustness of CPASSRnet.},
  keywords = {attention mechanism,Cameras,convolutional neural network,Estimation,single model for multiple scaling factors,Spatial resolution,Stereo super-resolution,Superresolution,Task analysis,Three-dimensional displays,Visualization},
  file = {C\:\\Users\\theun\\Zotero\\storage\\L7BC246D\\Chen et al. - 2022 - Cross Parallax Attention Network for Stereo Image Super-Resolution.pdf;C\:\\Users\\theun\\Zotero\\storage\\HTYI8H89\\9318556.html}
}

@article{chenDeepLabSemanticImage2018,
  title = {{{DeepLab}}: {{Semantic Image Segmentation}} with {{Deep Convolutional Nets}}, {{Atrous Convolution}}, and {{Fully Connected CRFs}}},
  shorttitle = {{{DeepLab}}},
  author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
  year = 2018,
  month = apr,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {40},
  number = {4},
  pages = {834--848},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2017.2699184},
  urldate = {2025-06-05},
  abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or `atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed ``DeepLab'' system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
  keywords = {atrous convolution,Computational modeling,conditional random fields,Context,Convolution,Convolutional neural networks,Image resolution,Image segmentation,Neural networks,semantic segmentation,Semantics},
  file = {C:\Users\theun\Zotero\storage\4LSM36ZR\Chen et al. - 2018 - DeepLab Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Con.pdf}
}

@article{chenDeepNeuralNetwork2021,
  title = {Deep {{Neural Network Based Vehicle}} and {{Pedestrian Detection}} for {{Autonomous Driving}}: {{A Survey}}},
  shorttitle = {Deep {{Neural Network Based Vehicle}} and {{Pedestrian Detection}} for {{Autonomous Driving}}},
  author = {Chen, Long and Lin, Shaobo and Lu, Xiankai and Cao, Dongpu and Wu, Hangbin and Guo, Chi and Liu, Chun and Wang, Fei-Yue},
  year = 2021,
  month = jun,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {22},
  number = {6},
  pages = {3234--3246},
  issn = {1558-0016},
  doi = {10.1109/TITS.2020.2993926},
  urldate = {2025-01-02},
  abstract = {Vehicle and pedestrian detection is one of the critical tasks in autonomous driving. Since heterogeneous techniques have been proposed, the selection of a detection system with an appropriate balance among detection accuracy, speed and memory consumption for a specific task has become very challenging. To deal with this issue and to provide guidance for model selection, this paper analyzes several mainstream object detection architectures, including Faster R-CNN, R-FCN, and SSD, along with several typical feature extractors, such as ResNet50, ResNet101, MobileNet\_V1, MobileNet\_V2, Inception\_V2 and Inception\_ResNet\_V2. By conducting extensive experiments using the KITTI benchmark, which is a commonly used street dataset, we demonstrate that Faster R-CNN ResNet50 obtains the best average precision (AP) (58\%) for vehicle and pedestrian detection, with a speed of 8.6 FPS. Faster R-CNN Inception\_V2 performs best for detecting cars and detecting pedestrians respectively (74.5\% and 47.3\%). ResNet101 consumes the highest memory (9907 MB) and has the largest number of parameters (64.42 millions), and Inception\_ResNet\_V2 is the slowest model (3.05 FPS). SSD MobileNet\_V2 is the fastest model (70 FPS), and SSD MobileNet\_V1 is the lightest model in terms of memory usage (875 MB), both of which are suitable for applications on mobile and embedded devices.},
  keywords = {/unread,autonomous driving,Computational modeling,Deep neural networks,Detectors,Feature extraction,Object detection,pedestrian detection,Proposals,Residual neural networks,survey,Task analysis,vehicle detection},
  file = {C\:\\Users\\theun\\Zotero\\storage\\YDQXIS92\\Chen et al. - 2021 - Deep Neural Network Based Vehicle and Pedestrian Detection for Autonomous Driving A Survey.pdf;C\:\\Users\\theun\\Zotero\\storage\\ESWY6USS\\9440863.html}
}

@inproceedings{chenEncoderDecoderAtrousSeparable2018,
  title = {Encoder-{{Decoder}} with {{Atrous Separable Convolution}} for {{Semantic Image Segmentation}}},
  booktitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  year = 2018,
  pages = {801--818},
  urldate = {2025-06-05},
  file = {C:\Users\theun\Zotero\storage\PBN2VJ55\Chen et al. - 2018 - Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation.pdf}
}

@misc{chenEndtoendAutonomousDriving2024,
  title = {End-to-End {{Autonomous Driving}}: {{Challenges}} and {{Frontiers}}},
  shorttitle = {End-to-End {{Autonomous Driving}}},
  author = {Chen, Li and Wu, Penghao and Chitta, Kashyap and Jaeger, Bernhard and Geiger, Andreas and Li, Hongyang},
  year = 2024,
  month = aug,
  number = {arXiv:2306.16927},
  eprint = {2306.16927},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.16927},
  urldate = {2025-01-06},
  abstract = {The autonomous driving community has witnessed a rapid growth in approaches that embrace an end-to-end algorithm framework, utilizing raw sensor input to generate vehicle motion plans, instead of concentrating on individual tasks such as detection and motion prediction. End-to-end systems, in comparison to modular pipelines, benefit from joint feature optimization for perception and planning. This field has flourished due to the availability of large-scale datasets, closed-loop evaluation, and the increasing need for autonomous driving algorithms to perform effectively in challenging scenarios. In this survey, we provide a comprehensive analysis of more than 270 papers, covering the motivation, roadmap, methodology, challenges, and future trends in end-to-end autonomous driving. We delve into several critical challenges, including multi-modality, interpretability, causal confusion, robustness, and world models, amongst others. Additionally, we discuss current advancements in foundation models and visual pre-training, as well as how to incorporate these techniques within the end-to-end driving framework. we maintain an active repository that contains up-to-date literature and open-source projects at https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\theun\\Zotero\\storage\\QZE22S5M\\Chen et al. - 2024 - End-to-end Autonomous Driving Challenges and Frontiers.pdf;C\:\\Users\\theun\\Zotero\\storage\\EI6NFEM2\\2306.html}
}

@article{chenImageEnhancementBased2025,
  title = {An Image Enhancement Based Method for Improving {{rPPG}} Extraction under Low-Light Illumination},
  author = {Chen, Shutao and Wong, Kwan Long and Chan, Tze Tai and Wang, Ya and So, Richard Hau Yue and Chin, Jing Wei},
  year = 2025,
  month = feb,
  journal = {Biomedical Signal Processing and Control},
  volume = {100},
  pages = {106963},
  issn = {17468094},
  doi = {10.1016/j.bspc.2024.106963},
  urldate = {2024-12-06},
  abstract = {Remote photoplethysmography (rPPG) has gained significant attention as a non-invasive approach for measuring human vital signs from videos. However, the reliance on capturing the reflection of ambient lighting causes it to be susceptible to the adverse effects of low illumination levels, leading to deterioration in the signal quality. The preliminary study introduces a novel methodology for enhancing rPPG signal extraction in low-light conditions through the integration of an Image Enhancement Model (IEM) inspired by Retinex theory, which significantly improved signal quality by preprocessing video frames to better capture the subtle changes in facial blood flow. Recognizing the challenge posed by the generalization capacity over different deep-learning models and unseen examples from various datasets, this study further evaluated the efficacy of the IEM + rPPG extraction model across multiple datasets (UBFC-rPPG, BH-rPPG, MMPD-rPPG, and VIPL-HR) by integrating IEM with existing deep-learning based rPPG extraction models including DeepPhys, PhysNet, and PhysFormer and traditional POS rPPG extraction method. Our experiments demonstrate a consistent improvement for all the methods in heart rate estimation accuracy across all datasets, underscoring the IEM's adaptability and effectiveness. The paper also explores the application of our method to traditional rPPG extraction techniques, further validating its potential for broader usage. Through comprehensive analysis, this research not only confirms the impact of lighting conditions on rPPG signal quality but also provides a robust solution for more reliable non-invasive vital sign monitoring in diverse environments.},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\EKQG9JEZ\Chen et al. - 2025 - An image enhancement based method for improving rPPG extraction under low-light illumination.pdf}
}

@inproceedings{chenLearningSeeDark2018,
  title = {Learning to {{See}} in the {{Dark}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Chen, Chen and Chen, Qifeng and Xu, Jia and Koltun, Vladlen},
  year = 2018,
  month = jun,
  pages = {3291--3300},
  publisher = {IEEE},
  address = {Salt Lake City, UT},
  doi = {10.1109/CVPR.2018.00347},
  urldate = {2025-06-03},
  abstract = {Imaging in low light is challenging due to low photon count and low SNR. Short-exposure images suffer from noise, while long exposure can induce blur and is often impractical. A variety of denoising, deblurring, and enhancement techniques have been proposed, but their effectiveness is limited in extreme conditions, such as video-rate imaging at night. To support the development of learningbased pipelines for low-light image processing, we introduce a dataset of raw short-exposure low-light images, with corresponding long-exposure reference images. Using the presented dataset, we develop a pipeline for processing low-light images, based on end-to-end training of a fullyconvolutional network. The network operates directly on raw sensor data and replaces much of the traditional image processing pipeline, which tends to perform poorly on such data. We report promising results on the new dataset, analyze factors that affect performance, and highlight opportunities for future work.},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\WLAFANEA\Chen et al. - 2018 - Learning to See in the Dark.pdf}
}

@article{chenLowLightSparsePolarization2024,
  title = {Low-{{Light Sparse Polarization Demosaicing Network}} ({{LLSPD-Net}}): {{Polarization Image Demosaicing Based}} on {{Stokes Vector Completion}} in {{Low-Light Environment}}},
  shorttitle = {Low-{{Light Sparse Polarization Demosaicing Network}} ({{LLSPD-Net}})},
  author = {Chen, Guangqiu and Hao, Youfei and Duan, Jin and Liu, Ju and Jia, Linfeng and Song, Jingyuan},
  year = 2024,
  month = jan,
  journal = {Sensors},
  volume = {24},
  number = {11},
  pages = {3299},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s24113299},
  urldate = {2025-02-17},
  abstract = {Polarization imaging has achieved a wide range of applications in military and civilian fields such as camouflage detection and autonomous driving. However, when the imaging environment involves a low-light condition, the number of photons is low and the photon transmittance of the conventional Division-of-Focal-Plane (DoFP) structure is small. Therefore, the traditional demosaicing methods are often used to deal with the serious noise and distortion generated by polarization demosaicing in low-light environment. Based on the aforementioned issues, this paper proposes a model called Low-Light Sparse Polarization Demosaicing Network (LLSPD-Net) for simulating a sparse polarization sensor acquisition of polarization images in low-light environments. The model consists of two parts: an intensity image enhancement network and a Stokes vector complementation network. In this work, the intensity image enhancement network is used to enhance low-light images and obtain high-quality RGB images, while the Stokes vector is used to complement the network. We discard the traditional idea of polarization intensity image interpolation and instead design a polarization demosaicing method with Stokes vector complementation. By using the enhanced intensity image as a guide, the completion of the Stokes vector is achieved. In addition, to train our network, we collected a dataset of paired color polarization images that includes both low-light and regular-light conditions. A comparison with state-of-the-art methods on both self-constructed and publicly available datasets reveals that our model outperforms traditional low-light image enhancement demosaicing methods in both qualitative and quantitative experiments.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {hourglass network,low light,polarization image demosaicing,Stokes vector completion},
  file = {C:\Users\theun\Zotero\storage\7H89NVQ6\Chen et al. - 2024 - Low-Light Sparse Polarization Demosaicing Network (LLSPD-Net) Polarization Image Demosaicing Based.pdf}
}

@article{chenMilestonesAutonomousDriving2023,
  title = {Milestones in {{Autonomous Driving}} and {{Intelligent Vehicles}}: {{Survey}} of {{Surveys}}},
  shorttitle = {Milestones in {{Autonomous Driving}} and {{Intelligent Vehicles}}},
  author = {Chen, Long and Li, Yuchen and Huang, Chao and Li, Bai and Xing, Yang and Tian, Daxin and Li, Li and Hu, Zhongxu and Na, Xiaoxiang and Li, Zixuan and Teng, Siyu and Lv, Chen and Wang, Jinjun and Cao, Dongpu and Zheng, Nanning and Wang, Fei-Yue},
  year = 2023,
  month = feb,
  journal = {IEEE Transactions on Intelligent Vehicles},
  volume = {8},
  number = {2},
  eprint = {2303.17220},
  primaryclass = {cs},
  pages = {1046--1056},
  issn = {2379-8904, 2379-8858},
  doi = {10.1109/TIV.2022.3223131},
  urldate = {2025-01-03},
  abstract = {Interest in autonomous driving (AD) and intelligent vehicles (IVs) is growing at a rapid pace due to the convenience, safety, and economic benefits. Although a number of surveys have reviewed research achievements in this field, they are still limited in specific tasks, lack of systematic summary and research directions in the future. Here we propose a Survey of Surveys (SoS) for total technologies of AD and IVs that reviews the history, summarizes the milestones, and provides the perspectives, ethics, and future research directions. To our knowledge, this article is the first SoS with milestones in AD and IVs, which constitutes our complete research work together with two other technical surveys. We anticipate that this article will bring novel and diverse insights to researchers and abecedarians, and serve as a bridge between past and future.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {C\:\\Users\\theun\\Zotero\\storage\\D22EGPLX\\Chen et al. - 2023 - Milestones in Autonomous Driving and Intelligent Vehicles Survey of Surveys.pdf;C\:\\Users\\theun\\Zotero\\storage\\YKUVWUHG\\2303.html}
}

@inproceedings{chenPreTrainedImageProcessing2021,
  title = {Pre-{{Trained Image Processing Transformer}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Chen, Hanting and Wang, Yunhe and Guo, Tianyu and Xu, Chang and Deng, Yiping and Liu, Zhenhua and Ma, Siwei and Xu, Chunjing and Xu, Chao and Gao, Wen},
  year = 2021,
  month = jun,
  pages = {12294--12305},
  publisher = {IEEE},
  address = {Nashville, TN, USA},
  doi = {10.1109/CVPR46437.2021.01212},
  urldate = {2025-03-12},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-6654-4509-2},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\UKF29B5U\Chen et al. - 2021 - Pre-Trained Image Processing Transformer.pdf}
}

@article{chenRealtimeVisionGuidance2025,
  title = {A Real-Time Vision Guidance Method for Autonomous Longan Picking by the {{UAV}}},
  author = {Chen, Hengxu and Wu, Kaixuan and Lin, Hengyi and Zhou, Haobo and Zhou, Zhengqi and Mai, Yuju and Shi, Linlin and Zhang, Meiqi and Ma, Zhe and Lin, Peihan and Li, Jun},
  year = 2025,
  month = feb,
  journal = {Computers and Electronics in Agriculture},
  volume = {229},
  pages = {109735},
  issn = {01681699},
  doi = {10.1016/j.compag.2024.109735},
  urldate = {2024-12-13},
  abstract = {The unmanned aerial vehicle (UAV) with high flexibility has great application prospect in autonomous longan picking. However, the robustness of the UAV preplanned flight mode is difficult to satisfy precise picking requirements. This study proposes a novel method for longan autonomous picking based on the UAV with real-time visual guidance. The correlations between the UAV velocity and positions of the longan cluster and picking point are established, and make the UAV dynamically adjusts its flight velocity to approach the cluster and accurately align the point for autonomous picking. The longan cluster and their picking points positions are obtained via the YOLOv8n improved by C2F-Dense, MS-OKS Loss and channel pruning. The improved model achieves high accuracy detection, low computation cost and real-time response capabilities. The experimental results revealed that the AP5m0ax of the improved YOLOv8n for longan cluster detection reached 74.30 \%, and the FPS increased by 40.57 \%. For picking point detection, the AP8m5a:x95 reached 86.68 \% with 88.77 \% FPS increase. Additionally, the proposed method has been tested on the DJI M300 without GPU, and its reliability was verified. This research provides technical support for autonomous picking.},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\QITDFXRH\Chen et al. - 2025 - A real-time vision guidance method for autonomous longan picking by the UAV.pdf}
}

@misc{chenRethinkingAtrousConvolution2017,
  title = {Rethinking {{Atrous Convolution}} for {{Semantic Image Segmentation}}},
  author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  year = 2017,
  month = dec,
  number = {arXiv:1706.05587},
  eprint = {1706.05587},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.05587},
  urldate = {2025-06-05},
  abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\5K6NUFM6\\Chen et al. - 2017 - Rethinking Atrous Convolution for Semantic Image Segmentation.pdf;C\:\\Users\\theun\\Zotero\\storage\\JQZVNCCH\\1706.html}
}

@misc{chenSemanticImageSegmentation2016,
  title = {Semantic {{Image Segmentation}} with {{Deep Convolutional Nets}} and {{Fully Connected CRFs}}},
  author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
  year = 2016,
  month = jun,
  number = {arXiv:1412.7062},
  eprint = {1412.7062},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1412.7062},
  urldate = {2025-06-05},
  abstract = {Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called "semantic image segmentation"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our "DeepLab" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6\% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\theun\\Zotero\\storage\\LNX7T6XV\\Chen et al. - 2016 - Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs.pdf;C\:\\Users\\theun\\Zotero\\storage\\XIEUYBWS\\1412.html}
}

@inproceedings{chenSingleImageDepthPerception2016,
  title = {Single-{{Image Depth Perception}} in the {{Wild}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chen, Weifeng and Fu, Zhao and Yang, Dawei and Deng, Jia},
  year = 2016,
  volume = {29},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-07-03},
  abstract = {This paper studies single-image depth perception in the wild, i.e., recovering depth from a single image taken in unconstrained settings. We introduce a new dataset ``Depth in the Wild'' consisting of images in the wild annotated with relative depth between pairs of random points. We also propose a new algorithm that learns to estimate metric depth using annotations of relative depth. Compared to the state of the art, our algorithm is simpler and performs better. Experiments show that our algorithm, combined with existing RGB-D data and our new relative depth annotations, significantly improves single-image depth perception in the wild.},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\ZBWJUT7D\Chen et al. - 2016 - Single-Image Depth Perception in the Wild.pdf}
}

@inproceedings{chenStructureCentricRobustMonocular2024,
  title = {Structure-{{Centric Robust Monocular Depth Estimation}} via {{Knowledge Distillation}}},
  booktitle = {Proceedings of the {{Asian Conference}} on {{Computer Vision}}},
  author = {Chen, Runze and Luo, Haiyong and Zhao, Fang and Yu, Jingze and Jia, Yupeng and Wang, Juan and Ma, Xuepeng},
  year = 2024,
  pages = {2970--2987},
  urldate = {2025-06-30},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\VVAFIPX5\Chen et al. - 2024 - Structure-Centric Robust Monocular Depth Estimation via Knowledge Distillation.pdf}
}

@article{chenSurveyComputerVision2024,
  title = {A {{Survey}} of {{Computer Vision Detection}}, {{Visual SLAM Algorithms}}, and {{Their Applications}} in {{Energy-Efficient Autonomous Systems}}},
  author = {Chen, Lu and Li, Gun and Xie, Weisi and Tan, Jie and Li, Yang and Pu, Junfeng and Chen, Lizhu and Gan, Decheng and Shi, Weimin},
  year = 2024,
  month = oct,
  journal = {Energies},
  volume = {17},
  number = {20},
  pages = {5177},
  issn = {1996-1073},
  doi = {10.3390/en17205177},
  urldate = {2024-12-06},
  abstract = {Within the area of environmental perception, automatic navigation, object detection, and computer vision are crucial and demanding fields with many applications in modern industries, such as multi-target long-term visual tracking in automated production, defect detection, and driverless robotic vehicles. The performance of computer vision has greatly improved recently thanks to developments in deep learning algorithms and hardware computing capabilities, which have spawned the creation of a large number of related applications. At the same time, with the rapid increase in autonomous systems in the market, energy consumption has become an increasingly critical issue in computer vision and SLAM (Simultaneous Localization and Mapping) algorithms. This paper presents the results of a detailed review of over 100 papers published over the course of two decades (1999--2024), with a primary focus on the technical advancement in computer vision. To elucidate the foundational principles, an examination of typical visual algorithms based on traditional correlation filtering was initially conducted. Subsequently, a comprehensive overview of the state-of-the-art advancements in deep learning-based computer vision techniques was compiled. Furthermore, a comparative analysis of conventional and novel algorithms was undertaken to discuss the future trends and directions of computer vision. Lastly, the feasibility of employing visual SLAM algorithms in the context of autonomous vehicles was explored. Additionally, in the context of intelligent robots for low-carbon, unmanned factories, we discussed model optimization techniques such as pruning and quantization, highlighting their importance in enhancing energy efficiency. We conducted a comprehensive comparison of the performance and energy consumption of various computer vision algorithms, with a detailed exploration of how to balance these factors and a discussion of potential future development trends.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\9H7Z4CQE\Chen et al. - 2024 - A Survey of Computer Vision Detection, Visual SLAM Algorithms, and Their Applications in Energy-Effi.pdf}
}

@inproceedings{chenWhenSemanticSegmentation2024,
  title = {When {{Semantic Segmentation Meets Frequency Aliasing}}},
  booktitle = {Int. {{Conf}}. {{Learn}}. {{Represent}}., {{ICLR}}},
  author = {Chen, L. and Gu, L. and Fu, Y.},
  year = 2024,
  publisher = {International Conference on Learning Representations, ICLR},
  doi = {10.48550/arXiv.2403.09065},
  abstract = {Despite recent advancements in semantic segmentation, where and what pixels are hard to segment remains largely unexplored. Existing research only separates an image into easy and hard regions and empirically observes the latter are associated with object boundaries. In this paper, we conduct a comprehensive analysis of hard pixel errors, categorizing them into three types: false responses, merging mistakes, and displacements. Our findings reveal a quantitative association between hard pixels and aliasing, which is distortion caused by the overlapping of frequency components in the Fourier domain during downsampling. To identify the frequencies responsible for aliasing, we propose using the equivalent sampling rate to calculate the Nyquist frequency, which marks the threshold for aliasing. Then, we introduce the aliasing score as a metric to quantify the extent of aliasing. While positively correlated with the proposed aliasing score, three types of hard pixels exhibit different patterns. Here, we propose two novel de-aliasing filter (DAF) and frequency mixing (FreqMix) modules to alleviate aliasing degradation by accurately removing or adjusting frequencies higher than the Nyquist frequency. The DAF precisely removes the frequencies responsible for aliasing before downsampling, while the FreqMix dynamically selects high-frequency components within the encoder block. Experimental results demonstrate consistent improvements in semantic segmentation and low-light instance segmentation tasks. The code is available at: https://github.com/Linwei-Chen/Seg-Aliasing. \copyright{} 2024 12th International Conference on Learning Representations, ICLR 2024. All rights reserved.},
  langid = {english},
  keywords = {Aliasing,Comprehensive analysis,Down sampling,Frequency aliasing,Frequency components,Frequency mixing,Nyquist frequency,Object boundaries,Pixel errors,Pixels,Semantic segmentation,Semantic Segmentation,Semantics,Signal sampling},
  file = {C\:\\Users\\theun\\Zotero\\storage\\ZAL7BAKM\\Chen et al. - 2024 - When Semantic Segmentation Meets Frequency Aliasing.pdf;C\:\\Users\\theun\\Zotero\\storage\\SCRZTF7E\\2403.html}
}

@article{chibRecentAdvancementsEndtoEnd2024,
  title = {Recent {{Advancements}} in {{End-to-End Autonomous Driving Using Deep Learning}}: {{A Survey}}},
  shorttitle = {Recent {{Advancements}} in {{End-to-End Autonomous Driving Using Deep Learning}}},
  author = {Chib, Pranav Singh and Singh, Pravendra},
  year = 2024,
  month = jan,
  journal = {IEEE Transactions on Intelligent Vehicles},
  volume = {9},
  number = {1},
  pages = {103--118},
  issn = {2379-8904},
  doi = {10.1109/TIV.2023.3318070},
  urldate = {2025-01-06},
  abstract = {End-to-End driving is a promising paradigm as it circumvents the drawbacks associated with modular systems, such as their overwhelming complexity and propensity for error propagation. Autonomous driving transcends conventional traffic patterns by proactively recognizing critical events in advance, ensuring passengers safety and providing them with comfortable transportation, particularly in highly stochastic and variable traffic settings. This article presents a comprehensive review of the End-to-End autonomous driving stack. It provides a taxonomy of automated driving tasks wherein neural networks have been employed in an End-to-End manner, encompassing the entire driving process from perception to control. Recent developments in End-to-End autonomous driving are analyzed, and research is categorized based on underlying principles, methodologies, and core functionality. These categories encompass sensorial input, main and auxiliary output, learning approaches ranging from imitation to reinforcement learning, and model evaluation techniques. The survey incorporates a detailed discussion of the explainability and safety aspects. Furthermore, it assesses the state-of-the-art, identifies challenges, and explores future possibilities.},
  keywords = {Autonomous driving,Autonomous vehicles,Computer architecture,deep learning,end-to-end driving,intelligent transportation system,Laser radar,Navigation,Pipelines,Surveys,Task analysis},
  file = {C\:\\Users\\theun\\Zotero\\storage\\SP5U26SU\\Chib and Singh - 2024 - Recent Advancements in End-to-End Autonomous Driving Using Deep Learning A Survey.pdf;C\:\\Users\\theun\\Zotero\\storage\\GVKNB5A8\\10258330.html}
}

@article{chiNewgenerationIntelligentWelding2025,
  title = {Towards New-Generation of Intelligent Welding Manufacturing: {{A}} Systematic Review on {{3D}} Vision Measurement and Path Planning of Humanoid Welding Robots},
  shorttitle = {Towards New-Generation of Intelligent Welding Manufacturing},
  author = {Chi, Peng and Wang, Zhenmin and Liao, Haipeng and Li, Ting and Wu, Xiangmiao and Zhang, Qin},
  year = 2025,
  month = jan,
  journal = {Measurement},
  volume = {242},
  pages = {116065},
  issn = {02632241},
  doi = {10.1016/j.measurement.2024.116065},
  urldate = {2024-12-06},
  abstract = {In recent years, intelligent welding technology has emerged as a prominent focus within the welding domain, amalgamating a diverse array of sophisticated technologies, including robotics, computer vision, artificial intelligence, and sensor systems. This integration heralds unprecedented levels of automation in welding processes, endowing them with heightened efficiency, precision, and cognitive capabilities. Notably, scholarly attention has been dedicated to the realms of intelligent welding manufacturing and welding robotics. However, a discernible lacuna exists in the form of a comprehensive review elucidating the pivotal technologies underpinning welding robots, while research about autonomous mobile welding robots appears to have encountered a developmental impasse. In response, this study undertakes a systematic literature review to scrutinize the core technologies of humanoid welding robots (HWR), positing their elevated research prospects within the milieu of next-generation intelligent welding manufacturing. This study explores the hardware of humanoid welding robots as an emerging technology, drawing on current advancements in humanoid robotics. The key technologies relevant to both humanoid and welding robots are also examined, highlighting their integration and potential applications. Initially, the discourse delves into hand-eye calibration methodologies, delineating a multifaceted approach predicated upon a multi-coordinate system tailored to HWR. Subsequently, the significance of visual-based pose estimation and three-dimensional (3D) reconstruction techniques is underscored, given their instrumental role in furnishing HWR with environmental cognition, a discourse expounded meticulously. Additionally, meticulous scrutiny is accorded to mobile robot path planning and dual-robot trajectory planning methodologies, pivotal for orchestrating welding operation sequences tailored to HWR. To assess the job completion and potential applications of HWR, this paper evaluates welding quality judgment and explores the humanoid robot's utility in non-welding scenes. Conclusively, this paper identifies the exigencies confronting HWR and proffers strategic directives delineating avenues for seminal research and practical application within this burgeoning domain.},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\WFINL4HL\Chi et al. - 2025 - Towards new-generation of intelligent welding manufacturing A systematic review on 3D vision measur.pdf}
}

@article{choiKAISTMultiSpectralDay2018,
  title = {{{KAIST Multi-Spectral Day}}/{{Night Data Set}} for {{Autonomous}} and {{Assisted Driving}}},
  author = {Choi, Yukyung and Kim, Namil and Hwang, Soonmin and Park, Kibaek and Yoon, Jae Shin and An, Kyounghwan and Kweon, In So},
  year = 2018,
  month = mar,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {19},
  number = {3},
  pages = {934--948},
  issn = {1558-0016},
  doi = {10.1109/TITS.2018.2791533},
  urldate = {2025-05-15},
  abstract = {We introduce the KAIST multi-spectral data set, which covers a great range of drivable regions, from urban to residential, for autonomous systems. Our data set provides the different perspectives of the world captured in coarse time slots (day and night), in addition to fine time slots (sunrise, morning, afternoon, sunset, night, and dawn). For all-day perception of autonomous systems, we propose the use of a different spectral sensor, i.e., a thermal imaging camera. Toward this goal, we develop a multi-sensor platform, which supports the use of a co-aligned RGB/Thermal camera, RGB stereo, 3-D LiDAR, and inertial sensors (GPS/IMU) and a related calibration technique. We design a wide range of visual perception tasks including the object detection, drivable region detection, localization, image enhancement, depth estimation, and colorization using a single/multi-spectral approach. In this paper, we provide a description of our benchmark with the recording platform, data format, development toolkits, and lessons about the progress of capturing data sets.},
  keywords = {advanced driver assistance system,autonomous driving,benchmarks,Calibration,Cameras,Dataset,KAIST multi-sepctral,Laser radar,multi-spectral dataset in day and night,multi-spectral vehicle system,Optical distortion,Optical imaging,Optical sensors,Thermal sensors},
  file = {C:\Users\theun\Zotero\storage\6XJ5B8Z4\Choi et al. - 2018 - KAIST Multi-Spectral DayNight Data Set for Autonomous and Assisted Driving.pdf}
}

@misc{choiSelfsupervisedMonocularDepth2025,
  title = {Self-Supervised {{Monocular Depth Estimation Robust}} to {{Reflective Surface Leveraged}} by {{Triplet Mining}}},
  author = {Choi, Wonhyeok and Hwang, Kyumin and Peng, Wei and Choi, Minwoo and Im, Sunghoon},
  year = 2025,
  month = feb,
  number = {arXiv:2502.14573},
  eprint = {2502.14573},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.14573},
  urldate = {2025-07-08},
  abstract = {Self-supervised monocular depth estimation (SSMDE) aims to predict the dense depth map of a monocular image, by learning depth from RGB image sequences, eliminating the need for ground-truth depth labels. Although this approach simplifies data acquisition compared to supervised methods, it struggles with reflective surfaces, as they violate the assumptions of Lambertian reflectance, leading to inaccurate training on such surfaces. To tackle this problem, we propose a novel training strategy for an SSMDE by leveraging triplet mining to pinpoint reflective regions at the pixel level, guided by the camera geometry between different viewpoints. The proposed reflection-aware triplet mining loss specifically penalizes the inappropriate photometric error minimization on the localized reflective regions while preserving depth accuracy in non-reflective areas. We also incorporate a reflection-aware knowledge distillation method that enables a student model to selectively learn the pixel-level knowledge from reflective and non-reflective regions. This results in robust depth estimation across areas. Evaluation results on multiple datasets demonstrate that our method effectively enhances depth quality on reflective surfaces and outperforms state-of-the-art SSMDE baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\theun\\Zotero\\storage\\4JZWR7YQ\\Choi et al. - 2025 - Self-supervised Monocular Depth Estimation Robust to Reflective Surface Leveraged by Triplet Mining.pdf;C\:\\Users\\theun\\Zotero\\storage\\PKZ7BLE5\\2502.html}
}

@article{choiSensorFusionSystem2023,
  title = {A Sensor Fusion System with Thermal Infrared Camera and {{LiDAR}} for Autonomous Vehicles and Deep Learning Based Object Detection},
  author = {Choi, Ji Dong and Kim, Min Young},
  year = 2023,
  month = apr,
  journal = {ICT Express},
  volume = {9},
  number = {2},
  pages = {222--227},
  issn = {2405-9595},
  doi = {10.1016/j.icte.2021.12.016},
  urldate = {2025-01-14},
  abstract = {Vision, Radar, and LiDAR sensors are widely used for autonomous vehicle perception technology. Especially object detection and classification are primarily dependent on vision sensors. However, under poor lighting conditions, dazzling sunlight, or bad weather an object might be difficult to be identified with general vision sensors. In this paper, we propose a sensor fusion system that combines a thermal infrared camera and a LiDAR sensor that can reliably detect and identify objects even in environments with poor visibility, such as day or night. The proposed method obtains the external parameters of the two sensors by designing and manufacturing a 3D calibration target to externally calibrate the thermal infrared camera and the LiDAR sensor. To verify the performance, experiments were conducted in day and night environments. The proposed sensor system and fusion algorithm show that it can reliably detect and identify objects even in environments with poor visibility, such as day or night.},
  keywords = {Autonomous vehicles,Convolution neural network,LiDAR,Object detection,Sensor fusion,Thermal infrared camera},
  file = {C\:\\Users\\theun\\Zotero\\storage\\QZLNBVCC\\Choi and Kim - 2023 - A sensor fusion system with thermal infrared camera and LiDAR for autonomous vehicles and deep learn.pdf;C\:\\Users\\theun\\Zotero\\storage\\6UDA48SW\\S2405959521001818.html}
}

@article{choModifiedPerceptualCycle2021,
  title = {Modified {{Perceptual Cycle Generative Adversarial Network-Based Image Enhancement}} for {{Improving Accuracy}} of {{Low Light Image Segmentation}}},
  author = {Cho, Se Woon and Baek, Na Rae and Koo, Ja Hyung and Park, Kang Ryoung},
  year = 2021,
  journal = {IEEE Access},
  volume = {9},
  pages = {6296--6324},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3048366},
  urldate = {2025-06-13},
  abstract = {In recent years, the importance of the semantic segmentation field has been increasingly emphasized because autonomous vehicle and artificial intelligence (AI)-based robot technology are being researched extensively; and methods for accurately recognizing objects are required. Previous state-of-the-art segmentation methods have been proven to be effective for databases obtained during daytime. However, in extremely low light or nighttime environments, the shape and color information of objects are very small or disappear due to an insufficient amount of external light, which makes it difficult to train the segmentation network and significantly degrades performance. In our previous work, segmentation performance in a low light environment was improved using the enhancement-based segmentation method. However, low light images could not be restored precisely and segmentation performance improvement was limited because only per-pixel loss functions were used when training the enhancement network. To overcome these drawbacks, we propose a low light image segmentation method based on a modified perceptual cycle generative adversarial network (CycleGAN). Perceptual image enhancement was performed using our network, which significantly improved segmentation performance. Unlike the existing perceptual loss, the Euclidean distance of the feature maps extracted from the pretrained segmentation network was used. In our experiments, we used low light databases generated from two famous road scene open databases, which are Cambridge-driving Labeled Video Database (CamVid) and Karlsruhe Institute of Technology and Toyota Technological Institute at Chicago (KITTI), and confirmed that our proposed method shows better segmentation performance in extremely low light environments than the existing state-of-the art methods.},
  keywords = {Adversarial networks,Artificial intelligence,Color information,Database systems,Databases,Euclidean distance,Feature extraction,Image color analysis,Image enhancement,Image segmentation,low light,modified perceptual CycleGAN,Motion segmentation,perceptual loss,road scene open database,Robot technology,Segmentation methods,Segmentation performance,Semantic segmentation,Semantics,State-of-the-art methods,Training},
  file = {C:\Users\theun\Zotero\storage\N3YK89HC\Cho et al. - 2021 - Modified Perceptual Cycle Generative Adversarial Network-Based Image Enhancement for Improving Accur.pdf}
}

@article{choSemanticSegmentationLow2020,
  title = {Semantic {{Segmentation With Low Light Images}} by {{Modified CycleGAN-Based Image Enhancement}}},
  author = {Cho, Se Woon and Baek, Na Rae and Koo, Ja Hyung and Arsalan, Muhammad and Park, Kang Ryoung},
  year = 2020,
  journal = {IEEE Access},
  volume = {8},
  pages = {93561--93585},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2994969},
  urldate = {2025-06-13},
  abstract = {In recent years, the importance of semantic segmentation has been widely recognized and the field has been actively studied. The existing state-of-the-art segmentation methods show high performance for bright and clear images. However, in low light or nighttime environments, images are blurred and noise increases due to the nature of the camera sensor, which makes it very difficult to perform segmentation for various objects. For this reason, there are few previous studies on multi-class segmentation in low light or nighttime environments. To address this challenge, we propose a modified cycle generative adversarial network (CycleGAN)-based multi-class segmentation method that improves multi-class segmentation performance for low light images. In this study, we used low light databases generated by two road scene open databases that provide segmentation labels, which are the Cambridge-driving labeled video database (CamVid) and Karlsruhe Institute of Technology and Toyota Technological Institute at Chicago (KITTI) database. Consequently, the proposed method showed superior segmentation performance compared with the other state-of-the-art methods.},
  keywords = {Adversarial networks,Brightness,Cameras,Database systems,Databases,Image color analysis,Image enhancement,Image segmentation,low light,Low-light images,modified CycleGAN,Multi-class segmentations,nighttime,road scene open database,Segmentation methods,Segmentation performance,Semantic segmentation,Semantics,State of the art,State-of-the-art methods,Training},
  file = {C:\Users\theun\Zotero\storage\94P6SEPZ\Cho et al. - 2020 - Semantic Segmentation With Low Light Images by Modified CycleGAN-Based Image Enhancement.pdf}
}

@article{congSRNSDStructureRegularizedNightTime2024,
  title = {{{SRNSD}}: {{Structure-Regularized Night-Time Self-Supervised Monocular Depth Estimation}} for {{Outdoor Scenes}}},
  shorttitle = {{{SRNSD}}},
  author = {Cong, Runmin and Wu, Chunlei and Song, Xibin and Zhang, Wei and Kwong, Sam and Li, Hongdong and Ji, Pan},
  year = 2024,
  journal = {IEEE Transactions on Image Processing},
  volume = {33},
  pages = {5538--5550},
  issn = {1941-0042},
  doi = {10.1109/TIP.2024.3465034},
  urldate = {2025-06-30},
  abstract = {Deep CNNs have achieved impressive improvements for night-time self-supervised depth estimation form a monocular image. However, the performance degrades considerably compared to day-time depth estimation due to significant domain gaps, low visibility, and varying illuminations between day and night images. To address these challenges, we propose a novel night-time self-supervised monocular depth estimation framework with structure regularization, i.e., SRNSD, which incorporates three aspects of constraints for better performance, including feature and depth domain adaptation, image perspective constraint, and cropped multi-scale consistency loss. Specifically, we utilize adaptations of both feature and depth output spaces for better night-time feature extraction and depth map prediction, along with high- and low-frequency decoupling operations for better depth structure and texture recovery. Meanwhile, we employ an image perspective constraint to enhance the smoothness and obtain better depth maps in areas where the luminosity jumps change. Furthermore, we introduce a simple yet effective cropped multi-scale consistency loss that utilizes consistency among different scales of depth outputs for further optimization, refining the detailed textures and structures of predicted depth. Experimental results on different benchmarks with depth ranges of 40m and 60m, including Oxford RobotCar dataset, nuScenes dataset and CARLA-EPE dataset, demonstrate the superiority of our approach over state-of-the-art night-time self-supervised depth estimation approaches across multiple metrics, proving our effectiveness.},
  keywords = {Adaptation models,Domain adaption,Estimation,Feature extraction,Lighting,night-time depth estimation,Periodic structures,Pipelines,Semantic segmentation,structure regularization,Training,Urban areas,Visualization},
  file = {C:\Users\theun\Zotero\storage\ICSJWAPP\Cong et al. - 2024 - SRNSD Structure-Regularized Night-Time Self-Supervised Monocular Depth Estimation for Outdoor Scene.pdf}
}

@article{ConvolutionalNeuralNetwork2024,
  title = {Convolutional Neural Network},
  year = 2024,
  month = dec,
  journal = {Wikipedia},
  urldate = {2024-12-09},
  abstract = {A convolutional neural network (CNN) is a regularized type of feed-forward neural network that learns features by itself via filter (or kernel) optimization. This type of deep learning network has been applied to process and make predictions from many different types of data including text, images and audio. Convolution-based networks are the de-facto standard in deep learning-based approaches to computer vision and image processing, and have only recently have been replaced -- in some cases -- by newer deep learning architectures such as the transformer. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by using regularized weights over fewer connections. For example, for each neuron in the fully-connected layer, 10,000 weights would be required for processing an image sized 100 \texttimes{} 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels, only 25 neurons are required to process 5x5-sized tiles. Higher-layer features are extracted from wider context windows, compared to lower-layer features. Some applications of CNNs include:  image and video recognition, recommender systems, image classification, image segmentation, medical image analysis, natural language processing, brain--computer interfaces, and financial time series. CNNs are also known as shift invariant or space invariant artificial neural networks, based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps. Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the downsampling operation they apply to the input. Feed-forward neural networks are usually fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The "full connectivity" of these networks makes them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) Robust datasets also increase the probability that CNNs will learn the generalized principles that characterize a given dataset rather than the biases of a poorly-populated set. Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field. CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered. This independence from prior knowledge and human intervention in feature extraction is a major advantage.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  keywords = {/unread},
  annotation = {Page Version ID: 1261565222}
}

@article{cooperAnalysisExtensionsFrankleMcCann2004,
  title = {Analysis and Extensions of the {{Frankle-McCann Retinex}} Algorithm},
  author = {Cooper, Ted J. and Baqai, Farhan A.},
  year = 2004,
  month = jan,
  journal = {Journal of Electronic Imaging},
  volume = {13},
  number = {1},
  pages = {85--92},
  publisher = {SPIE},
  issn = {1017-9909, 1560-229X},
  doi = {10.1117/1.1636182},
  urldate = {2025-05-29},
  abstract = {The Journal of Electronic Imaging publishes papers that are normally considered in the design, engineering, and applications of electronic imaging technologies.},
  file = {C:\Users\theun\Zotero\storage\X5WLY6JL\Cooper and Baqai - 2004 - Analysis and extensions of the Frankle-McCann Retinex algorithm.pdf}
}

@inproceedings{cordtsCityscapesDatasetSemantic2016,
  title = {The {{Cityscapes Dataset}} for {{Semantic Urban Scene Understanding}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
  year = 2016,
  month = jun,
  pages = {3213--3223},
  publisher = {IEEE},
  address = {Las Vegas, NV, USA},
  doi = {10.1109/CVPR.2016.350},
  urldate = {2025-05-15},
  abstract = {Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\8HPASJ5U\Cordts et al. - 2016 - The Cityscapes Dataset for Semantic Urban Scene Understanding.pdf}
}

@inproceedings{cortinhalSalsaNextFastUncertaintyAware2020,
  title = {{{SalsaNext}}: {{Fast}}, {{Uncertainty-Aware Semantic Segmentation}} of {{LiDAR Point Clouds}}},
  shorttitle = {{{SalsaNext}}},
  booktitle = {Advances in {{Visual Computing}}},
  author = {Cortinhal, Tiago and Tzelepis, George and Erdal Aksoy, Eren},
  editor = {Bebis, George and Yin, Zhaozheng and Kim, Edward and Bender, Jan and Subr, Kartic and Kwon, Bum Chul and Zhao, Jian and Kalkofen, Denis and Baciu, George},
  year = 2020,
  pages = {207--222},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-64559-5_16},
  abstract = {In this paper, we introduce SalsaNext for the uncertainty-aware semantic segmentation of a full 3D LiDAR point cloud in real-time. SalsaNext is the next version of SalsaNet~[1] which has an encoder-decoder architecture where the encoder unit has a set of ResNet blocks and the decoder part combines upsampled features from the residual blocks. In contrast to SalsaNet, we introduce a new context module, replace the ResNet encoder blocks with a new residual dilated convolution stack with gradually increasing receptive fields and add the pixel-shuffle layer in the decoder. Additionally, we switch from stride convolution to average pooling and also apply central dropout treatment. To directly optimize the Jaccard index, we further combine the weighted cross entropy loss with Lov\'asz-Softmax loss [4]. We finally inject a Bayesian treatment to compute the epistemic and aleatoric uncertainties for each point in the cloud. We provide a thorough quantitative evaluation on the Semantic-KITTI dataset [3], which demonstrates that the proposed SalsaNext outperforms other published semantic segmentation networks and achieves \$\$3.6\textbackslash\%\$\$3.6\%more accuracy over the previous state-of-the-art method. We also release our source code (https://github.com/TiagoCortinhal/SalsaNext).},
  isbn = {978-3-030-64559-5},
  langid = {english},
  keywords = {Deep learning,LiDAR Point Clouds,Semantic segmentation},
  file = {C:\Users\theun\Zotero\storage\3NHJ489Q\Cortinhal et al. - 2020 - SalsaNext Fast, Uncertainty-Aware Semantic Segmentation of LiDAR Point Clouds.pdf}
}

@article{cosmanCombiningVectorQuantization1992,
  title = {Combining Vector Quantization and Histogram Equalization},
  author = {Cosman, Pamela C. and Riskin, Eve A. and Gray, Robert M.},
  year = 1992,
  month = nov,
  journal = {Information Processing \& Management},
  series = {Special {{Issue}}: {{Data}} Compression for Images and Texts},
  volume = {28},
  number = {6},
  pages = {681--686},
  issn = {0306-4573},
  doi = {10.1016/0306-4573(92)90061-4},
  urldate = {2025-06-03},
  file = {C\:\\Users\\theun\\Zotero\\storage\\MLGMJVBS\\Cosman et al. - 1992 - Combining vector quantization and histogram equalization.pdf;C\:\\Users\\theun\\Zotero\\storage\\AY5CTMAQ\\0306457392900614.html}
}

@article{cuiDeepLearningImage2022,
  title = {Deep {{Learning}} for {{Image}} and {{Point Cloud Fusion}} in {{Autonomous Driving}}: {{A Review}}},
  shorttitle = {Deep {{Learning}} for {{Image}} and {{Point Cloud Fusion}} in {{Autonomous Driving}}},
  author = {Cui, Yaodong and Chen, Ren and Chu, Wenbo and Chen, Long and Tian, Daxin and Li, Ying and Cao, Dongpu},
  year = 2022,
  month = feb,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {23},
  number = {2},
  pages = {722--739},
  issn = {1558-0016},
  doi = {10.1109/TITS.2020.3023541},
  urldate = {2025-01-02},
  abstract = {Autonomous vehicles were experiencing rapid development in the past few years. However, achieving full autonomy is not a trivial task, due to the nature of the complex and dynamic driving environment. Therefore, autonomous vehicles are equipped with a suite of different sensors to ensure robust, accurate environmental perception. In particular, the camera-LiDAR fusion is becoming an emerging research theme. However, so far there has been no critical review that focuses on deep-learning-based camera-LiDAR fusion methods. To bridge this gap and motivate future research, this article devotes to review recent deep-learning-based data fusion approaches that leverage both image and point cloud. This review gives a brief overview of deep learning on image and point cloud data processing. Followed by in-depth reviews of camera-LiDAR fusion methods in depth completion, object detection, semantic segmentation, tracking and online cross-sensor calibration, which are organized based on their respective fusion levels. Furthermore, we compare these methods on publicly available datasets. Finally, we identified gaps and over-looked challenges between current academic researches and real-world applications. Based on these observations, we provide our insights and point out promising research directions.},
  keywords = {/unread,Camera-LiDAR fusion,Convolution,deep learning,Deep learning,depth completion,Feature extraction,Geometry,Laser radar,object detection,semantic segmentation,Semantics,sensor fusion,Three-dimensional displays,tracking},
  file = {C\:\\Users\\theun\\Zotero\\storage\\FYQBEXSY\\Cui et al. - 2022 - Deep Learning for Image and Point Cloud Fusion in Autonomous Driving A Review.pdf;C\:\\Users\\theun\\Zotero\\storage\\CXW9DTIG\\9380166.html}
}

@inproceedings{cuiMultitaskAETOrthogonal2021,
  title = {Multitask {{AET}} with {{Orthogonal Tangent Regularity}} for {{Dark Object Detection}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Cui, Ziteng and Qi, Guo-Jun and Gu, Lin and You, Shaodi and Zhang, Zenghui and Harada, Tatsuya},
  year = 2021,
  month = oct,
  pages = {2533--2542},
  publisher = {IEEE},
  address = {Montreal, QC, Canada},
  doi = {10.1109/ICCV48922.2021.00255},
  urldate = {2025-03-10},
  abstract = {Dark environment becomes a challenge for computer vision algorithms owing to insufficient photons and undesirable noise. To enhance object detection in a dark environment, we propose a novel multitask auto encoding transformation (MAET) model which is able to explore the intrinsic pattern behind illumination translation. In a self-supervision manner, the MAET learns the intrinsic visual structure by encoding and decoding the realistic illumination-degrading transformation considering the physical noise model and image signal processing (ISP). Based on this representation, we achieve the object detection task by decoding the bounding box coordinates and classes. To avoid the over-entanglement of two tasks, our MAET disentangles the object and degrading features by imposing an orthogonal tangent regularity. This forms a parametric manifold along which multitask predictions can be geometrically formulated by maximizing the orthogonality between the tangents along the outputs of respective tasks. Our framework can be implemented based on the mainstream object detection architecture and directly trained end-to-end using normal target detection datasets, such as VOC and COCO. We have achieved the state-of-the-art performance using synthetic and real-world datasets. Codes will be released at https://github.com/cuiziteng/MAET.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-2812-5},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\H85DRT8B\Cui et al. - 2021 - Multitask AET with Orthogonal Tangent Regularity for Dark Object Detection.pdf}
}

@misc{cuiYouOnlyNeed2022,
  title = {You {{Only Need 90K Parameters}} to {{Adapt Light}}: {{A Light Weight Transformer}} for {{Image Enhancement}} and {{Exposure Correction}}},
  shorttitle = {You {{Only Need 90K Parameters}} to {{Adapt Light}}},
  author = {Cui, Ziteng and Li, Kunchang and Gu, Lin and Su, Shenghan and Gao, Peng and Jiang, Zhengkai and Qiao, Yu and Harada, Tatsuya},
  year = 2022,
  month = oct,
  number = {arXiv:2205.14871},
  eprint = {2205.14871},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.14871},
  urldate = {2025-02-27},
  abstract = {Challenging illumination conditions (low-light, under-exposure and over-exposure) in the real world not only cast an unpleasant visual appearance but also taint the computer vision tasks. After camera captures the raw-RGB data, it renders standard sRGB images with image signal processor (ISP). By decomposing ISP pipeline into local and global image components, we propose a lightweight fast Illumination Adaptive Transformer (IAT) to restore the normal lit sRGB image from either low-light or under/over-exposure conditions. Specifically, IAT uses attention queries to represent and adjust the ISP-related parameters such as colour correction, gamma correction. With only \textasciitilde 90k parameters and \textasciitilde 0.004s processing speed, our IAT consistently achieves superior performance over SOTA on the current benchmark low-light enhancement and exposure correction datasets. Competitive experimental performance also demonstrates that our IAT significantly enhances object detection and semantic segmentation tasks under various light conditions. Training code and pretrained model is available at https://github.com/cuiziteng/Illumination-Adaptive-Transformer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\6U6RVHMN\\Cui et al. - 2022 - You Only Need 90K Parameters to Adapt Light A Light Weight Transformer for Image Enhancement and Ex.pdf;C\:\\Users\\theun\\Zotero\\storage\\3MDRLLQ5\\2205.html}
}

@misc{CVPR2022Open,
  title = {{{CVPR}} 2022 {{Open Access Repository}}},
  urldate = {2025-02-27},
  howpublished = {https://openaccess.thecvf.com/content/CVPR2022/html/Ma\_Toward\_Fast\_Flexible\_and\_Robust\_Low-Light\_Image\_Enhancement\_CVPR\_2022\_paper.html?ref=https://githubhelp.com},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\82KN58Q4\Ma_Toward_Fast_Flexible_and_Robust_Low-Light_Image_Enhancement_CVPR_2022_paper.html}
}

@inproceedings{daiDarkModelAdaptation2018,
  title = {Dark {{Model Adaptation}}: {{Semantic Image Segmentation}} from {{Daytime}} to {{Nighttime}}},
  shorttitle = {Dark {{Model Adaptation}}},
  booktitle = {2018 21st {{International Conference}} on {{Intelligent Transportation Systems}} ({{ITSC}})},
  author = {Dai, Dengxin and Van Gool, Luc},
  year = 2018,
  month = nov,
  pages = {3819--3824},
  issn = {2153-0017},
  doi = {10.1109/ITSC.2018.8569387},
  urldate = {2025-06-03},
  abstract = {This work addresses the problem of semantic image segmentation of nighttime scenes. Although considerable progress has been made in semantic image segmentation, it is mainly related to daytime scenarios. This paper proposes a novel method to progressive adapt the semantic models trained on daytime scenes, along with large-scale annotations therein, to nighttime scenes via the bridge of twilight time - the time between dawn and sunrise, or between sunset and dusk. The goal of the method is to alleviate the cost of human annotation for nighttime images by transferring knowledge from standard daytime conditions. In addition to the method, a new dataset of road scenes is compiled; it consists of 35,000 images ranging from daytime to twilight time and to nighttime. Also, a subset of the nighttime images are densely annotated for method evaluation. Our experiments show that our method is effective for knowledge transfer from daytime scenes to nighttime scenes, without human annotation.},
  keywords = {Adaptation models,Cameras,Image segmentation,Mathematical model,Meteorology,Roads,Semantics},
  file = {C:\Users\theun\Zotero\storage\MKGNQV7B\Dai and Gool - 2018 - Dark Model Adaptation Semantic Image Segmentation from Daytime to Nighttime.pdf}
}

@inproceedings{daiDeformableConvolutionalNetworks2017,
  title = {Deformable {{Convolutional Networks}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},
  year = 2017,
  pages = {764--773},
  urldate = {2025-06-20},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\YHDP9GSD\Dai et al. - 2017 - Deformable Convolutional Networks.pdf}
}

@article{daiMultiexposureImageEnhancement2026,
  title = {Multi-Exposure Image Enhancement and {{YOLO}} Integration for Nighttime Pedestrian Detection},
  author = {Dai, Xiaobiao and Lan, Junbo and Chen, Zhigang and Wang, Botao and Wen, Xue},
  year = 2026,
  month = jan,
  journal = {Signal Processing: Image Communication},
  volume = {140},
  pages = {117421},
  issn = {0923-5965},
  doi = {10.1016/j.image.2025.117421},
  urldate = {2025-10-17},
  abstract = {This paper presents DCExYOLO, a novel method integrating multi-exposure image enhancement with YOLO object detection for real-time pedestrian detection in nighttime driving scenarios. To address the challenges of uneven illumination and low-light conditions in nighttime driving scenarios, we introduce an improved Zero-DCE++ algorithm to generate enhanced images at multiple exposure levels, which are then combined with the original image as input to the YOLO detector. The method significantly enhances the synergistic effect between image enhancement and object detection through the design of multi-task loss functions and a two-stage optimization strategy. Extensive experiments on multiple datasets demonstrate that DCExYOLO achieves an optimal balance between detection performance and efficiency, significantly reducing the log-average miss rate (MR-2) compared to the YOLO baseline. Therefore, this research validates the potential of multi-exposure enhancement in object detection under complex illumination environments, providing an efficient and reliable solution for intelligent driving and traffic safety, while establishing a foundation for future optimization of detection technologies in complex scenarios.},
  keywords = {/unread,Image enhancement,Intelligent vehicles,Object detection,Pedestrian detection,Real-time vision},
  file = {C\:\\Users\\theun\\Zotero\\storage\\IHWQQJTC\\Dai et al. - 2026 - Multi-exposure image enhancement and YOLO integration for nighttime pedestrian detection.pdf;C\:\\Users\\theun\\Zotero\\storage\\YPP7VW3E\\S0923596525001675.html}
}

@misc{DecoupledCrossScaleCrossView,
  title = {Decoupled {{Cross-Scale Cross-View Interaction}} for {{Stereo Image Enhancement}} in the {{Dark}} \textbar{} {{Proceedings}} of the 31st {{ACM International Conference}} on {{Multimedia}}},
  urldate = {2025-03-28},
  howpublished = {https://dl-acm-org.ezproxy-b.deakin.edu.au/doi/abs/10.1145/3581783.3611962},
  keywords = {/unread}
}

@article{deng5KEfficientLowLight2024,
  title = {A {{5K Efficient Low-Light Enhancement Model}} by {{Estimating Increment}} between {{Dark Image}} and {{Transmission Map Based}} on {{Local Maximum Color Value Prior}}},
  author = {Deng, Qikang and Choo, Dongwon and Ji, Hyochul and Lee, Dohoon},
  year = 2024,
  journal = {Electronics},
  volume = {13},
  number = {10},
  pages = {1814},
  publisher = {MDPI AG},
  address = {Basel, Switzerland},
  doi = {10.3390/electronics13101814},
  urldate = {2025-01-19},
  abstract = {Low-light enhancement (LLE) has seen significant advancements over decades, leading to substantial improvements in image quality that even surpass ground truth. However, these advancements have come with a downside as the models grew in size and complexity, losing their lightweight and real-time capabilities crucial for applications like surveillance, autonomous driving, smartphones, and unmanned aerial vehicles (UAVs). To address this challenge, we propose an exceptionally lightweight model with just around 5K parameters, which is capable of delivering high-quality LLE results. Our method focuses on estimating the incremental changes from dark images to transmission maps based on the low maximum color value prior, and we introduce a novel three-channel transmission map to capture more details and information compared to the traditional one-channel transmission map. This innovative design allows for more effective matching of incremental estimation results, enabling distinct transmission adjustments to be applied to the R, G, and B channels of the image. This streamlined approach ensures that our model remains lightweight, making it suitable for deployment on low-performance devices without compromising real-time performance. Our experiments confirm the effectiveness of our model, achieving high-quality LLE comparable to the IAT (local) model. Impressively, our model achieves this level of performance while utilizing only 0.512 GFLOPs and 4.7K parameters, representing just 39.1\% of the GFLOPs and 23.5\% of the parameters used by the IAT (local) model.},
  copyright = {\copyright{} 2024 by the authors.  Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).  Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
  langid = {english},
  keywords = {Accuracy,Algorithms,Color,Deep learning,Efficiency,Estimation,Ground truth,Image enhancement,Image quality,Image transmission,Light,lightweight,Lightweight,local maximum color value prior,low light enhancement,low-performance devices,Maps,Methods,Parameters,real time,Real time,surveillance video,transmission map estimation,Unmanned aerial vehicles,Weight reduction},
  file = {C:\Users\theun\Zotero\storage\8YHPR5GH\Deng et al. - 2024 - A 5K Efficient Low-Light Enhancement Model by Estimating Increment between Dark Image and Transmissi.pdf}
}

@article{dengCooperativeCollisionAvoidance2019,
  title = {Cooperative {{Collision Avoidance}} for {{Overtaking Maneuvers}} in {{Cellular V2X-Based Autonomous Driving}}},
  author = {Deng, Ruoqi and Di, Boya and Song, Lingyang},
  year = 2019,
  month = may,
  journal = {IEEE Transactions on Vehicular Technology},
  volume = {68},
  number = {5},
  pages = {4434--4446},
  issn = {1939-9359},
  doi = {10.1109/TVT.2019.2906509},
  urldate = {2025-01-02},
  abstract = {In this paper, we consider a cooperative autonomous driving system where a vehicle overtakes the one in front based on collective perception. To avoid collisions with vehicles on the other lane, we propose a V2X-based cooperative collision avoidance scheme. The overtaking vehicle estimates the distance between itself and the neighbors via V2V communications and decides whether to overtake or not. Two cases where the distance information is obtained independently and cooperatively are taken into account. We derive the probability of collision avoidance and analyze the influence of different factors such as speed and density of vehicles on the system performance. Simulation results verify our analysis and show that our V2X-based cooperative collision avoidance scheme performs better than traditional GNSS-based collision avoidance scheme. The performance gain brought by the cooperative case compared to the independent case in our scheme can also be observed.},
  keywords = {/unread,Automated driving,Autonomous vehicles,Collision avoidance,cooperative collision avoidance,Interference,Simulation,Time-frequency analysis,V2X,Vehicle-to-everything,Vehicular ad hoc networks},
  file = {C\:\\Users\\theun\\Zotero\\storage\\MVAK675J\\Deng et al. - 2019 - Cooperative Collision Avoidance for Overtaking Maneuvers in Cellular V2X-Based Autonomous Driving.pdf;C\:\\Users\\theun\\Zotero\\storage\\4HTPRRJC\\8671732.html}
}

@inproceedings{dengNightLabDualLevelArchitecture2022,
  title = {{{NightLab}}: {{A Dual-Level Architecture With Hardness Detection}} for {{Segmentation}} at {{Night}}},
  shorttitle = {{{NightLab}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Xueqing and Wang, Peng and Lian, Xiaochen and Newsam, Shawn},
  year = 2022,
  pages = {16938--16948},
  urldate = {2025-06-16},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\QK3T59BR\Deng et al. - 2022 - NightLab A Dual-Level Architecture With Hardness Detection for Segmentation at Night.pdf}
}

@article{dequeirozmendesDeepLearningTechniques2021,
  title = {On Deep Learning Techniques to Boost Monocular Depth Estimation for Autonomous Navigation},
  author = {{de Queiroz Mendes}, Raul and Ribeiro, Eduardo Godinho and {dos Santos Rosa}, Nicolas and Grassi, Valdir},
  year = 2021,
  month = feb,
  journal = {Robotics and Autonomous Systems},
  volume = {136},
  pages = {103701},
  issn = {0921-8890},
  doi = {10.1016/j.robot.2020.103701},
  urldate = {2025-06-30},
  abstract = {Inferring the depth of images is a fundamental inverse problem within the field of Computer Vision since depth information is obtained through 2D images, which can be generated from infinite possibilities of observed real scenes. Benefiting from the progress of Convolutional Neural Networks (CNNs) to explore structural features and spatial image information, Single Image Depth Estimation (SIDE) is often highlighted in scopes of scientific and technological innovation, as this concept provides advantages related to its low implementation cost and robustness to environmental conditions. In the context of autonomous vehicles, state-of-the-art CNNs optimize the SIDE task by producing high-quality depth maps, which are essential during the autonomous navigation process in different locations. However, such networks are usually supervised by sparse and noisy depth data, from Light Detection and Ranging (LiDAR) laser scans, and are carried out at high computational cost, requiring high-performance Graphic Processing Units (GPUs). Therefore, we propose a new lightweight and fast supervised CNN architecture combined with novel feature extraction models which are designed for real-world autonomous navigation. We also introduce an efficient surface normals module, jointly with a simple geometric 2.5D loss function, to solve SIDE problems. We also innovate by incorporating multiple Deep Learning techniques, such as the use of densification algorithms and additional semantic, surface normals and depth information to train our framework. The method introduced in this work focuses on robotic applications in indoor and outdoor environments and its results are evaluated on the competitive and publicly available NYU Depth V2 and KITTI Depth datasets.},
  keywords = {CNN,Deep learning,SIDE},
  file = {C\:\\Users\\theun\\Zotero\\storage\\T5DBB8MZ\\de Queiroz Mendes et al. - 2021 - On deep learning techniques to boost monocular depth estimation for autonomous navigation.pdf;C\:\\Users\\theun\\Zotero\\storage\\2DWGPNJT\\S0921889020305418.html}
}

@article{dhalHistogramEqualizationVariants2021,
  title = {Histogram {{Equalization Variants}} as {{Optimization Problems}}: {{A Review}}},
  shorttitle = {Histogram {{Equalization Variants}} as {{Optimization Problems}}},
  author = {Dhal, Krishna Gopal and Das, Arunita and Ray, Swarnajit and G{\'a}lvez, Jorge and Das, Sanjoy},
  year = 2021,
  month = may,
  journal = {Archives of Computational Methods in Engineering},
  volume = {28},
  number = {3},
  pages = {1471--1496},
  issn = {1886-1784},
  doi = {10.1007/s11831-020-09425-1},
  urldate = {2025-05-28},
  abstract = {In the consumer electronics field, the main challenge in image processing is to preserve the original brightness. Histogram Equalization (HE) is one of the simplest and widely used methods for contrast enhancement. However, HE does not suit into the consumer electronics field as this procedure flattens the histogram by distributing the entire gray levels uniformly. Therefore, several HE variants have been proposed based on proper histogram segmentation, histogram weighting, and range optimization techniques to overcome this flattening effect. However, sometimes these modifications become complex and computationally expensive. Recently, researchers have formulated the HE variants for image enhancement as optimization problems and solved, using Nature-Inspired Optimization Algorithms (NIOA), which starts a new era in the image enhancement field. This study presents an up-to-date review over the application of NIOAs for HE variants in image enhancement domain. The main issues which are involved in the application of NIOAs with HE are also discussed here.},
  langid = {english},
  keywords = {Algorithms,Calculus of Variations and Optimization,Continuous Optimization,Histone variants,Image Processing,Optimization},
  file = {C:\Users\theun\Zotero\storage\A62SEUPZ\Dhal et al. - 2021 - Histogram Equalization Variants as Optimization Problems A Review.pdf}
}

@article{dhanachandraImageSegmentationUsing2015,
  title = {Image {{Segmentation Using}} {{{\emph{K}}}} -Means {{Clustering Algorithm}} and {{Subtractive Clustering Algorithm}}},
  author = {Dhanachandra, Nameirakpam and Manglem, Khumanthem and Chanu, Yambem Jina},
  year = 2015,
  month = jan,
  journal = {Procedia Computer Science},
  series = {Eleventh {{International Conference}} on {{Communication Networks}}, {{ICCN}} 2015, {{August}} 21-23, 2015, {{Bangalore}}, {{India Eleventh International Conference}} on {{Data Mining}} and {{Warehousing}}, {{ICDMW}} 2015, {{August}} 21-23, 2015, {{Bangalore}}, {{India Eleventh International Conference}} on {{Image}} and {{Signal Processing}}, {{ICISP}} 2015, {{August}} 21-23, 2015, {{Bangalore}}, {{India}}},
  volume = {54},
  pages = {764--771},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2015.06.090},
  urldate = {2025-06-06},
  abstract = {Image segmentation is the classification of an image into different groups. Many researches have been done in the area of image segmentation using clustering. There are different methods and one of the most popular methods is k-means clustering algorithm. K -means clustering algorithm is an unsupervised algorithm and it is used to segment the interest area from the background. But before applying K -means algorithm, first partial stretching enhancement is applied to the image to improve the quality of the image. Subtractive clustering method is data clustering method where it generates the centroid based on the potential value of the data points. So subtractive cluster is used to generate the initial centers and these centers are used in k-means algorithm for the segmentation of image. Then finally medial filter is applied to the segmented image to remove any unwanted region from the image.},
  keywords = {-means clustering,/unread,Image segmentation,Median filter,Partial contrast stretching,Subtractive clustering.},
  file = {C:\Users\theun\Zotero\storage\BTUEPZ8I\Dhanachandra et al. - 2015 - Image Segmentation Using K -means Clustering Algorithm and Subtractive Clustering Algorithm.pdf}
}

@article{diSurveyAutonomousVehicle2021,
  title = {A Survey on Autonomous Vehicle Control in the Era of Mixed-Autonomy: {{From}} Physics-Based to {{AI-guided}} Driving Policy Learning},
  shorttitle = {A Survey on Autonomous Vehicle Control in the Era of Mixed-Autonomy},
  author = {Di, Xuan and Shi, Rongye},
  year = 2021,
  month = apr,
  journal = {Transportation Research Part C: Emerging Technologies},
  volume = {125},
  pages = {103008},
  issn = {0968-090X},
  doi = {10.1016/j.trc.2021.103008},
  urldate = {2025-01-09},
  abstract = {This paper serves as an introduction and overview of the potentially useful models and methodologies from artificial intelligence (AI) into the field of transportation engineering for autonomous vehicle (AV) control in the era of mixed autonomy when AVs drive alongside human-driven vehicles (HV). It is the first-of-its-kind survey paper to comprehensively review literature in both transportation engineering and AI for mixed traffic modeling. We will discuss state-of-the-art applications of AI-guided methods, identify opportunities and obstacles, and raise open questions. We divide the stage of AV deployment into four phases: the pure HVs, the HV-dominated, the AV-dominated, and the pure AVs. This paper is primarily focused on the latter three phases. Models used for each phase are summarized, encompassing game theory, deep (reinforcement) learning, and imitation learning. While reviewing the methodologies, we primarily focus on the following research questions: (1) What scalable driving policies are to control a large number of AVs in mixed traffic comprised of human drivers and uncontrollable AVs? (2) How do we estimate human driver behaviors? (3) How should the driving behavior of uncontrollable AVs be modeled in the environment? (4) How are the interactions between human drivers and autonomous vehicles characterized? We also provide a list of public datasets and simulation software related to AVs. Hopefully this paper will not only inspire our transportation community to rethink the conventional models that are developed in the data-shortage era, but also start conversations with other disciplines, in particular robotics and machine learning, to join forces towards creating a safe and efficient mixed traffic ecosystem.},
  keywords = {Artificial intelligence (AI),Autonomous vehicle (AV) control,Mixed autonomy},
  file = {C\:\\Users\\theun\\Zotero\\storage\\RXGNYEPZ\\Di and Shi - 2021 - A survey on autonomous vehicle control in the era of mixed-autonomy From physics-based to AI-guided.pdf;C\:\\Users\\theun\\Zotero\\storage\\IUKB9LVG\\S0968090X21000401.html}
}

@misc{dongApplicationsComputerVision2024,
  title = {Applications of {{Computer Vision}} in {{Autonomous Vehicles}}: {{Methods}}, {{Challenges}} and {{Future Directions}}},
  shorttitle = {Applications of {{Computer Vision}} in {{Autonomous Vehicles}}},
  author = {Dong, Xingshuai and Cappuccio, Massimiliano L.},
  year = 2024,
  month = jun,
  number = {arXiv:2311.09093},
  eprint = {2311.09093},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.09093},
  urldate = {2024-12-04},
  abstract = {Autonomous vehicle refers to a vehicle capable of perceiving its surrounding environment and driving with little or no human driver input. The perception system is a fundamental component which enables the autonomous vehicle to gather data and extract essential information from its surrounding to ensure safe driving. Benefiting from recent advances in computer vision, the perception task can be achieved using sensors like cameras, LiDAR, radar, and ultrasonic sensors. This paper reviews publications on computer vision and autonomous driving that are published during the last ten years. In particular, we first investigate the evolution of autonomous driving systems and summarize systems developed by major automotive manufacturers from different countries. Second, we investigate the sensors and benchmark data sets that are commonly utilized for autonomous driving. Then, a comprehensive overview of computer vision applications for autonomous driving such as depth estimation, object detection, lane detection, and traffic sign recognition are discussed. Moreover, we review public opinions and concerns on autonomous vehicles. Based on the discussion, we analyze the current technological challenges that autonomous vehicles face. Finally, we present our insights and point out some promising directions for future research. This paper will help the reader understand autonomous vehicles from both academic and industry perspectives.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C:\Users\theun\Zotero\storage\C8NAL74C\Dong and Cappuccio - 2024 - Applications of Computer Vision in Autonomous Vehicles Methods, Challenges and Future Directions.pdf}
}

@article{dongLowLightingImage2016,
  title = {Low Lighting Image Enhancement Using Local Maximum Color Value Prior},
  author = {Dong, Xuan and Wen, Jiangtao},
  year = 2016,
  month = feb,
  journal = {Frontiers of Computer Science},
  volume = {10},
  number = {1},
  pages = {147--156},
  publisher = {Higher Education Press},
  issn = {2095-2236},
  doi = {10.1007/s11704-015-4353-1},
  urldate = {2025-02-21},
  abstract = {We study the problem of low lighting image enhancement. Previous enhancement methods for images under low lighting conditions usually fail to consider the factor of image degradation during image formation. As a result, the lost contrast could not be recovered after enhancement. This paper will adaptively recover the contrast and adjust the exposure for low lighting images. Our first contribution is the modeling of image degradation in low lighting conditions. Then, the local maximum color value prior is proposed, i.e., in most regions of well exposed images, the local maximum color value of a pixel will be very high. By combining the image degradation model and local maximum color value prior, we propose to recover the un-degraded images under low lighting conditions. Last, an adaptive exposure adjustment module is proposed to obtain the final result. We show that our approach enables better enhancement comparing with popular image editing tools and academic algorithms.},
  copyright = {2016 Higher Education Press and Springer-Verlag Berlin Heidelberg},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\84W8532F\Dong and Wen - 2016 - Low lighting image enhancement using local maximum color value prior.pdf}
}

@misc{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = 2021,
  month = jun,
  number = {arXiv:2010.11929},
  eprint = {2010.11929},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.11929},
  urldate = {2025-10-30},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\theun\\Zotero\\storage\\FCAW4U6R\\Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf;C\:\\Users\\theun\\Zotero\\storage\\M3UK7QV3\\2010.html}
}

@inproceedings{duanNovelHistogramProcessing2004,
  title = {Novel Histogram Processing for Colour Image Enhancement},
  booktitle = {Third {{International Conference}} on {{Image}} and {{Graphics}} ({{ICIG}}'04)},
  author = {Duan, Jiang and Qiu, Guping},
  year = 2004,
  month = dec,
  pages = {55--58},
  doi = {10.1109/ICIG.2004.105},
  urldate = {2025-05-27},
  abstract = {In practice, the histogram equalization often produces images with unnatural appearances and visually disturbing artefacts. One of the reasons for these unwanted effects is that the histogram equalization attempts to force the output image to have a uniform pixel distribution regardless of what the original image's pixel distribution may be. In this paper, we present a novel histogram processing algorithm which takes into account the original image's pixel distribution in the equalization process. The method uses a single parameter to control the degree of contrast enhancement to ensure that the output have an enhanced appearance which is also faithful to that of the original image and is free of unwanted visually disturbing artefacts. We first develop the algorithm for the luminance channel and then extend the method to the colour components. We present experimental results to demonstrate the better performances of our new method over established methods in the literature.},
  keywords = {Color,Computer science,Distribution functions,Equations,Frequency,Histograms,Image enhancement,Image processing,Pixel,Visualization},
  file = {C:\Users\theun\Zotero\storage\5KFBQM6X\Duan and Qiu - 2004 - Novel histogram processing for colour image enhancement.pdf}
}

@article{dubeyCalculatingDepthDark2025,
  title = {Calculating {{Depth}} for {{Dark Images Using Self-Supervised Learning}}: {{An Investigation}} of the {{Role}} of {{Edge Maps}} as {{Input}}},
  shorttitle = {Calculating {{Depth}} for {{Dark Images Using Self-Supervised Learning}}},
  author = {Dubey, Shwetang and Watts, Satvik and Malhotra, Ashok and Prajapati, Kunal and Chakraborty, Pavan},
  year = 2025,
  journal = {IEEE Access},
  volume = {13},
  pages = {58710--58721},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2025.3556610},
  urldate = {2025-06-30},
  abstract = {Depth estimation from a single image is a challenging problem that has received significant attention recently. In low-light conditions and with multiple light sources where the pixel intensities are not uniform, the problem becomes even more difficult due to the lack of visual information in the image. In response to the lack of suitable datasets that adequately represent these specific issues, we undertook the task of collecting our own dataset. This initiative was driven by the need for a more accurate and comprehensive representation of the problem space. Leveraging this custom dataset, which we call the dark depth dataset, we developed and applied a novel self-supervised approach to effectively address the problem. In this paper, we propose a novel method for depth estimation in low-light conditions that utilizes edge information. We have used edge information since they are prominent even in low-light conditions and gives a better understanding of the scene than simple images. Our approach is based on the self-supervised monocular depth estimation framework and extends it by incorporating edge information for improved accuracy. We have tested the edge input at different network parts to see the effect on accuracy. With this research, edge information is crucial for depth estimation as most of the points in an object are usually at the same distance, and edges can give boundaries to identify objects. However, the edge information gets lost during deep learning processes while going through different layers, so we provided the edge map at different layers to check where this edge information is most effective for the depth estimation process. Our experiment shows that giving edge information with edge-concatenation gives the best results with dark dataset. We achieved an improvement of nearly 50\% over the base model in terms of \textbackslash delta values.},
  keywords = {Accuracy,Annotations,Cameras,Computer vision,deep learning,Deep learning,Depth measurement,edge information,Image edge detection,Light sources,Lighting,low light image,night time depth estimation,self-supervised depth estimation,supervised depth estimation,Training,Training data},
  file = {C:\Users\theun\Zotero\storage\AMNCZ4IP\Dubey et al. - 2025 - Calculating Depth for Dark Images Using Self-Supervised Learning An Investigation of the Role of Ed.pdf}
}

@inproceedings{dubeyMonocularDepthEstimation2024,
  title = {Monocular {{Depth Estimation}} for {{Dark Images Using Asymmetric GAN}} and {{MIM Based Model}}},
  booktitle = {2024 15th {{International Conference}} on {{Computing Communication}} and {{Networking Technologies}} ({{ICCCNT}})},
  author = {Dubey, Shwetang and Bhimte, Sumit and Chakraborty, Pavan},
  year = 2024,
  month = jun,
  pages = {1--6},
  issn = {2473-7674},
  doi = {10.1109/ICCCNT61001.2024.10724638},
  urldate = {2025-06-30},
  abstract = {The task of estimating depth from a single image is quite difficult and has attracted a lot of attention lately. This problem is exacerbated when low light levels and several light sources lead to uneven pixel intensities throughout the picture. These circumstances greatly reduce the available visual information, increasing the complexity of depth estimates. In our paper, we made an attempt to solve a problem using deep learning techniques. Specifically, we worked on transforming dark images into well-lit ones using an Asymmetric GAN. We then used a MIM-based depth estimation technique to obtain a depth map from the resulting daytime image. As changes in lighting conditions do not affect depth, we achieved good results. We used our own dataset, including RGB and depth images captured in low-light conditions, as well as the NYUv2 dataset for indoor images, to train and test our model.},
  keywords = {Complexity theory,Computational modeling,Deep learning,Depth estimation,Generative adversarial networks,Light sources,Lighting,Masked Image Modeling,Supervised learning,Swin Transformer,Transformers,Visualization},
  file = {C:\Users\theun\Zotero\storage\VEQNF2VA\Dubey et al. - 2024 - Monocular Depth Estimation for Dark Images Using Asymmetric GAN and MIM Based Model.pdf}
}

@inproceedings{eigenDepthMapPrediction2014,
  title = {Depth {{Map Prediction}} from a {{Single Image}} Using a {{Multi-Scale Deep Network}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Eigen, David and Puhrsch, Christian and Fergus, Rob},
  year = 2014,
  volume = {27},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-07-03},
  abstract = {Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation.},
  file = {C:\Users\theun\Zotero\storage\B3U9UUIR\Eigen et al. - 2014 - Depth Map Prediction from a Single Image using a Multi-Scale Deep Network.pdf}
}

@article{eilertsenHDRImageReconstruction2017,
  title = {{{HDR}} Image Reconstruction from a Single Exposure Using Deep {{CNNs}}},
  author = {Eilertsen, Gabriel and Kronander, Joel and Denes, Gyorgy and Mantiuk, Rafa{\l} K. and Unger, Jonas},
  year = 2017,
  month = nov,
  journal = {ACM Trans. Graph.},
  volume = {36},
  number = {6},
  pages = {178:1--178:15},
  issn = {0730-0301},
  doi = {10.1145/3130800.3130816},
  urldate = {2025-01-18},
  abstract = {Camera sensors can only capture a limited range of luminance simultaneously, and in order to create high dynamic range (HDR) images a set of different exposures are typically combined. In this paper we address the problem of predicting information that have been lost in saturated image areas, in order to enable HDR reconstruction from a single exposure. We show that this problem is well-suited for deep learning algorithms, and propose a deep convolutional neural network (CNN) that is specifically designed taking into account the challenges in predicting HDR values. To train the CNN we gather a large dataset of HDR images, which we augment by simulating sensor saturation for a range of cameras. To further boost robustness, we pre-train the CNN on a simulated HDR dataset created from a subset of the MIT Places database. We demonstrate that our approach can reconstruct high-resolution visually convincing HDR results in a wide range of situations, and that it generalizes well to reconstruction of images captured with arbitrary and low-end cameras that use unknown camera response functions and post-processing. Furthermore, we compare to existing methods for HDR expansion, and show high quality results also for image based lighting. Finally, we evaluate the results in a subjective experiment performed on an HDR display. This shows that the reconstructed HDR images are visually convincing, with large improvements as compared to existing methods.},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\UNSGSIH3\Eilertsen et al. - 2017 - HDR image reconstruction from a single exposure using deep CNNs.pdf}
}

@inproceedings{el-shairSIDStereoImage2024,
  title = {{{SID}}: {{Stereo Image Dataset}} for {{Autonomous Driving}} in {{Adverse Conditions}}},
  shorttitle = {{{SID}}},
  booktitle = {{{NAECON}} 2024 - {{IEEE National Aerospace}} and {{Electronics Conference}}},
  author = {{El-Shair}, Zaid A. and {Abu-raddaha}, Abdalmalek and Cofield, Aaron and Alawneh, Hisham and Aladem, Mohamed and Hamzeh, Yazan and Rawashdeh, Samir A.},
  year = 2024,
  month = jul,
  pages = {403--408},
  issn = {2379-2027},
  doi = {10.1109/NAECON61878.2024.10670659},
  urldate = {2025-05-06},
  abstract = {Robust perception is critical for autonomous driving, especially under adverse weather and lighting conditions that commonly occur in real-world environments. In this paper, we introduce the Stereo Image Dataset (SID), a large-scale stereo-image dataset that captures a wide spectrum of challenging real-world environmental scenarios. Recorded at a rate of 20 Hz using a ZED stereo camera mounted on a vehicle, SID consists of 27 sequences totaling over 178k stereo image pairs that showcase conditions from clear skies to heavy snow, captured during the day, dusk, and night. The dataset includes detailed sequence-level annotations for weather conditions, time of day, location, and road conditions, along with instances of camera lens soiling, offering a realistic representation of the challenges in autonomous navigation. Our work aims to address a notable gap in research for autonomous driving systems by presenting high-fidelity stereo images essential for the development and testing of advanced perception algorithms. These algorithms support consistent and reliable operation across variable weather and lighting conditions, even when handling challenging situations like lens soiling. SID is publicly available at: https://doi-org.ezproxy-b.deakin.edu.au/10.7302/esz6-nv83.},
  keywords = {Adverse Weather,Annotations,Autonomous Driving,Cameras,Computer Vision,Lighting,Object detection,Perception Algorithms,Reliability,Roads,Snow,Stereo Image Dataset},
  file = {C:\Users\theun\Zotero\storage\BDQR7366\El-Shair et al. - 2024 - SID Stereo Image Dataset for Autonomous Driving in Adverse Conditions.pdf}
}

@article{ellmauthalerVisiblelightInfraredVideo2019,
  title = {A Visible-Light and Infrared Video Database for Performance Evaluation of Video/Image Fusion Methods},
  author = {Ellmauthaler, Andreas and Pagliari, Carla L. and {da Silva}, Eduardo A. B. and Gois, Jonathan N. and Neves, Sergio R.},
  year = 2019,
  month = jan,
  journal = {Multidimensional Systems and Signal Processing},
  volume = {30},
  number = {1},
  pages = {119--143},
  publisher = {Springer US},
  issn = {1573-0824},
  doi = {10.1007/s11045-017-0548-y},
  urldate = {2025-05-15},
  abstract = {In general, the fusion of visible-light and infrared images produces a composite representation where both data are pictured in a single image. The successful development of image/video fusion algorithms relies on realistic infrared/visible-light datasets. To the best of our knowledge, there is a particular shortage of databases with registered and synchronized videos from the infrared and visible-light spectra suitable for image/video fusion research. To address this need we recorded an image/video fusion database using infrared and visible-light cameras under varying illumination conditions. Moreover, different scenarios have been defined to better challenge the fusion methods, with various contexts and contents providing a wide variety of meaningful data for fusion purposes, including non-planar scenes, where objects appear on different depth planes. However, there are several difficulties in creating datasets for research in infrared/visible-light image fusion. Camera calibration, registration, and synchronization can be listed as important steps of this task. In particular, image registration between imagery from sensors of different spectral bands imposes additional difficulties, as it is very challenging to solve the correspondence problem between such images. Motivated by these challenges, this work introduces a novel spatiotemporal video registration method capable of generating registered and temporally aligned infrared/visible-light video sequences. The proposed workflow improves the registration accuracy when compared to the state-of-the art. By applying the proposed methodology to the recorded database we have generated the visible-light and infrared video database for image fusion, a publicly available database to be used by the research community to test and benchmark fusion schemes.},
  copyright = {2017 Springer Science+Business Media, LLC, part of Springer Nature},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\4HEJBQMU\Ellmauthaler et al. - 2019 - A visible-light and infrared video database for performance evaluation of videoimage fusion methods.pdf}
}

@inproceedings{elmahdyRHRSegNetRelightingHighResolution2024,
  title = {{{RHRSegNet}}: {{Relighting High-Resolution Night-Time Semantic Segmentation}}},
  shorttitle = {{{RHRSegNet}}},
  booktitle = {2024 {{Intelligent Methods}}, {{Systems}}, and {{Applications}} ({{IMSA}})},
  author = {Elmahdy, Sarah and Hebishy, Rodaina and Hamdi, Ali},
  year = 2024,
  month = jul,
  pages = {456--461},
  doi = {10.1109/IMSA61967.2024.10652620},
  urldate = {2025-06-18},
  abstract = {Night-time semantic segmentation is a crucial task in computer vision, focusing on accurately classifying and segmenting objects in low-light conditions. Unlike daytime techniques, which often perform worse in nighttime scenes, it is essential for autonomous driving due to insufficient lighting, low illumination, dynamic lighting, shadow effects, and reduced contrast. We propose RHRSegNet, implementing a relighting model over a High-Resolution Network for semantic segmentation. RHRSeg-Net implements residual convolutional feature learning to handle complex lighting conditions. Our model then feeds the lightened scene feature maps into a high-resolution network for scene segmentation. The network consists of a convolutional producing feature maps with varying resolutions, achieving different levels of resolution through down-sampling and up-sampling. Large nighttime datasets are used for training and evaluation, such as NightCity, City-Scape, and Dark-Zurich datasets. Our proposed model increases the HRnet segmentation performance by 5\% in low-light or nighttime images. The code is available at the following URL: https://github.com/SarahELMAHDY03/RHRSegNet.git.},
  keywords = {Autonomous driving,Computational modeling,Computer vision,Convolutional codes,Dynamic lighting,Economic and social effects,Feature map,High resolution,HTTP,Image coding,Image resolution,Lighting,Low illuminations,Low light conditions,Night time,Relighting,Semantic segmentation,Semantic Segmentation,Shadow effects,Training,Uniform resource locators},
  file = {C:\Users\theun\Zotero\storage\44HH96DA\Elmahdy et al. - 2024 - RHRSegNet Relighting High-Resolution Night-Time Semantic Segmentation.pdf}
}

@article{endlerWritingScientificPapers2015,
  title = {Writing Scientific Papers, with Special Reference to {{Evolutionary Ecology}}},
  author = {Endler, John A.},
  year = 2015,
  month = jul,
  journal = {Evolutionary Ecology},
  volume = {29},
  number = {4},
  pages = {465--478},
  issn = {0269-7653, 1573-8477},
  doi = {10.1007/s10682-015-9773-8},
  urldate = {2025-03-12},
  abstract = {The advancement of science, as well as scientific careers, depends upon good and clear scientific writing. Science is the most democratic of human endeavours because, in principle, anyone can replicate a scientific discovery. In order for this to continue, writing must be clear enough to be understood well enough to allow replication, either in principle or in fact. In this paper I will present data on the publication process in Evolutionary Ecology, use it to illustrate some of the problems in scientific papers, make some general remarks about writing scientific papers, summarise two new paper categories in the journal which will fill gaps that appear to be expanding in the literature, and summarise new journal policies to help mitigate existing problems. Most of the suggestions about writing would apply to any scientific journal.},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\G7HHEXFH\Endler - 2015 - Writing scientific papers, with special reference to Evolutionary Ecology.pdf}
}

@misc{EnhancingLowlightInstance,
  title = {Enhancing Low-Light Instance Segmentation through Feature-Level Denoising},
  urldate = {2025-06-04},
  howpublished = {https://www-spiedigitallibrary-org.ezproxy-b.deakin.edu.au/conference-proceedings-of-spie/13460/134600A/Enhancing-low-light-instance-segmentation-through-feature-level-denoising/10.1117/12.3054001.full},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\EP8DUAED\12.3054001.html}
}

@article{espinosaDetectionMotorcyclesUrban2021,
  title = {Detection of {{Motorcycles}} in {{Urban Traffic Using Video Analysis}}: {{A Review}}},
  shorttitle = {Detection of {{Motorcycles}} in {{Urban Traffic Using Video Analysis}}},
  author = {Espinosa, Jorge E. and Velast{\'i}n, Sergio A. and Branch, John W.},
  year = 2021,
  month = oct,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {22},
  number = {10},
  pages = {6115--6130},
  issn = {1558-0016},
  doi = {10.1109/TITS.2020.2997084},
  urldate = {2025-01-02},
  abstract = {Motorcycles are Vulnerable Road Users (VRU) and as such, in addition to bicycles and pedestrians, they are the traffic actors most affected by accidents in urban areas. Automatic video processing for urban surveillance cameras has the potential to effectively detect and track these road users. The present review focuses on algorithms used for detection and tracking of motorcycles, using the surveillance infrastructure provided by CCTV cameras. Given the importance of results achieved by Deep Learning theory in the field of computer vision, the use of such techniques for detection and tracking of motorcycles is also reviewed. The paper ends by describing the performance measures generally used, publicly available datasets (introducing the Urban Motorbike Dataset (UMD) with quantitative evaluation results for different detectors), discussing the challenges ahead and presenting a set of conclusions with proposed future work in this evolving area.},
  keywords = {/unread,Cameras,computer vision,convolutional neural networks (CNNs),deep learning,Feature extraction,Head,motorcycle detection,Motorcycles,Roads,Safety,Shape,tracking,vehicle detection,Vulnerable road users (VRU)},
  file = {C\:\\Users\\theun\\Zotero\\storage\\MNANQQXX\\Espinosa et al. - 2021 - Detection of Motorcycles in Urban Traffic Using Video Analysis A Review.pdf;C\:\\Users\\theun\\Zotero\\storage\\Q6SZFXN4\\9112620.html}
}

@article{everinghamPascalVisualObject2015,
  title = {The {{Pascal Visual Object Classes Challenge}}: {{A Retrospective}}},
  shorttitle = {The {{Pascal Visual Object Classes Challenge}}},
  author = {Everingham, Mark and Eslami, S. M. Ali and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
  year = 2015,
  month = jan,
  journal = {International Journal of Computer Vision},
  volume = {111},
  number = {1},
  pages = {98--136},
  publisher = {Springer US},
  issn = {1573-1405},
  doi = {10.1007/s11263-014-0733-5},
  urldate = {2025-06-05},
  abstract = {The Pascal Visual Object Classes (VOC) challenge consists of two components: (i) a publicly available dataset of images together with ground truth annotation and standardised evaluation software; and (ii) an annual competition and workshop. There are five challenges: classification, detection, segmentation, action classification, and person layout. In this paper we provide a review of the challenge from 2008--2012. The paper is intended for two audiences: algorithm designers, researchers who want to see what the state of the art is, as measured by performance on the VOC datasets, along with the limitations and weak points of the current generation of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the process and our recommendations for the organisation of future challenges. To analyse the performance of submitted algorithms on the VOC datasets we introduce a number of novel evaluation methods: a bootstrapping method for determining whether differences in the performance of two algorithms are significant or not; a normalised average precision so that performance can be compared across classes with different proportions of positive instances; a clustering method for visualising the performance across multiple algorithms so that the hard and easy images can be identified; and the use of a joint classifier over the submitted algorithms in order to measure their complementarity and combined performance. We also analyse the community's progress through time using the methods of Hoiem et al. (Proceedings of European Conference on Computer Vision, 2012) to identify the types of occurring errors. We conclude the paper with an appraisal of the aspects of the challenge that worked well, and those that could be improved in future challenges.},
  copyright = {2014 Springer Science+Business Media New York},
  langid = {english},
  file = {C\:\\Users\\theun\\Zotero\\storage\\7VJHB3IY\\Everingham et al. - 2015 - The Pascal Visual Object Classes Challenge A Retrospective.pdf;C\:\\Users\\theun\\Zotero\\storage\\XN3DJFXX\\index.html}
}

@article{faisalUnderstandingAutonomousVehicles2019,
  title = {Understanding Autonomous Vehicles: {{A}} Systematic Literature Review on Capability, Impact, Planning and Policy},
  shorttitle = {Understanding Autonomous Vehicles},
  author = {Faisal, Asif and Yigitcanlar, Tan and Kamruzzaman, {\relax Md}. and Currie, Graham},
  year = 2019,
  month = jan,
  journal = {Journal of Transport and Land Use},
  volume = {12},
  number = {1},
  issn = {1938-7849},
  doi = {10.5198/jtlu.2019.1405},
  urldate = {2024-12-04},
  abstract = {Advancement in automated driving technology has created opportunities for smart urban mobility. Automated vehicles are now a popular topic with the rise of the smart city agenda. However, legislators, urban administrators, policymakers, and planners are unprepared to deal with the possible disruption of autonomous vehicles, which potentially could replace conventional transport. There is a lack of knowledge on how the new capabilities will disrupt and which policy strategies are needed to address such disruption. This paper aims to determine where we are, where we are headed, what the likely impacts of a wider uptake could be, and what needs to be done to generate desired smart urban mobility outcomes. The methodology includes a systematic review of the existing evidence base to understand capability, impact, planning, and policy issues associated with autonomous vehicles. The review reveals the trajectories of technological development, disruptive effects caused by such development, strategies to address the disruptions, and possible gaps in the literature. The paper develops a framework outlining the inter-links among driving forces, uptake factors, impacts and possible interventions. It concludes by advocating the necessity of preparing our cities for autonomous vehicles, although a wider uptake may take quite some time.},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\49FSN562\Faisal et al. - 2019 - Understanding autonomous vehicles A systematic literature review on capability, impact, planning an.pdf}
}

@article{fangTrafficAccidentDetection2022,
  title = {Traffic {{Accident Detection}} via {{Self-Supervised Consistency Learning}} in {{Driving Scenarios}}},
  author = {Fang, Jianwu and Qiao, Jiahuan and Bai, Jie and Yu, Hongkai and Xue, Jianru},
  year = 2022,
  month = jul,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {23},
  number = {7},
  pages = {9601--9614},
  issn = {1558-0016},
  doi = {10.1109/TITS.2022.3157254},
  urldate = {2025-01-02},
  abstract = {With the rapid progress of autonomous driving and advanced driver assistance systems, there are growing efforts to promote their safety in natural driving scenarios, especially for the detection of the traffic accidents. However, because of the dynamic camera motion and complex scene in driving situations, traffic accident detection is still challenging. In this work, we aim to give the ability of Traffic Accident Detection for driving systems by proposing a Self-Supervised Consistency learning framework, termed as SSC-TAD, that involves the appearance, motion, and context consistency learning. The key formulation is to find the inconsistency of video frames, object locations and the spatial relation structure of scene temporally between different frames captured by the dashcam videos. Within this field, different from the previous works which concentrate on predicting the future object locations or frames, we further focus on predicting the visual scene context in driving scenarios and detecting the traffic accident by considering the temporal frame consistency, temporal object location consistency, and the spatial-temporal relation consistency of road participants. In this work, this formulation is fulfilled by a collaborative multi-task consistency learning network and the visual scene context feature is represented by a graph convolution network. The superiority to the state-of-the-art is verified by exhaustive evaluations on two large scale datasets, i.e., the AnAn Accident Detection (A3D) dataset and DADA-2000 dataset collected recently.},
  keywords = {/unread,Accidents,adversarial learning,Anomaly detection,Collaboration,frame and location prediction,Multitasking,Roads,scene context,Traffic accident detection,Trajectory,Visualization},
  file = {C\:\\Users\\theun\\Zotero\\storage\\ARM2DH4P\\Fang et al. - 2022 - Traffic Accident Detection via Self-Supervised Consistency Learning in Driving Scenarios.pdf;C\:\\Users\\theun\\Zotero\\storage\\8V69IKCP\\9733965.html}
}

@inproceedings{fanIntegratingSemanticSegmentation2020,
  title = {Integrating {{Semantic Segmentation}} and {{Retinex Model}} for {{Low-Light Image Enhancement}}},
  booktitle = {Proceedings of the 28th {{ACM International Conference}} on {{Multimedia}}},
  author = {Fan, Minhao and Wang, Wenjing and Yang, Wenhan and Liu, Jiaying},
  year = 2020,
  month = oct,
  pages = {2317--2325},
  publisher = {ACM},
  address = {Seattle WA USA},
  doi = {10.1145/3394171.3413757},
  urldate = {2025-05-04},
  isbn = {978-1-4503-7988-5},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\7EG6M7IS\Fan et al. - 2020 - Integrating Semantic Segmentation and Retinex Model for Low-Light Image Enhancement.pdf}
}

@article{fanLACNLightweightAttentionguided2023,
  title = {{{LACN}}: {{A}} Lightweight Attention-Guided {{ConvNeXt}} Network for Low-Light Image Enhancement},
  author = {Fan, S. and Liang, W. and Ding, D. and Yu, H.},
  year = 2023,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {117},
  doi = {10.1016/j.engappai.2022.105632},
  keywords = {/unread}
}

@article{feijooDarkIRRobustLowLight,
  title = {{{DarkIR}}: {{Robust Low-Light Image Restoration}} --- {{Supplementary Material}} ---},
  author = {Feijoo, Daniel and Benito, Juan C and Garcia, Alvaro and Conde, Marcos V},
  langid = {english},
  keywords = {/unread}
}

@inproceedings{feijooDarkIRRobustLowLight2025,
  title = {{{DarkIR}}: {{Robust Low-Light Image Restoration}}},
  shorttitle = {{{DarkIR}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Feijoo, Daniel and Benito, Juan C. and Garcia, Alvaro and Conde, Marcos V.},
  year = 2025,
  pages = {10879--10889},
  urldate = {2025-10-15},
  langid = {english},
  file = {C\:\\Users\\theun\\Zotero\\storage\\EUYZ3G7M\\Feijoo et al. - 2025 - DarkIR Robust Low-Light Image Restoration.pdf;C\:\\Users\\theun\\Zotero\\storage\\YQGYEJXG\\Feijoo et al. - DarkIR Robust Low-Light Image Restoration — Supplementary Material —.pdf}
}

@article{fredrikssonGRIDFASTGridbasedIntersection2024,
  title = {{{GRID-FAST}}: {{A Grid-based Intersection Detection}} for {{Fast Semantic Topometric Mapping}}},
  shorttitle = {{{GRID-FAST}}},
  author = {Fredriksson, Scott and Saradagi, Akshit and Nikolakopoulos, George},
  year = 2024,
  month = oct,
  journal = {Journal of Intelligent \& Robotic Systems},
  volume = {110},
  number = {4},
  pages = {154},
  issn = {1573-0409},
  doi = {10.1007/s10846-024-02180-6},
  urldate = {2024-12-04},
  abstract = {This article introduces a novel approach to constructing a topometric map that allows for efficient navigation and decisionmaking in mobile robotics applications. The method generates the topometric map from a 2D grid-based map. The topometric map segments areas of the input map into different structural-semantic classes: intersections, pathways, dead ends, and pathways leading to unexplored areas. This method is grounded in a new technique for intersection detection that identifies the area and the openings of intersections in a semantically meaningful way. The framework introduces two levels of pre-filtering with minimal computational cost to eliminate small openings and objects from the map which are unimportant in the context of high-level map segmentation and decision making. The topological map generated by GRID-FAST enables fast navigation in large-scale environments, and the structural semantics can aid in mission planning, autonomous exploration, and human-torobot cooperation. The efficacy of the proposed method is demonstrated through validation on real maps gathered from robotic experiments: 1) a structured indoor environment, 2) an unstructured cave-like subterranean environment, and 3) a large-scale outdoor environment, which comprises pathways, buildings, and scattered objects. Additionally, the proposed framework has been compared with state-of-the-art topological mapping solutions and is able to produce a topometric and topological map with up to 92\% fewer nodes than the next best solution. The method proposed in this article has been implemented in the robotics framework ROS and is open-sourced. The code is available at: https://github.com/LTU-RAI/GRID-FAST.},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\UHBZY3VC\Fredriksson et al. - 2024 - GRID-FAST A Grid-based Intersection Detection for Fast Semantic Topometric Mapping.pdf}
}

@misc{fuDualAttentionNetwork2019,
  title = {Dual {{Attention Network}} for {{Scene Segmentation}}},
  author = {Fu, Jun and Liu, Jing and Tian, Haijie and Li, Yong and Bao, Yongjun and Fang, Zhiwei and Lu, Hanqing},
  year = 2019,
  month = apr,
  number = {arXiv:1809.02983},
  eprint = {1809.02983},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1809.02983},
  urldate = {2025-06-28},
  abstract = {In this paper, we address the scene segmentation task by capturing rich contextual dependencies based on the selfattention mechanism. Unlike previous works that capture contexts by multi-scale features fusion, we propose a Dual Attention Networks (DANet) to adaptively integrate local features with their global dependencies. Specifically, we append two types of attention modules on top of traditional dilated FCN, which model the semantic interdependencies in spatial and channel dimensions respectively. The position attention module selectively aggregates the features at each position by a weighted sum of the features at all positions. Similar features would be related to each other regardless of their distances. Meanwhile, the channel attention module selectively emphasizes interdependent channel maps by integrating associated features among all channel maps. We sum the outputs of the two attention modules to further improve feature representation which contributes to more precise segmentation results. We achieve new state-of-the-art segmentation performance on three challenging scene segmentation datasets, i.e., Cityscapes, PASCAL Context and COCO Stuff dataset. In particular, a Mean IoU score of 81.5\% on Cityscapes test set is achieved without using coarse data. We make the code and trained model publicly available at https://github.com/junfu1115/DANet},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\9PH4L7M8\\Fu et al. - 2019 - Dual Attention Network for Scene Segmentation.pdf;C\:\\Users\\theun\\Zotero\\storage\\JEN3G2RX\\1809.html}
}

@article{fuHOMPCLocalFeature2018,
  title = {{{HOMPC}}: {{A Local Feature Descriptor Based}} on the {{Combination}} of {{Magnitude}} and {{Phase Congruency Information}} for {{Multi-Sensor Remote Sensing Images}}},
  shorttitle = {{{HOMPC}}},
  author = {Fu, Zhitao and Qin, Qianqing and Luo, Bin and Sun, Hong and Wu, Chun},
  year = 2018,
  month = aug,
  journal = {Remote Sensing},
  volume = {10},
  number = {8},
  pages = {1234},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2072-4292},
  doi = {10.3390/rs10081234},
  urldate = {2025-05-15},
  abstract = {Local region description of multi-sensor images remains a challenging task in remote sensing image analysis and applications due to the non-linear radiation variations between images. This paper presents a novel descriptor based on the combination of the magnitude and phase congruency information of local regions to capture the common features of images with non-linear radiation changes. We first propose oriented phase congruency maps (PCMs) and oriented magnitude binary maps (MBMs) using the multi-oriented phase congruency and magnitude information of log-Gabor filters. The two feature vectors are then quickly constructed based on the convolved PCMs and MBMs. Finally, a dense descriptor named the histograms of oriented magnitude and phase congruency (HOMPC) is developed by combining the histograms of oriented phase congruency (HPC) and the histograms of oriented magnitude (HOM) to capture the structure and shape properties of local regions. HOMPC was evaluated with three datasets composed of multi-sensor remote sensing images obtained from unmanned ground vehicle, unmanned aerial vehicle, and satellite platforms. The descriptor performance was evaluated by recall, precision, F1-measure, and area under the precision-recall curve. The experimental results showed the advantages of the HOM and HPC combination and confirmed that HOMPC is far superior to the current state-of-the-art local feature descriptors.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {/unread,local feature descriptor,log-Gabor filters,multi-sensor images,non-linear radiation variations,phase congruency and magnitude},
  file = {C:\Users\theun\Zotero\storage\68FFJHXU\Fu et al. - 2018 - HOMPC A Local Feature Descriptor Based on the Combination of Magnitude and Phase Congruency Informa.pdf}
}

@inproceedings{fuWeightedVariationalModel2016,
  title = {A {{Weighted Variational Model}} for {{Simultaneous Reflectance}} and {{Illumination Estimation}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Fu, Xueyang and Zeng, Delu and Huang, Yue and Zhang, Xiao-Ping and Ding, Xinghao},
  year = 2016,
  month = jun,
  pages = {2782--2790},
  publisher = {IEEE},
  address = {Las Vegas, NV, USA},
  doi = {10.1109/CVPR.2016.304},
  urldate = {2025-03-11},
  abstract = {We propose a weighted variational model to estimate both the reflectance and the illumination from an observed image. We show that, though it is widely adopted for ease of modeling, the log-transformed image for this task is not ideal. Based on the previous investigation of the logarithmic transformation, a new weighted variational model is proposed for better prior representation, which is imposed in the regularization terms. Different from conventional variational models, the proposed model can preserve the estimated reflectance with more details. Moreover, the proposed model can suppress noise to some extent. An alternating minimization scheme is adopted to solve the proposed model. Experimental results demonstrate the effectiveness of the proposed model with its algorithm. Compared with other variational methods, the proposed method yields comparable or better results on both subjective and objective assessments.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\GVNN26PN\Fu et al. - 2016 - A Weighted Variational Model for Simultaneous Reflectance and Illumination Estimation.pdf}
}

@inproceedings{fuYouNotNeed2023,
  title = {You {{Do Not Need Additional Priors}} or {{Regularizers}} in {{Retinex-Based Low-Light Image Enhancement}}},
  booktitle = {2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Fu, Huiyuan and Zheng, Wenkai and Meng, Xiangyu and Wang, Xin and Wang, Chuanming and Ma, Huadong},
  year = 2023,
  month = jun,
  pages = {18125--18134},
  publisher = {IEEE},
  address = {Vancouver, BC, Canada},
  doi = {10.1109/CVPR52729.2023.01738},
  urldate = {2025-02-27},
  abstract = {Images captured in low-light conditions often suffer from significant quality degradation. Recent works have built a large variety of deep Retinex-based networks to enhance low-light images. The Retinex-based methods require decomposing the image into reflectance and illumination components, which is a highly ill-posed problem and there is no available ground truth. Previous works addressed this problem by imposing some additional priors or regularizers. However, finding an effective prior or regularizer that can be applied in various scenes is challenging, and the performance of the model suffers from too many additional constraints. We propose a contrastive learning method and a self-knowledge distillation method for Retinex decomposition that allow training our Retinex-based model without elaborate hand-crafted regularization functions. Rather than estimating reflectance and illuminance images and representing the final images as their element-wise products as in previous works, our regularizer-free Retinex decomposition and synthesis network (RFR) extracts reflectance and illuminance features and synthesizes them end-to-end. In addition, we propose a loss function for contrastive learning and a progressive learning strategy for self-knowledge distillation. Extensive experimental results demonstrate that our proposed methods can achieve superior performance compared with state-of-the-art approaches.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-0129-8},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\F4NYDAYT\Fu et al. - 2023 - You Do Not Need Additional Priors or Regularizers in Retinex-Based Low-Light Image Enhancement.pdf}
}

@misc{gahlertCityscapes3DDataset2020,
  title = {Cityscapes {{3D}}: {{Dataset}} and {{Benchmark}} for 9 {{DoF Vehicle Detection}}},
  shorttitle = {Cityscapes {{3D}}},
  author = {G{\"a}hlert, Nils and Jourdan, Nicolas and Cordts, Marius and Franke, Uwe and Denzler, Joachim},
  year = 2020,
  month = jun,
  number = {arXiv:2006.07864},
  eprint = {2006.07864},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.07864},
  urldate = {2025-05-15},
  abstract = {Detecting vehicles and representing their position and orientation in the three dimensional space is a key technology for autonomous driving. Recently, methods for 3D vehicle detection solely based on monocular RGB images gained popularity. In order to facilitate this task as well as to compare and drive state-of-the-art methods, several new datasets and benchmarks have been published. Ground truth annotations of vehicles are usually obtained using lidar point clouds, which often induces errors due to imperfect calibration or synchronization between both sensors. To this end, we propose Cityscapes 3D, extending the original Cityscapes dataset with 3D bounding box annotations for all types of vehicles. In contrast to existing datasets, our 3D annotations were labeled using stereo RGB images only and capture all nine degrees of freedom. This leads to a pixel-accurate reprojection in the RGB image and a higher range of annotations compared to lidar-based approaches. In order to ease multitask learning, we provide a pairing of 2D instance segments with 3D bounding boxes. In addition, we complement the Cityscapes benchmark suite with 3D vehicle detection based on the new annotations as well as metrics presented in this work. Dataset and benchmark are available online.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C\:\\Users\\theun\\Zotero\\storage\\THAXEBTM\\Gählert et al. - 2020 - Cityscapes 3D Dataset and Benchmark for 9 DoF Vehicle Detection.pdf;C\:\\Users\\theun\\Zotero\\storage\\LNQXHAC2\\2006.html}
}

@inproceedings{gaoCrossDomainCorrelationDistillation2022,
  title = {Cross-{{Domain Correlation Distillation}} for {{Unsupervised Domain Adaptation}} in {{Nighttime Semantic Segmentation}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Gao, Huan and Guo, Jichang and Wang, Guoli and Zhang, Qian},
  year = 2022,
  pages = {9913--9923},
  urldate = {2025-06-15},
  langid = {english},
  keywords = {Computer vision,Cross-domain,Distillation,Domain adaptation,Grouping and shape analyse,grouping and shape analysis,Long tail,Meta-segmentation,Performance,Self-& semi-& meta- segmentation,Self-& semi-& meta- Segmentation,Semantic segmentation,Semantic Segmentation,Semantics,Shape-analysis,Transfer/low-shot/long-tail learning},
  file = {C:\Users\theun\Zotero\storage\PFGVE446\Gao et al. - 2022 - Cross-Domain Correlation Distillation for Unsupervised Domain Adaptation in Nighttime Semantic Segme.pdf}
}

@article{gaoImprovedVehicleLocalization2022,
  title = {Improved {{Vehicle Localization Using On-Board Sensors}} and {{Vehicle Lateral Velocity}}},
  author = {Gao, Letian and Xiong, Lu and Xia, Xin and Lu, Yishi and Yu, Zhuoping and Khajepour, Amir},
  year = 2022,
  month = apr,
  journal = {IEEE Sensors Journal},
  volume = {22},
  number = {7},
  pages = {6818--6831},
  issn = {1558-1748},
  doi = {10.1109/JSEN.2022.3150073},
  urldate = {2025-01-02},
  abstract = {Vehicle localization is essential for intelligent and autonomous vehicles. To improve the accuracy of vehicle stand-alone localization in highly dynamic driving conditions during GNSS (Global Navigation Satellites Systems) outages, this paper proposes a vehicle localization system based on vehicle chassis sensors considering vehicle lateral velocity. Firstly, a GNSS/On-board sensors fusion localization framework is established, which could estimate vehicle states such as attitude, velocity, and position. Secondly, when the vehicle has a large lateral motion, nonholonomic constraint in the lateral direction loses fidelity. Instead of using nonholonomic constraint, we propose a vehicle dynamics/kinematics fusion lateral velocity estimation algorithm, which combines the advantage of vehicle dynamic model in low dynamic driving conditions and the advantage of kinematic model in highly dynamic driving conditions. Thirdly, vehicle longitudinal velocity estimated by WSS (Wheel Speed Sensor) and lateral velocity estimated by proposed method are as measurements for the localization system. All information is fused by an adaptive Kalman filter. Finally, vehicle experiments in U-turn maneuver and left-turn maneuver at a traffic intersection are conducted to verify the proposed method. Four different methods are compared in the experiments, and the results show that the estimated position accuracy of our method is below half a meter during a 5s GNSS outage and could keep a sub-meter-level during a 20s GNSS outage while the vehicle has a relatively large lateral motion.},
  keywords = {/unread,Estimation,Global navigation satellite system,Heuristic algorithms,Kalman filter,lateral velocity,Location awareness,on-board sensors,Sensor fusion,Sensors,vehicle dynamics,Vehicle dynamics,Vehicle localization},
  file = {C\:\\Users\\theun\\Zotero\\storage\\G9DDYFII\\Gao et al. - 2022 - Improved Vehicle Localization Using On-Board Sensors and Vehicle Lateral Velocity.pdf;C\:\\Users\\theun\\Zotero\\storage\\IE6MBUV7\\9707770.html}
}

@article{gaoUnsupervisedLearningMonocular2022,
  title = {Unsupervised {{Learning}} of {{Monocular Depth}} and {{Ego-Motion}} in {{Outdoor}}/{{Indoor Environments}}},
  author = {Gao, Ruipeng and Xiao, Xuan and Xing, Weiwei and Li, Chi and Liu, Lei},
  year = 2022,
  month = sep,
  journal = {IEEE Internet of Things Journal},
  volume = {9},
  number = {17},
  pages = {16247--16258},
  issn = {2327-4662},
  doi = {10.1109/JIOT.2022.3151629},
  urldate = {2025-06-29},
  abstract = {Visual-based unsupervised learning [1]--[3] has emerged as a promising approach in estimating monocular depth and ego-motion, avoiding intensive efforts on collecting and labeling the ground truth. However, they are still restrained by the brightness constancy assumption among video sequences, especially susceptible with frequent illumination variations or nearby textureless surroundings in indoor environments. In this article, we selectively combine the complementary strength of visual and inertial measurements, i.e., videos extract static and distinct features while inertial readings depict scale-consistent and environment-agnostic movements, and propose a novel unsupervised learning framework to predict both monocular depth and ego-motion trajectory simultaneously. This challenging task is solved by learning both forward and backward inertial sequences to eliminate inevitable noises, and reweighting visual and inertial features via gated neural networks in various environments or with user-specific moving dynamics. In addition, we also employ structure cues to produce scene depths from a single image and explore structure consistency constraints to calibrate the depth estimates in indoor buildings. Experiments on the outdoor KITTI data set and our dedicated indoor prototype reveal that our approach consistently outperforms the state of the art on both depth and ego-motion estimates. To the best of our knowledge, this is the first work to fuse visual and inertial data without any supervision signals for monocular depth and ego-motion estimation, and our solution remain effective and robust even in textureless indoor scenarios.},
  keywords = {Cameras,Ego-motion,Estimation,Feature extraction,monocular depth,Sensors,structure cues,unsupervised learning,Unsupervised learning,Videos,visual-inertial fusion,Visualization},
  file = {C:\Users\theun\Zotero\storage\CGPAHMF8\Gao et al. - 2022 - Unsupervised Learning of Monocular Depth and Ego-Motion in OutdoorIndoor Environments.pdf}
}

@incollection{gargUnsupervisedCNNSingle2016,
  title = {Unsupervised {{CNN}} for {{Single View Depth Estimation}}: {{Geometry}} to the {{Rescue}}},
  shorttitle = {Unsupervised {{CNN}} for {{Single View Depth Estimation}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2016},
  author = {Garg, Ravi and B.G., Vijay Kumar and Carneiro, Gustavo and Reid, Ian},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  year = 2016,
  volume = {9912},
  pages = {740--756},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-46484-8_45},
  urldate = {2025-07-03},
  abstract = {A significant weakness of most current deep Convolutional Neural Networks is the need to train them using vast amounts of manually labelled data. In this work we propose a unsupervised framework to learn a deep convolutional neural network for single view depth prediction, without requiring a pre-training stage or annotated ground-truth depths. We achieve this by training the network in a manner analogous to an autoencoder. At training time we consider a pair of images, source and target, with small, known camera motion between the two such as a stereo pair. We train the convolutional encoder for the task of predicting the depth map for the source image. To do so, we explicitly generate an inverse warp of the target image using the predicted depth and known inter-view displacement, to reconstruct the source image; the photometric error in the reconstruction is the reconstruction loss for the encoder. The acquisition of this training data is considerably simpler than for equivalent systems, requiring no manual annotation, nor calibration of depth sensor to camera. We show that our network trained on less than half of the KITTI dataset gives comparable performance to that of the state-of-the-art supervised methods for single view depth estimation.},
  isbn = {978-3-319-46483-1 978-3-319-46484-8},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\M97SM3TL\Garg et al. - 2016 - Unsupervised CNN for Single View Depth Estimation Geometry to the Rescue.pdf}
}

@inproceedings{gasperiniRobustMonocularDepth2023,
  title = {Robust {{Monocular Depth Estimation}} under {{Challenging Conditions}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Gasperini, Stefano and Morbitzer, Nils and Jung, HyunJun and Navab, Nassir and Tombari, Federico},
  year = 2023,
  pages = {8177--8186},
  urldate = {2025-06-29},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\EEP52DUK\Gasperini et al. - 2023 - Robust Monocular Depth Estimation under Challenging Conditions.pdf}
}

@article{geigerVisionMeetsRobotics2013,
  title = {Vision Meets Robotics: {{The KITTI}} Dataset},
  shorttitle = {Vision Meets Robotics},
  author = {Geiger, A and Lenz, P and Stiller, C and Urtasun, R},
  year = 2013,
  month = sep,
  journal = {The International Journal of Robotics Research},
  volume = {32},
  number = {11},
  pages = {1231--1237},
  publisher = {SAGE Publications Ltd STM},
  issn = {0278-3649},
  doi = {10.1177/0278364913491297},
  urldate = {2025-05-29},
  abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10--100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations, and range from freeways over rural areas to inner-city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets, and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide.},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\ZGY33689\Geiger et al. - 2013 - Vision meets robotics The KITTI dataset.pdf}
}

@article{ghariPedestrianDetectionLowlight2024,
  title = {Pedestrian Detection in Low-Light Conditions: {{A}} Comprehensive Survey},
  shorttitle = {Pedestrian Detection in Low-Light Conditions},
  author = {Ghari, Bahareh and Tourani, Ali and Shahbahrami, Asadollah and Gaydadjiev, Georgi},
  year = 2024,
  month = aug,
  journal = {Image and Vision Computing},
  volume = {148},
  pages = {105106},
  issn = {02628856},
  doi = {10.1016/j.imavis.2024.105106},
  urldate = {2025-03-10},
  abstract = {Pedestrian detection remains a critical problem in various domains, such as computer vision, surveillance, and autonomous driving. In particular, accurate and instant detection of pedestrians in low-light conditions and reduced visibility is of utmost importance for autonomous vehicles to prevent accidents and save lives. This paper aims to comprehensively survey various pedestrian detection approaches, baselines, and datasets that specifically target low-light conditions. The survey discusses the challenges faced in detecting pedestrians at night and explores state-of-the-art methodologies proposed in recent years to address this issue. These methodologies encompass a diverse range, including deep learning-based, feature-based, and hybrid approaches, which have shown promising results in enhancing pedestrian detection performance under challenging lighting conditions. Furthermore, the paper highlights current research directions in the field and identifies potential solutions that merit further investigation by researchers. By thoroughly examining pedestrian detection techniques in low-light conditions, this survey seeks to contribute to the advancement of safer and more reliable autonomous driving systems and other applications related to pedestrian safety. Accordingly, most of the current approaches in the field use deep learning-based image fusion methodologies (i.e., early, halfway, and late fusion) for accurate and reliable pedestrian detection. Moreover, the majority of the works in the field (approximately 48\%) have been evaluated on the KAIST dataset, while the real-world video feeds recorded by authors have been used in less than 6 \% of the works.},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\E6TPTMQS\Ghari et al. - 2024 - Pedestrian detection in low-light conditions A comprehensive survey.pdf}
}

@inproceedings{godardDiggingSelfSupervisedMonocular2019,
  title = {Digging {{Into Self-Supervised Monocular Depth Estimation}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Godard, Clement and Aodha, Oisin Mac and Firman, Michael and Brostow, Gabriel},
  year = 2019,
  month = oct,
  pages = {3827--3837},
  publisher = {IEEE},
  address = {Seoul, Korea (South)},
  doi = {10.1109/ICCV.2019.00393},
  urldate = {2025-02-21},
  abstract = {Per-pixel ground-truth depth data is challenging to acquire at scale. To overcome this limitation, self-supervised learning has emerged as a promising alternative for training models to perform monocular depth estimation. In this paper, we propose a set of improvements, which together result in both quantitatively and qualitatively improved depth maps compared to competing self-supervised methods.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-7281-4803-8},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\EBEGEBU3\Godard et al. - 2019 - Digging Into Self-Supervised Monocular Depth Estimation.pdf}
}

@inproceedings{godardUnsupervisedMonocularDepth2017,
  title = {Unsupervised {{Monocular Depth Estimation With Left-Right Consistency}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Godard, Clement and Mac Aodha, Oisin and Brostow, Gabriel J.},
  year = 2017,
  pages = {270--279},
  urldate = {2025-06-29},
  file = {C:\Users\theun\Zotero\storage\J8UH5D4R\Godard et al. - 2017 - Unsupervised Monocular Depth Estimation With Left-Right Consistency.pdf}
}

@inproceedings{gongContinuousPseudoLabelRectified2023,
  title = {Continuous {{Pseudo-Label Rectified Domain Adaptive Semantic Segmentation With Implicit Neural Representations}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Gong, Rui and Wang, Qin and Danelljan, Martin and Dai, Dengxin and Van Gool, Luc},
  year = 2023,
  pages = {7225--7235},
  urldate = {2025-06-16},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\Z2G5UGXN\Gong et al. - 2023 - Continuous Pseudo-Label Rectified Domain Adaptive Semantic Segmentation With Implicit Neural Represe.pdf}
}

@book{gonzalezDigitalImageProcessing2002,
  title = {Digital Image Processing},
  author = {Gonzalez, Rafael C. and Woods, Richard E.},
  year = 2002,
  edition = {2nd ed},
  publisher = {Prentice Hall},
  address = {Upper Saddle River, N.J},
  isbn = {978-0-201-18075-6},
  langid = {english},
  lccn = {TA1632 .G66 2002},
  keywords = {Digital techniques,Image processing},
  file = {C:\Users\theun\Zotero\storage\MDZ3NZEM\Gonzalez and Woods - 2002 - Digital image processing.pdf}
}

@article{grayGLAREDatasetTraffic2023,
  title = {{{GLARE}}: {{A Dataset}} for {{Traffic Sign Detection}} in {{Sun Glare}}},
  shorttitle = {{{GLARE}}},
  author = {Gray, Nicholas and Moraes, Megan and Bian, Jiang and Wang, Alex and Tian, Allen and Wilson, Kurt and Huang, Yan and Xiong, Haoyi and Guo, Zhishan},
  year = 2023,
  month = nov,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {24},
  number = {11},
  pages = {12323--12330},
  issn = {1558-0016},
  doi = {10.1109/TITS.2023.3294411},
  urldate = {2025-01-19},
  abstract = {Real-time machine learning object detection algorithms are often found within autonomous vehicle technology and depend on quality datasets. It is essential that these algorithms work correctly in everyday conditions as well as under strong sun glare. Reports indicate glare is one of the two most prominent environment-related reasons for crashes. However, existing datasets, such as the Laboratory for Intelligent \& Safe Automobiles Traffic Sign (LISA) Dataset and the German Traffic Sign Recognition Benchmark, do not reflect the existence of sun glare at all. This paper presents the GLARE (GLARE is available at: https://github.com/NicholasCG/GLARE\_Dataset) traffic sign dataset: a collection of images with U.S-based traffic signs under heavy visual interference by sunlight. GLARE contains 2,157 images of traffic signs with sun glare, pulled from 33 videos of dashcam footage of roads in the United States. It provides an essential enrichment to the widely used LISA Traffic Sign dataset. Our experimental study shows that although several state-of-the-art baseline architectures have demonstrated good performance on traffic sign detection in conditions without sun glare in the past, they performed poorly when tested against GLARE (e.g., average mAP0.5:0.95 of 19.4). We also notice that current architectures have better detection when trained on images of traffic signs in sun glare performance (e.g., average mAP0.5:0.95 of 39.6), and perform best when trained on a mixture of conditions (e.g., average mAP0.5:0.95 of 42.3).},
  keywords = {/unread,Annotations,Classification algorithms,Object detection,public data set,Real-time systems,Sun,sun glare,Task analysis,Traffic sign detection,Training},
  file = {C\:\\Users\\theun\\Zotero\\storage\\JLSU9FFP\\Gray et al. - 2023 - GLARE A Dataset for Traffic Sign Detection in Sun Glare.pdf;C\:\\Users\\theun\\Zotero\\storage\\RSBXWISL\\10197287.html}
}

@inproceedings{guizilini3DPackingSelfSupervised2020,
  title = {{{3D Packing}} for {{Self-Supervised Monocular Depth Estimation}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Guizilini, Vitor and Ambrus, Rares and Pillai, Sudeep and Raventos, Allan and Gaidon, Adrien},
  year = 2020,
  pages = {2485--2494},
  urldate = {2025-07-03},
  file = {C:\Users\theun\Zotero\storage\GGGM6RY8\Guizilini et al. - 2020 - 3D Packing for Self-Supervised Monocular Depth Estimation.pdf}
}

@article{guiziliniFullSurroundMonodepth2022,
  title = {Full {{Surround Monodepth From Multiple Cameras}}},
  author = {Guizilini, Vitor and Vasiljevic, Igor and Ambrus, Rares and Shakhnarovich, Greg and Gaidon, Adrien},
  year = 2022,
  month = apr,
  journal = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {2},
  pages = {5397--5404},
  issn = {2377-3766},
  doi = {10.1109/LRA.2022.3150884},
  urldate = {2025-07-04},
  abstract = {Self-supervised monocular depth and ego-motion estimation is a promising approach to replace or supplement expensive depth sensors such as LiDAR for robotics applications like autonomous driving. However, most research in this area focuses on a single monocular camera or stereo pairs that cover only a fraction of the scene around the vehicle. In this work, we extend monocular self-supervised depth and ego-motion estimation to large-baseline multi-camera rigs. Using generalized spatio-temporal contexts, pose consistency constraints, and carefully designed photometric loss masking, we learn a single network generating dense, consistent, and scale-aware point clouds that cover the same full surround 360\textsuperscript{\textbackslash circ } field of view as a typical LiDAR scanner. We also propose a new scale-consistent evaluation metric more suitable to multi-camera settings. Experiments on two challenging benchmarks illustrate the benefits of our approach over strong baselines.},
  keywords = {autonomous automobiles,Cameras,Computer vision,Estimation,Laser radar,machine learning,Point cloud compression,Robot vision systems,Task analysis,Three-dimensional displays},
  file = {C:\Users\theun\Zotero\storage\49BXADB8\Guizilini et al. - 2022 - Full Surround Monodepth From Multiple Cameras.pdf}
}

@article{guNoReferenceQualityMetric2017,
  title = {No-{{Reference Quality Metric}} of {{Contrast-Distorted Images Based}} on {{Information Maximization}}},
  author = {Gu, Ke and Lin, Weisi and Zhai, Guangtao and Yang, Xiaokang and Zhang, Wenjun and Chen, Chang Wen},
  year = 2017,
  month = dec,
  journal = {IEEE Transactions on Cybernetics},
  volume = {47},
  number = {12},
  pages = {4559--4565},
  issn = {2168-2267, 2168-2275},
  doi = {10.1109/TCYB.2016.2575544},
  urldate = {2025-03-28},
  abstract = {The general purpose of seeing a picture is to attain information as much as possible. With it, we in this paper devise a new no-reference/blind metric for image quality assessment (IQA) of contrast distortion. For local details, we first roughly remove predicted regions in an image since unpredicted remains are of much information. We then compute entropy of particular unpredicted areas of maximum information via visual saliency. From global perspective, we compare the image histogram with the uniformly distributed histogram of maximum information via the symmetric K-L divergence. The proposed blind IQA method generates an overall quality estimation of a contrast-distorted image by properly combining local and global considerations. Thorough experiments on five databases/subsets demonstrate the superiority of our training-free blind technique over state-ofthe-art full- and no-reference IQA methods. Furthermore, the proposed model is also applied to amend the performance of general-purpose blind quality metrics to a sizable margin.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\3TWGL33X\Gu et al. - 2017 - No-Reference Quality Metric of Contrast-Distorted Images Based on Information Maximization.pdf}
}

@inproceedings{guoConvolutionalBlindDenoising2019,
  title = {Toward {{Convolutional Blind Denoising}} of {{Real Photographs}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Guo, Shi and Yan, Zifei and Zhang, Kai and Zuo, Wangmeng and Zhang, Lei},
  year = 2019,
  month = jun,
  pages = {1712--1722},
  publisher = {IEEE},
  address = {Long Beach, CA, USA},
  doi = {10.1109/CVPR.2019.00181},
  urldate = {2025-03-24},
  abstract = {While deep convolutional neural networks (CNNs) have achieved impressive success in image denoising with additive white Gaussian noise (AWGN), their performance remains limited on real-world noisy photographs. The main reason is that their learned models are easy to overfit on the simplified AWGN model which deviates severely from the complicated real-world noise model. In order to improve the generalization ability of deep CNN denoisers, we suggest training a convolutional blind denoising network (CBDNet) with more realistic noise model and real-world noisy-clean image pairs. On the one hand, both signaldependent noise and in-camera signal processing pipeline is considered to synthesize realistic noisy images. On the other hand, real-world noisy photographs and their nearly noise-free counterparts are also included to train our CBDNet. To further provide an interactive strategy to rectify denoising result conveniently, a noise estimation subnetwork with asymmetric learning to suppress under-estimation of noise level is embedded into CBDNet. Extensive experimental results on three datasets of real-world noisy photographs clearly demonstrate the superior performance of CBDNet over state-of-the-arts in terms of quantitative metrics and visual quality. The code has been made available at https://github.com/GuoShi28/CBDNet.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-7281-3293-8},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\EADLVJT7\Guo et al. - 2019 - Toward Convolutional Blind Denoising of Real Photographs.pdf}
}

@article{guoDeepLearningenhancedEnvironment2025,
  title = {Deep Learning-Enhanced Environment Perception for Autonomous Driving: {{MDNet}} with {{CSP-DarkNet53}}},
  shorttitle = {Deep Learning-Enhanced Environment Perception for Autonomous Driving},
  author = {Guo, Xuyao and Jiang, Feng and Chen, Quanzhen and Wang, Yuxuan and Sha, Kaiyue and Chen, Jing},
  year = 2025,
  month = apr,
  journal = {Pattern Recognition},
  volume = {160},
  pages = {111174},
  issn = {00313203},
  doi = {10.1016/j.patcog.2024.111174},
  urldate = {2024-12-04},
  abstract = {Implementing environmental perception in intelligent vehicles is a crucial application, but the parallel processing of numerous algorithms on the vehicle side is complex, and their integration remains a critical challenge. To address this problem, this paper proposes a multitask detection algorithm Multitask Detection Network (MDNet) based on Cross Stage Partial Networks with Darknet53 Backbone (CSP-DarkNet53) with high feature extraction capability, which can simultaneously detect vehicles, pedestrians, traffic lights, traffic signs, and bicycles as well as lane lines. MDNet obtains exceptional results in multitask scenarios by employing innovative architectural designs consisting of a Feature Extraction Module, Target-level Branches, and Pixel-level Branches. The feature extraction module proposes an improved CSPPF structure to extract features more efficiently for three tasks, facilitating MDNet's capacity. The target-level branch suggests PFPN, which combines features from the backbone network, and the pixel-level branch utilizes a primary feature fusion network and an enhanced C2F\_Faster method to spot lane lines more precisely. By incorporating these designs, MDNet's performance in complex environments is enhanced significantly. The algorithm underwent testing on the Berkeley DeepDrive 100K (BDD100K) and Cityscapes datasets, in which it could identify traffic targets and lane lines in numerous challenging settings, resulting in a 9.8 \% measure of improvement in detection accuracy map for all three tasks relative to You Only Look Once for Panoptic Driving Perception (YOLOP, a multitask detection network), an 8.9 \% improvement in IoU, a 22.1 \% improvement in accuracy. It reached a speed of 46fps, which serves the practical applications' requirements more effectively.},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\C5TPVNMP\Guo et al. - 2025 - Deep learning-enhanced environment perception for autonomous driving MDNet with CSP-DarkNet53.pdf}
}

@article{guoImageDehazingEnhancement2022,
  title = {Image Dehazing via Enhancement, Restoration, and Fusion: {{A}} Survey},
  shorttitle = {Image Dehazing via Enhancement, Restoration, and Fusion},
  author = {Guo, Xiaojie and Yang, Yang and Wang, Chaoyue and Ma, Jiayi},
  year = 2022,
  month = oct,
  journal = {Information Fusion},
  volume = {86--87},
  pages = {146--170},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2022.07.005},
  urldate = {2025-10-20},
  abstract = {Haze usually causes severe interference to image visibility. Such degradation on images troubles both human observers and computer vision systems. To seek high-quality images from degraded ones, a large number of image dehazing algorithms have been proposed from different perspectives like image enhancement, restoration, and fusion. Especially in recent years, with the rapid development of deep learning, CNN-based methods have already dominated the mainstream of image dehazing and gained significant progress on benchmark datasets. This paper firstly presents a comprehensive survey of existing image dehazing methods, and then conducts both qualitative and quantitative comparisons among representative methods, from classic methods to recent advanced approaches. We expect the literature survey and benchmark analysis could help readers better understand the advantages and limitations of existing dehazing methods. Moreover, a discussion on possible trends in single image dehazing is put forward to innovate further works.},
  keywords = {/unread,Image dehazing,Image enhancement,Image fusion,Image restoration},
  file = {C\:\\Users\\theun\\Zotero\\storage\\79SXBFV7\\Guo et al. - 2022 - Image dehazing via enhancement, restoration, and fusion A survey.pdf;C\:\\Users\\theun\\Zotero\\storage\\SLTCUXC3\\S1566253522000641.html}
}

@article{guoLightweightDeepNetworkenabled2022,
  title = {Lightweight Deep Network-Enabled Real-Time Low-Visibility Enhancement for Promoting Vessel Detection in Maritime Video Surveillance},
  author = {Guo, Y. and Lu, Y. and Liu, R.W.},
  year = 2022,
  journal = {Journal of Navigation},
  volume = {75},
  number = {1},
  pages = {230--250},
  doi = {10.1017/S0373463321000783},
  keywords = {/unread}
}

@article{guoLIMELowLightImage2017,
  title = {{{LIME}}: {{Low-Light Image Enhancement}} via {{Illumination Map Estimation}}},
  shorttitle = {{{LIME}}},
  author = {Guo, Xiaojie and Li, Yu and Ling, Haibin},
  year = 2017,
  month = feb,
  journal = {IEEE Transactions on Image Processing},
  volume = {26},
  number = {2},
  pages = {982--993},
  issn = {1941-0042},
  doi = {10.1109/TIP.2016.2639450},
  urldate = {2025-03-11},
  abstract = {When one captures images in low-light conditions, the images often suffer from low visibility. Besides degrading the visual aesthetics of images, this poor quality may also significantly degenerate the performance of many computer vision and multimedia algorithms that are primarily designed for high-quality inputs. In this paper, we propose a simple yet effective low-light image enhancement (LIME) method. More concretely, the illumination of each pixel is first estimated individually by finding the maximum value in R, G, and B channels. Furthermore, we refine the initial illumination map by imposing a structure prior on it, as the final illumination map. Having the well-constructed illumination map, the enhancement can be achieved accordingly. Experiments on a number of challenging low-light images are present to reveal the efficacy of our LIME and show its superiority over several state-of-the-arts in terms of enhancement quality and efficiency.},
  keywords = {Atmospheric modeling,Estimation,Histograms,illumination (light) transmission,Illumination estimation,Image color analysis,Image enhancement,Lighting,low-light image enhancement,Visualization},
  file = {C\:\\Users\\theun\\Zotero\\storage\\46LFDSRM\\Guo et al. - 2017 - LIME Low-Light Image Enhancement via Illumination Map Estimation.pdf;C\:\\Users\\theun\\Zotero\\storage\\FKF4DK9U\\Guo et al. - 2017 - LIME Low-Light Image Enhancement via Illumination Map Estimation.pdf;C\:\\Users\\theun\\Zotero\\storage\\SW9YRNFQ\\7782813.html}
}

@inproceedings{guoShadowDiffusionWhenDegradation2023,
  title = {{{ShadowDiffusion}}: {{When Degradation Prior Meets Diffusion Model}} for {{Shadow Removal}}},
  shorttitle = {{{ShadowDiffusion}}},
  booktitle = {2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Guo, Lanqing and Wang, Chong and Yang, Wenhan and Huang, Siyu and Wang, Yufei and Pfister, Hanspeter and Wen, Bihan},
  year = 2023,
  month = jun,
  pages = {14049--14058},
  publisher = {IEEE},
  address = {Vancouver, BC, Canada},
  doi = {10.1109/CVPR52729.2023.01350},
  urldate = {2025-03-10},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-0129-8},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\5RIX8ZTI\Guo et al. - 2023 - ShadowDiffusion When Degradation Prior Meets Diffusion Model for Shadow Removal.pdf}
}

@article{guoSurveyImageEnhancement2023,
  title = {A Survey on Image Enhancement for {{Low-light}} Images},
  author = {Guo, Jiawei and Ma, Jieming and {Garc{\'i}a-Fern{\'a}ndez}, {\'A}ngel F. and Zhang, Yungang and Liang, Haining},
  year = 2023,
  month = apr,
  journal = {Heliyon},
  volume = {9},
  number = {4},
  publisher = {Elsevier},
  issn = {2405-8440},
  doi = {10.1016/j.heliyon.2023.e14558},
  urldate = {2025-01-15},
  langid = {english},
  keywords = {Deep learning,Image enhancement,Image processing,Low-light images},
  file = {C:\Users\theun\Zotero\storage\24DP95U2\Guo et al. - 2023 - A survey on image enhancement for Low-light images.pdf}
}

@misc{guoZeroReferenceDeepCurve2020,
  title = {Zero-{{Reference Deep Curve Estimation}} for {{Low-Light Image Enhancement}}},
  author = {Guo, Chunle and Li, Chongyi and Guo, Jichang and Loy, Chen Change and Hou, Junhui and Kwong, Sam and Cong, Runmin},
  year = 2020,
  month = mar,
  number = {arXiv:2001.06826},
  eprint = {2001.06826},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2001.06826},
  urldate = {2025-01-03},
  abstract = {The paper presents a novel method, Zero-Reference Deep Curve Estimation (Zero-DCE), which formulates light enhancement as a task of image-specific curve estimation with a deep network. Our method trains a lightweight deep network, DCE-Net, to estimate pixel-wise and high-order curves for dynamic range adjustment of a given image. The curve estimation is specially designed, considering pixel value range, monotonicity, and differentiability. Zero-DCE is appealing in its relaxed assumption on reference images, i.e., it does not require any paired or unpaired data during training. This is achieved through a set of carefully formulated non-reference loss functions, which implicitly measure the enhancement quality and drive the learning of the network. Our method is efficient as image enhancement can be achieved by an intuitive and simple nonlinear curve mapping. Despite its simplicity, we show that it generalizes well to diverse lighting conditions. Extensive experiments on various benchmarks demonstrate the advantages of our method over state-of-the-art methods qualitatively and quantitatively. Furthermore, the potential benefits of our Zero-DCE to face detection in the dark are discussed. Code and model will be available at https://github.com/Li-Chongyi/Zero-DCE.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\JUJQDGH6\\Guo et al. - 2020 - Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement.pdf;C\:\\Users\\theun\\Zotero\\storage\\2PV2QVZ2\\2001.html}
}

@article{guptaOverviewImageSegmentation2024,
  title = {An {{Overview}} on {{Image Segmentation Techniques}} for {{Reversible Data Hiding}}},
  author = {Gupta, Rasika},
  year = 2024,
  month = oct,
  journal = {International Journal of Mathematical, Engineering and Management Sciences},
  volume = {9},
  number = {5},
  pages = {1163--1184},
  issn = {2455-7749},
  doi = {10.33889/IJMEMS.2024.9.5.061},
  urldate = {2024-12-06},
  abstract = {The fields of image processing and computer vision have witnessed significant growth due to the proliferation of digital images across diverse domains. Image Segmentation is the fundamental task in digital image processing, finding applications in pivotal areas such as medical imaging, covert communication, autonomous driving, satellite imaging, among others. One particularly intriguing application of image segmentation lies in Reversible Data Hiding (RDH), where the delineation of the main Region of Interest (ROI) and Non-Region of Interest (NROI) using segmentation plays a crucial role for effective data encryption in the images. Over the last two decades, various studies focussed on developing an efficient data hiding approach, which can embed secret data within ROI and NROI part of image while ensuring its quality. A comprehensive survey has been conducted that meticulously examines different segmentation techniques, along with its usage in reversible data hiding. The main objective of this survey is to compare the performance metrics of reversible data hiding after applying different image segmentation techniques. The image segmentation techniques have been categorized systematically into three main classes: i) Traditional segmentation techniques, encompassing a spectrum of approaches like thresholding, region-based and edge detection based techniques, ii) Machine Learning (ML) based approach consisting of Clustering, Support Vector Machine (SVM) and iii) Deep Learning (DL) based technique, propelled by Convolutional Neural Networks (CNNs) that have emerged as a transformative paradigm, revolutionizing segmentation tasks with their ability to learn complex images. The survey finds out that PSNR value of data embedded images is high after applying deep learning based segmentation technique.},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\AZBIPGCA\Gupta - 2024 - An Overview on Image Segmentation Techniques for Reversible Data Hiding.pdf}
}

@article{guSpatialadaptiveMixupDomain2025,
  title = {Spatial-Adaptive Mixup for Domain Adaptation in Nighttime Semantic Segmentation},
  author = {Gu, Zhuoming and Huang, Wei and Xu, Mengfan and Zeng, Dan and Huang, Rui},
  year = 2025,
  month = aug,
  journal = {Neurocomputing},
  volume = {641},
  pages = {130315},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2025.130315},
  urldate = {2025-06-18},
  abstract = {In the field of autonomous driving, semantic segmentation model should adapt to different time periods throughout the day. However, nighttime semantic segmentation poses a significant challenge due to the low illumination and the lack of annotated data. In this paper, we propose a novel domain adaptation framework consisting of a bidirectional style transfer step (BST) and multi-target mean teacher step (MTMT), to improve the prediction accuracy. BST sets twilight scenes as the intermediate domain and migrates the style from other domains towards the intermediate one, which helps to promote style alignment. In MTMT, we design a new spatial-adaptive mixup method for dynamic objects (SA-DMix) that employs label based geometric matching method to adaptively adjust the mixing position. It can boost the robustness of dynamic object segmentation and ensure the coherence of contextual information. MTMT further conducts multi-level adversarial learning, including feature-level and prediction-level, to achieve domain alignment and enhance adaptability. Extensive experiments are conducted on the Dark Zurich, ACDC, and Nighttime Driving dataset and the result demonstrates that our method can bring significant improvement in nighttime semantic segmentation.},
  keywords = {adaptation,adversarial learning,article,Autonomous driving,controlled study,diagnosis,Domain adaptation,Domain mixup,Dynamic objects,Geometry,illumination,Multi-targets,Nighttime vision,prediction,Segmentation models,Semantic segmentation,Semantic Segmentation,Teachers',Time-periods,vision},
  file = {C:\Users\theun\Zotero\storage\MQE5G3LM\Gu et al. - 2025 - Spatial-adaptive mixup for domain adaptation in nighttime semantic segmentation.pdf}
}

@article{haiR2RNetLowlightImage2023,
  title = {{{R2RNet}}: {{Low-light}} Image Enhancement via {{Real-low}} to {{Real-normal Network}}},
  shorttitle = {{{R2RNet}}},
  author = {Hai, Jiang and Xuan, Zhu and Yang, Ren and Hao, Yutong and Zou, Fengzhu and Lin, Fang and Han, Songchen},
  year = 2023,
  month = feb,
  journal = {Journal of Visual Communication and Image Representation},
  volume = {90},
  pages = {103712},
  issn = {1047-3203},
  doi = {10.1016/j.jvcir.2022.103712},
  urldate = {2025-05-04},
  abstract = {Images captured in weak illumination conditions could seriously degrade the image quality. Solving a series of degradation of low-light images can effectively improve the visual quality of images and the performance of high-level visual tasks. In this study, a novel Retinex-based Real-low to Real-normal Network (R2RNet) is proposed for low-light image enhancement, which includes three subnets: a Decom-Net, a Denoise-Net, and a Relight-Net. These three subnets are used for decomposing, denoising, contrast enhancement and detail preservation, respectively. Our R2RNet not only uses the spatial information of the image to improve the contrast but also uses the frequency information to preserve the details. Therefore, our model achieved more robust results for all degraded images. Unlike most previous methods that were trained on synthetic images, we collected the first Large-Scale Real-World paired low/normal-light images dataset (LSRW dataset) to satisfy the training requirements and make our model have better generalization performance in real-world scenes. Extensive experiments on publicly available datasets demonstrated that our method outperforms the existing state-of-the-art methods both quantitatively and visually. In addition, our results showed that the performance of the high-level visual task (i.e., face detection) can be effectively improved by using the enhanced results obtained by our method in low-light conditions.~Our codes and the LSRW dataset are available at: https://github.com/JianghaiSCU/R2RNet.},
  keywords = {/unread,Image processing,Low-light image enhancement,Real-world low/normal-light image pairs,Retinex theory},
  file = {C\:\\Users\\theun\\Zotero\\storage\\GQTR7CNB\\Hai et al. - 2023 - R2RNet Low-light image enhancement via Real-low to Real-normal Network.pdf;C\:\\Users\\theun\\Zotero\\storage\\GBXBHIBU\\S1047320322002322.html}
}

@inproceedings{hambardeDepthEstimationSingle2020,
  title = {Depth {{Estimation From Single Image And Semantic Prior}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Hambarde, Praful and Dudhane, Akshay and Patil, Prashant W. and Murala, Subrahmanyam and Dhall, Abhinav},
  year = 2020,
  month = oct,
  pages = {1441--1445},
  issn = {2381-8549},
  doi = {10.1109/ICIP40778.2020.9190985},
  urldate = {2025-06-23},
  abstract = {The multi-modality sensor fusion technique is an active research area in scene understating. In this work, we explore the RGB image and semantic-map fusion methods for depth estimation. The LiDARs, Kinect, and TOF depth sensors are unable to predict the depth-map at illuminate and monotonous pattern surface. In this paper, we propose a semantic-to-depth generative adversarial network (S2D-GAN) for depth estimation from RGB image and its semantic-map. In the first stage, the proposed S2D-GAN estimates the coarse level depthmap using a semantic-to-coarse-depth generative adversarial network (S2CD-GAN) while the second stage estimates the fine-level depth-map using a cascaded multi-scale spatial pooling network. The experimental analysis of the proposed S2D-GAN performed on NYU-Depth-V2 dataset shows that the proposed S2D-GAN gives outstanding result over existing single image depth estimation and RGB with sparse samples methods. The proposed S2D-GAN also gives efficient results on the real-world indoor and outdoor image depth estimation.},
  keywords = {Coarse-level depth-map,Depth estimation,Estimation,Fine-level depth-map.,Generative adversarial networks,Generators,Laser radar,Robot sensing systems,Semantic map,Semantics,Single image,Training},
  file = {C:\Users\theun\Zotero\storage\358AHMAH\Hambarde et al. - 2020 - Depth Estimation From Single Image And Semantic Prior.pdf}
}

@article{hanCollaborativePerceptionAutonomous2023,
  title = {Collaborative {{Perception}} in {{Autonomous Driving}}: {{Methods}}, {{Datasets}}, and {{Challenges}}},
  shorttitle = {Collaborative {{Perception}} in {{Autonomous Driving}}},
  author = {Han, Yushan and Zhang, Hui and Li, Huifang and Jin, Yi and Lang, Congyan and Li, Yidong},
  year = 2023,
  month = nov,
  journal = {IEEE Intelligent Transportation Systems Magazine},
  volume = {15},
  number = {6},
  pages = {131--151},
  issn = {1941-1197},
  doi = {10.1109/MITS.2023.3298534},
  urldate = {2025-01-02},
  abstract = {Collaborative perception is essential to address occlusion and sensor failure issues in autonomous driving. In recent years, theoretical and experimental investigations of novel works for collaborative perception have increased tremendously. So far, however, few reviews have focused on systematical collaboration modules and large-scale collaborative perception datasets. This article reviews recent achievements in this field to bridge this gap and motivate future research. We start with a brief overview of collaboration schemes. After that, we systematically summarize the collaborative perception methods for ideal scenarios and real-world issues. The former focuses on collaboration modules and efficiency, and the latter is devoted to addressing the problems in actual application. Furthermore, we present large-scale public datasets and summarize quantitative results on these benchmarks. Finally, we highlight gaps and overlooked challenges between current academic research and real-world applications.},
  keywords = {/unread,Autonomous driving,Autonomous vehicles,Bandwidth,Collaboration,Data integration,Failure analysis,Point cloud compression,Safety,Task analysis},
  file = {C\:\\Users\\theun\\Zotero\\storage\\FWU2ZJKY\\Han et al. - 2023 - Collaborative Perception in Autonomous Driving Methods, Datasets, and Challenges.pdf;C\:\\Users\\theun\\Zotero\\storage\\565E4W83\\10248946.html}
}

@inproceedings{hanMonocularDepthEstimation2023,
  title = {Monocular Depth Estimation Based on {{Chained Residual Pooling}} and {{Gradient Weighted Loss}}},
  booktitle = {2023 3rd {{International Conference}} on {{Consumer Electronics}} and {{Computer Engineering}} ({{ICCECE}})},
  author = {Han, Jiajun and Jiang, Zhengang and Feng, Guanyuan},
  year = 2023,
  month = jan,
  pages = {278--282},
  doi = {10.1109/ICCECE58074.2023.10135509},
  urldate = {2025-06-30},
  abstract = {In recent years, the self-supervised monocular depth estimation task in the field of autonomous driving has achieved remarkable results. The brightness consistency assumption is adopted to guide network training, The image brightness needs to be kept constant in adjacent frames. However, this assumption does not apply to laparoscopic scenarios, where the intensity of light for the same tissue changes over time during surgery. In addition, the defined receptive fields in laparoscopy lead to low utilization of structured cues, the predicted depth map performs poorly on the tissue contour when the laparoscopic frame is fed into the depth estimation network. In this work, aiming at the problem of luminance consistency, it is proposed to integrate the second-order gradient of the image and the second-order gradient of the parallax map into the photometric reconstruction error to guide the network. In view of the problem of the low utilization rate of laparoscopic image context clues, the following clues are weighted in the decoder part of the network to improve the reuse of low-resolution feature maps for tissue contour clues. Experiments were performed on the SCARED dataset, and new losses and new modules were put into the network separately to train to verify their effectiveness, the results showed good performance on all four commonly used indicators.},
  keywords = {Brightness,chained residual pooling,depth estimation,Estimation,gradient weightd loss,Laparoscopes,Streaming media,Surgery,Training,Urban areas},
  file = {C:\Users\theun\Zotero\storage\Y7YD9Q79\Han et al. - 2023 - Monocular depth estimation based on Chained Residual Pooling and Gradient Weighted Loss.pdf}
}

@inproceedings{hashmiFeatEnHancerEnhancingHierarchical2023a,
  title = {{{FeatEnHancer}}: {{Enhancing Hierarchical Features}} for {{Object Detection}} and {{Beyond Under Low-Light Vision}}},
  shorttitle = {{{FeatEnHancer}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Hashmi, Khurram Azeem and Kallempudi, Goutham and Stricker, Didier and Afzal, Muhammad Zeshan},
  year = 2023,
  pages = {6725--6735},
  urldate = {2025-06-18},
  langid = {english},
  keywords = {Computer vision,Down-stream,Face recognition,Feature extraction,Hierarchical features,Image enhancement,Image representation,Low light,Machine perception,Machine Perception,Object detection,Object recognition,Objects detection,Pre-training,Semantic Segmentation,Semantics,Synthetic datasets,Transformation methods,Vision,Visual cues,Visual qualities},
  file = {C\:\\Users\\theun\\Zotero\\storage\\2DBE8IQL\\Hashmi et al. - 2023 - FeatEnHancer Enhancing Hierarchical Features for Object Detection and Beyond Under Low-Light Vision.pdf;C\:\\Users\\theun\\Zotero\\storage\\2VQ3L45F\\Hashmi et al. - 2023 - FeatEnHancer Enhancing Hierarchical Features for Object Detection and Beyond Under Low-Light Vision.pdf}
}

@article{heDeepLearningBased2025,
  title = {Deep Learning Based {{3D}} Segmentation in Computer Vision: {{A}} Survey},
  shorttitle = {Deep Learning Based {{3D}} Segmentation in Computer Vision},
  author = {He, Yong and Yu, Hongshan and Liu, Xiaoyan and Yang, Zhengeng and Sun, Wei and Anwar, Saeed and Mian, Ajmal},
  year = 2025,
  month = mar,
  journal = {Information Fusion},
  volume = {115},
  pages = {102722},
  issn = {15662535},
  doi = {10.1016/j.inffus.2024.102722},
  urldate = {2024-12-04},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\AJVYU4N6\He et al. - 2025 - Deep learning based 3D segmentation in computer vision A survey.pdf}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = 2016,
  month = jun,
  pages = {770--778},
  publisher = {IEEE},
  address = {Las Vegas, NV, USA},
  doi = {10.1109/CVPR.2016.90},
  urldate = {2025-06-15},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8\texttimes{} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\3LCZVGE7\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf}
}

@inproceedings{heMaskRCNN2017,
  title = {Mask {{R-CNN}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
  year = 2017,
  pages = {2961--2969},
  urldate = {2025-06-05},
  file = {C:\Users\theun\Zotero\storage\26CJBA5W\He et al. - 2017 - Mask R-CNN.pdf}
}

@article{hospedalesMetaLearningNeuralNetworks2022,
  title = {Meta-{{Learning}} in {{Neural Networks}}: {{A Survey}}},
  shorttitle = {Meta-{{Learning}} in {{Neural Networks}}},
  author = {Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  year = 2022,
  month = sep,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {9},
  pages = {5149--5169},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2021.3079209},
  urldate = {2025-01-03},
  abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.},
  keywords = {/unread,Deep learning,few-shot learning,learning-to-learn,Machine learning algorithms,Meta-learning,neural architecture search,Neural networks,Optimization,Predictive models,Task analysis,Training,transfer learning},
  file = {C\:\\Users\\theun\\Zotero\\storage\\ETM2NKPQ\\Hospedales et al. - 2022 - Meta-Learning in Neural Networks A Survey.pdf;C\:\\Users\\theun\\Zotero\\storage\\Z5ZPPAE6\\9428530.html}
}

@inproceedings{houSelfsupervisedMonocularDepth2024,
  title = {Self-Supervised {{Monocular Depth Estimation}} in {{Challenging Environments Based}} on {{Illumination Compensation PoseNet}}},
  booktitle = {2024 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Hou, Shengyu and Song, Wenjie and Wang, Rongchuan and Wang, Meiling and Yang, Yi and Fu, Mengyin},
  year = 2024,
  month = oct,
  pages = {9396--9403},
  issn = {2153-0866},
  doi = {10.1109/IROS58592.2024.10801331},
  urldate = {2025-06-30},
  abstract = {Self-supervised depth estimation has attracted much attention due to its ability to improve the 3D perception capabilities of unmanned systems. However, existing unsupervised frameworks rely on the assumption of photometric consistency, which may not hold in challenging environments such as night-time, rainy nights, or snowy winters due to complex lighting and reflections, resulting in inconsistent photometry across different frames for the same pixel. To address this problem, we propose a self-supervised monocular depth estimation unified framework that can handle these complex scenarios, which has the following characteristics: (1) an Illumination Compensation PoseNet (ICP) is designed, which is based on the classic Phong illumination theory and compensates for lighting changes in adjacent frames by estimating per-pixel transformations; (2) a Dual-Axis Transformer (DAT) block is proposed as the backbone network of the depth encoder, which infers the depth of local repeat-texture areas through spatial-channel dual-dimensional global context information of images. Experimental results demonstrate that our approach achieves state-of-the-art depth estimation results in complex environments on the challenging Oxford RobotCar dataset.},
  keywords = {Autonomous systems,Depth measurement,Intelligent robots,Lighting,Photometry,Reflection,Snow,Three-dimensional displays,Transformers},
  file = {C:\Users\theun\Zotero\storage\S9IHS2QF\Hou et al. - 2024 - Self-supervised Monocular Depth Estimation in Challenging Environments Based on Illumination Compens.pdf}
}

@article{houSelfSupervisedMonocularDepth2024a,
  title = {Self-{{Supervised Monocular Depth Estimation}} for {{All-Day Images Based}} on {{Dual-Axis Transformer}}},
  author = {Hou, Shengyu and Fu, Mengyin and Wang, Rongchuan and Yang, Yi and Song, Wenjie},
  year = 2024,
  month = oct,
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {34},
  number = {10},
  pages = {9939--9953},
  issn = {1558-2205},
  doi = {10.1109/TCSVT.2024.3406043},
  urldate = {2025-06-30},
  abstract = {All-day self-supervised monocular depth estimation has strong practical significance for autonomous systems to continuously perceive the 3D information of the world. However, night-time scenes pose challenges of weak texture and violating the brightness consistency assumption due to low illumination and varying lighting, respectively, which easily leads to most existing self-supervised models only being able to handle day-time scenes. To address this problem, we propose a self-supervised monocular depth estimation unified framework that can handle all-day scenarios, which has three features: 1) an Illumination Compensation PoseNet (ICP) is designed, which is based on the classic Phong illumination theory and compensates for lighting changes in adjacent frames by estimating per-pixel transformations; 2) a Dual-Axis Transformer (DAT) block is proposed as the backbone network of the depth encoder, which infers the depth of local low-illumination areas through spatial-channel dual-dimensional global context information of night-time images; 3) a cross-layer Adaptive Fusion Module (AFM) is introduced between multiple DAT blocks, which learns attention weights between different layer features and adaptively fuses cross-layer features using the learned weights, enhancing the complementarity of different layer features. This work was evaluated on multiple datasets, including: RobotCar, Waymo and KITTI datasets, achieving state-of-the-art results in both day-time and night-time scenarios.},
  keywords = {Adaptation models,Circuits and systems,Estimation,Light sources,Lighting,Monocular depth estimation,multi-task learning,Training,transformer network,Transformers,unsupervised estimation},
  file = {C:\Users\theun\Zotero\storage\VBF3D2Z9\Hou et al. - 2024 - Self-Supervised Monocular Depth Estimation for All-Day Images Based on Dual-Axis Transformer.pdf}
}

@article{huangImprovingConventionalTransit2025,
  title = {Improving Conventional Transit Services with Modular Autonomous Vehicles: {{A}} Bi-Level Programming Approach},
  shorttitle = {Improving Conventional Transit Services with Modular Autonomous Vehicles},
  author = {Huang, Di and Hu, Zhitao and Tian, Jingyang and Tu, Ran},
  year = 2025,
  month = apr,
  journal = {Travel Behaviour and Society},
  volume = {39},
  pages = {100939},
  issn = {2214367X},
  doi = {10.1016/j.tbs.2024.100939},
  urldate = {2024-12-04},
  abstract = {Modular autonomous bus technology is highly anticipated to be introduced into public transit systems with a gradually increasing market penetration, and it will be operated within a multimodal public transport system with a mixed fleet of autonomous buses and conventional human-driving buses. This study establishes a bi-level programming model for the coordinated optimization of hybrid transit system, considering transit company and passengers with conflicting objectives in passenger flow assignment and minimizing the systematic cost from operational and environmental perspectives. Results show that the integration of autonomous buses significantly improves the sustainability of the transit system, reducing operating costs by 25.1\% and environmental costs by 27.72\% compared to the conventional transit system. While reducing passenger waiting times, it also decreases carbon emissions of the current transit system. This study concludes with some managerial insights into the introduction of autonomous buses.},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\5UMBJ3GK\Huang et al. - 2025 - Improving conventional transit services with modular autonomous vehicles A bi-level programming app.pdf}
}

@article{huangLowLightStereoImage2023,
  title = {Low-{{Light Stereo Image Enhancement}}},
  author = {Huang, Jie and Fu, Xueyang and Xiao, Zeyu and Zhao, Feng and Xiong, Zhiwei},
  year = 2023,
  journal = {IEEE Transactions on Multimedia},
  volume = {25},
  pages = {2978--2992},
  issn = {1941-0077},
  doi = {10.1109/TMM.2022.3154152},
  urldate = {2025-03-15},
  abstract = {Stereo cameras are now commonly used in more and more devices. Nevertheless, visually unpleasant images captured under low-light conditions hinder their practical application. As an initial attempt at low-light stereo image enhancement, we propose a novel Dual-View Enhancement Network (DVENet) based on the Retinex theory, which consists of two stages. The first stage estimates an illumination map to obtain a coarse enhancement result, which boosts the correlation of two views, while the second stage recovers details by integrating the information from two views to achieve fine image quality improvement with the guidance of the illumination map. To fully utilize the dual-view correlation, we further design a wavelet-based view transfer module to efficiently carry out multi-scale detail recovery. Then, we design an illumination-aware attention fusion module to exploit the complementarity between the fused features from two views and the single-view features. Experiments on both synthetic and real-world stereo datasets demonstrate the superiority of our proposed method over existing solutions. The code and model are publicly available at: https://github.com/KevinJ-Huang/Stereo-Low-Light.},
  keywords = {Correlation,deep learning,Image enhancement,Image reconstruction,Image restoration,Lighting,low-light enhancement,retinex,Stereo images,Superresolution,Task analysis},
  file = {C\:\\Users\\theun\\Zotero\\storage\\SQQAESXC\\Huang et al. - 2023 - Low-Light Stereo Image Enhancement.pdf;C\:\\Users\\theun\\Zotero\\storage\\7A98P7RF\\9720943.html}
}

@article{huangRealTimeAttentiveDilated2024,
  title = {Real-{{Time Attentive Dilated U-Net}} for {{Extremely Dark Image Enhancement}}},
  author = {Huang, J. and Ren, H. and Liu, S. and Liu, Y. and Lv, C. and Lu, J. and Xie, C. and Lu, H.},
  year = 2024,
  journal = {ACM Transactions on Multimedia Computing, Communications and Applications},
  volume = {20},
  number = {8},
  publisher = {Association for Computing Machinery},
  doi = {10.1145/3654668},
  abstract = {Images taken under low-light conditions suffer from poor visibility, color distortion, and graininess, all of which degrade the image quality and hamper the performance of downstream vision tasks, such as object detection and instance segmentation in the field of autonomous driving, making low-light enhancement an indispensable basic component of high-level visual tasks. Low-light enhancement aims to mitigate these issues, and has garnered extensive attention and research over several decades. The primary challenge in low-light image enhancement arises from the low signal-to-noise ratio caused by insufficient lighting. This challenge becomes even more pronounced in near-zero lux conditions, where noise overwhelms the available image information. Both traditional image signal processing pipeline and conventional low-light image enhancement methods struggle in such scenarios. Recently, deep neural networks have been used to address this challenge. These networks take unmodified RAW images as input and produce the enhanced sRGB images, forming a deep learning based image signal processing pipeline. However, most of these networks are computationally expensive and thus far from practical use. In this article, we propose a lightweight model called attentive dilated U-Net (ADU-Net) to tackle this issue. Our model incorporates several innovative designs, including an asymmetric U-shape architecture, dilated residual modules for feature extraction, and attentive fusion modules for feature fusion. The dilated residual modules provide strong representative capability, whereas the attentive fusion modules effectively leverage low-level texture information and high-level semantic information within the network. Both modules employ a lightweight design but offer significant performance gains. Extensive experiments demonstrate that our method is highly effective, achieving an excellent balance between image quality and computational complexity - that is, taking less than 4ms for a high-definition 4K image on a single GTX 1080Ti GPU and yet maintaining competitive visual quality. Furthermore, our method exhibits pleasing scalability and generalizability, highlighting its potential for widespread applicability. \copyright{} 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
  file = {C:\Users\theun\Zotero\storage\GPNSUZSP\Huang et al. - 2024 - Real-Time Attentive Dilated U-Net for Extremely Dark Image Enhancement.pdf}
}

@article{huchMultiTaskEndtoEndSelfDriving2021,
  title = {Multi-{{Task End-to-End Self-Driving Architecture}} for {{CAV Platoons}}},
  author = {Huch, Sebastian and Ongel, Aybike and Betz, Johannes and Lienkamp, Markus},
  year = 2021,
  month = jan,
  journal = {Sensors},
  volume = {21},
  number = {4},
  pages = {1039},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s21041039},
  urldate = {2025-01-05},
  abstract = {Connected and autonomous vehicles (CAVs) could reduce emissions, increase road safety, and enhance ride comfort. Multiple CAVs can form a CAV platoon with a close inter-vehicle distance, which can further improve energy efficiency, save space, and reduce travel time. To date, there have been few detailed studies of self-driving algorithms for CAV platoons in urban areas. In this paper, we therefore propose a self-driving architecture combining the sensing, planning, and control for CAV platoons in an end-to-end fashion. Our multi-task model can switch between two tasks to drive either the leading or following vehicle in the platoon. The architecture is based on an end-to-end deep learning approach and predicts the control commands, i.e., steering and throttle/brake, with a single neural network. The inputs for this network are images from a front-facing camera, enhanced by information transmitted via vehicle-to-vehicle (V2V) communication. The model is trained with data captured in a simulated urban environment with dynamic traffic. We compare our approach with different concepts used in the state-of-the-art end-to-end self-driving research, such as the implementation of recurrent neural networks or transfer learning. Experiments in the simulation were conducted to test the model in different urban environments. A CAV platoon consisting of two vehicles, each controlled by an instance of the network, completed on average 67\% of the predefined point-to-point routes in the training environment and 40\% in a never-seen-before environment. Using V2V communication, our approach eliminates casual confusion for the following vehicle, which is a known limitation of end-to-end self-driving.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {artificial neural networks,connected and autonomous vehicles,end-to-end learning,multi-task learning,simulation,urban vehicle platooning},
  file = {C:\Users\theun\Zotero\storage\BCJQGHTN\Huch et al. - 2021 - Multi-Task End-to-End Self-Driving Architecture for CAV Platoons.pdf}
}

@techreport{hummelImageEnhancementHistogram1975,
  title = {Image Enhancement by Histogram Transformation},
  author = {Hummel, R.},
  year = 1975,
  month = sep,
  journal = {Unknown},
  urldate = {2025-05-26},
  abstract = {A number of simple and inexpensive enhancement techniques are suggested. These techniques attempt to make use of easily computed local context features to aid in the reassignment of each point's gray level during histogram transformation.},
  keywords = {/unread,Histograms,Images,Imaging Techniques,Optics,Pattern Recognition,Visual Perception},
  annotation = {ADS Bibcode: 1975ieht.rept.....H}
}

@article{husseinRetinexTheoryColor2019,
  title = {Retinex Theory for Color Image Enhancement: {{A}} Systematic Review},
  shorttitle = {Retinex Theory for Color Image Enhancement},
  author = {Hussein, Ruaa Riyadh and Hamodi, Yaser Issam and Sabri, Rooa Adnan},
  year = 2019,
  month = dec,
  journal = {International Journal of Electrical and Computer Engineering (IJECE)},
  volume = {9},
  number = {6},
  pages = {5560},
  issn = {2088-8708, 2088-8708},
  doi = {10.11591/ijece.v9i6.pp5560-5569},
  urldate = {2025-01-15},
  abstract = {A short but comprehensive review of Retinex has been presented in this paper. Retinex theory aims to explain human color perception. In addition, its derivation on modifying the reflectance components has introduced effective approaches for images contrast enhancement. In this review, the classical theory of Retinex has been covered. Moreover, advance and improved techniques of Retinex, proposed in the literature, have been addressed. Strength and weakness aspects of each technique are discussed and compared. An optimum parameter is needed to be determined to define the image degradation level. Such parameter determination would help in quantifying the amount of adjustment in the Retinex theory. Thus, a robust framework to modify the reflectance component of the Retinex theory can be developed to enhance the overall quality of color images.},
  copyright = {http://creativecommons.org/licenses/by-nc/4.0},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\8UESLM6W\Hussein et al. - 2019 - Retinex theory for color image enhancement A systematic review.pdf}
}

@article{hyyppaCanPerceptionData2023,
  title = {Can the {{Perception Data}} of {{Autonomous Vehicles Be Used}} to {{Replace Mobile Mapping Surveys}}?---{{A Case Study Surveying Roadside City Trees}}},
  shorttitle = {Can the {{Perception Data}} of {{Autonomous Vehicles Be Used}} to {{Replace Mobile Mapping Surveys}}?},
  author = {Hyypp{\"a}, Eric and Manninen, Petri and Maanp{\"a}{\"a}, Jyri and Taher, Josef and Litkey, Paula and Hyyti, Heikki and Kukko, Antero and Kaartinen, Harri and Ahokas, Eero and Yu, Xiaowei and Muhojoki, Jesse and Lehtom{\"a}ki, Matti and Virtanen, Juho-Pekka and Hyypp{\"a}, Juha},
  year = 2023,
  month = mar,
  journal = {Remote Sensing},
  volume = {15},
  number = {7},
  pages = {1790},
  issn = {2072-4292},
  doi = {10.3390/rs15071790},
  urldate = {2024-12-04},
  abstract = {The continuous flow of autonomous vehicle-based data could revolutionize current map updating procedures and allow completely new types of mapping applications. Therefore, in this article, we demonstrate the feasibility of using perception data of autonomous vehicles to replace traditionally conducted mobile mapping surveys with a case study focusing on updating a register of roadside city trees. In our experiment, we drove along a 1.3-km-long road in Helsinki to collect laser scanner data using our autonomous car platform ARVO, which is based on a Ford Mondeo hybrid passenger vehicle equipped with a Velodyne VLS-128 Alpha Prime scanner and other high-grade sensors for autonomous perception. For comparison, laser scanner data from the same region were also collected with a specially-planned high-grade mobile mapping laser scanning system. Based on our results, the diameter at breast height, one of the key parameters of city tree registers, could be estimated with a lower root-mean-square error from the perception data of the autonomous car than from the specially-planned mobile laser scanning survey, provided that time-based filtering was included in the post-processing of the autonomous perception data to mitigate distortions in the obtained point cloud. Therefore, appropriately performed post-processing of the autonomous perception data can be regarded as a viable option for keeping maps updated in road environments. However, point cloud-processing algorithms may need to be adapted for the post-processing of autonomous perception data due to the differences in the sensors and their arrangements compared to designated mobile mapping systems. We also emphasize that time-based filtering may be required in the post-processing of autonomous perception data due to point cloud distortions around objects seen at multiple times. This highlights the importance of saving the time stamp for each data point in the autonomous perception data or saving the temporal order of the data points.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\E8M98LU2\Hyyppä et al. - 2023 - Can the Perception Data of Autonomous Vehicles Be Used to Replace Mobile Mapping Surveys—A Case Stu.pdf}
}

@misc{ImageSegmentationNightvision,
  title = {Image Segmentation for Night-Vision Surveillance Camera Based on Deep Learning},
  urldate = {2025-06-15},
  howpublished = {https://www-spiedigitallibrary-org.ezproxy-f.deakin.edu.au/conference-proceedings-of-spie/12478/1247836/Image-segmentation-for-night-vision-surveillance-camera-based-on-deep/10.1117/12.2654811.full},
  file = {C:\Users\theun\Zotero\storage\IBZUW4SH\12.2654811.html}
}

@misc{jainOneFormerOneTransformer2022,
  title = {{{OneFormer}}: {{One Transformer}} to {{Rule Universal Image Segmentation}}},
  shorttitle = {{{OneFormer}}},
  author = {Jain, Jitesh and Li, Jiachen and Chiu, MangTik and Hassani, Ali and Orlov, Nikita and Shi, Humphrey},
  year = 2022,
  month = dec,
  number = {arXiv:2211.06220},
  eprint = {2211.06220},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.06220},
  urldate = {2025-08-26},
  abstract = {Universal Image Segmentation is not a new concept. Past attempts to unify image segmentation in the last decades include scene parsing, panoptic segmentation, and, more recently, new panoptic architectures. However, such panoptic architectures do not truly unify image segmentation because they need to be trained individually on the semantic, instance, or panoptic segmentation to achieve the best performance. Ideally, a truly universal framework should be trained only once and achieve SOTA performance across all three image segmentation tasks. To that end, we propose OneFormer, a universal image segmentation framework that unifies segmentation with a multi-task train-once design. We first propose a task-conditioned joint training strategy that enables training on ground truths of each domain (semantic, instance, and panoptic segmentation) within a single multi-task training process. Secondly, we introduce a task token to condition our model on the task at hand, making our model task-dynamic to support multi-task training and inference. Thirdly, we propose using a query-text contrastive loss during training to establish better inter-task and inter-class distinctions. Notably, our single OneFormer model outperforms specialized Mask2Former models across all three segmentation tasks on ADE20k, CityScapes, and COCO, despite the latter being trained on each of the three tasks individually with three times the resources. With new ConvNeXt and DiNAT backbones, we observe even more performance improvement. We believe OneFormer is a significant step towards making image segmentation more universal and accessible. To support further research, we open-source our code and models at https://github.com/SHI-Labs/OneFormer},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\JIE73FVP\\Jain et al. - 2022 - OneFormer One Transformer to Rule Universal Image Segmentation.pdf;C\:\\Users\\theun\\Zotero\\storage\\VB6PAHQC\\2211.html}
}

@misc{jenicekNoFearDark2019,
  title = {No {{Fear}} of the {{Dark}}: {{Image Retrieval}} under {{Varying Illumination Conditions}}},
  shorttitle = {No {{Fear}} of the {{Dark}}},
  author = {Jenicek, Tomas and Chum, Ond{\v r}ej},
  year = 2019,
  month = aug,
  number = {arXiv:1908.08999},
  eprint = {1908.08999},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1908.08999},
  urldate = {2025-06-03},
  abstract = {Image retrieval under varying illumination conditions, such as day and night images, is addressed by image preprocessing, both hand-crafted and learned. Prior to extracting image descriptors by a convolutional neural network, images are photometrically normalised in order to reduce the descriptor sensitivity to illumination changes. We propose a learnable normalisation based on the U-Net architecture, which is trained on a combination of single-camera multi-exposure images and a newly constructed collection of similar views of landmarks during day and night. We experimentally show that both hand-crafted normalisation based on local histogram equalisation and the learnable normalisation outperform standard approaches in varying illumination conditions, while staying on par with the state-of-the-art methods on daylight illumination benchmarks, such as Oxford or Paris datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\UWKTT2HU\\Jenicek and Chum - 2019 - No Fear of the Dark Image Retrieval under Varying Illumination Conditions.pdf;C\:\\Users\\theun\\Zotero\\storage\\KLMKCVCG\\1908.html}
}

@inproceedings{jeonEnhancingSpatialResolution2018,
  title = {Enhancing the {{Spatial Resolution}} of {{Stereo Images Using}} a {{Parallax Prior}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Jeon, Daniel S. and Baek, Seung-Hwan and Choi, Inchang and Kim, Min H.},
  year = 2018,
  pages = {1721--1730},
  urldate = {2025-03-24},
  file = {C:\Users\theun\Zotero\storage\PYFRKYMR\Jeon et al. - 2018 - Enhancing the Spatial Resolution of Stereo Images Using a Parallax Prior.pdf}
}

@misc{jeonRainSDRainStyle2023,
  title = {{{RainSD}}: {{Rain Style Diversification Module}} for {{Image Synthesis Enhancement}} Using {{Feature-Level Style Distribution}}},
  shorttitle = {{{RainSD}}},
  author = {Jeon, Hyeonjae and Seo, Junghyun and Kim, Taesoo and Son, Sungho and Lee, Jungki and Choi, Gyeungho and Lim, Yongseob},
  year = 2023,
  month = dec,
  number = {arXiv:2401.00460},
  eprint = {2401.00460},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.00460},
  urldate = {2024-12-04},
  abstract = {Autonomous driving technology nowadays targets to level 4 or beyond, but the researchers are faced with some limitations for developing reliable driving algorithms in diverse challenges. To promote the autonomous vehicles to spread widely, it is important to address safety issues on this technology. Among various safety concerns, the sensor blockage problem by severe weather conditions can be one of the most frequent threats for multi-task learning based perception algorithms during autonomous driving. To handle this problem, the importance of the generation of proper datasets is becoming more significant. In this paper, a synthetic road dataset with sensor blockage generated from real road dataset BDD100K is suggested in the format of BDD100K annotation. Rain streaks for each frame were made by an experimentally established equation and translated utilizing the image-to-image translation network based on style transfer. Using this dataset, the degradation of the diverse multi-task networks for autonomous driving, such as lane detection, driving area segmentation, and traffic object detection, has been thoroughly evaluated and analyzed. The tendency of the performance degradation of deep neural network-based perception systems for autonomous vehicle has been analyzed in depth. Finally, we discuss the limitation and the future directions of the deep neural networkbased perception algorithms and autonomous driving dataset generation based on image-to-image translation based.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {/unread,Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\theun\Zotero\storage\NEAZTXCV\Jeon et al. - 2023 - RainSD Rain Style Diversification Module for Image Synthesis Enhancement using Feature-Level Style.pdf}
}

@inproceedings{jeonStereoMatchingColor2016,
  title = {Stereo {{Matching}} with {{Color}} and {{Monochrome Cameras}} in {{Low-Light Conditions}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Jeon, Hae-Gon and Lee, Joon-Young and Im, Sunghoon and Ha, Hyowon and Kweon, In So},
  year = 2016,
  month = jun,
  pages = {4086--4094},
  publisher = {IEEE},
  address = {Las Vegas, NV, USA},
  doi = {10.1109/CVPR.2016.443},
  urldate = {2025-05-06},
  abstract = {Consumer devices with stereo cameras have become popular because of their low-cost depth sensing capability. However, those systems usually suffer from low imaging quality and inaccurate depth acquisition under low-light conditions. To address the problem, we present a new stereo matching method with a color and monochrome camera pair. We focus on the fundamental trade-off that monochrome cameras have much better light-efficiency than color-filtered cameras. Our key ideas involve compensating for the radiometric difference between two crossspectral images and taking full advantage of complementary data. Consequently, our method produces both an accurate depth map and high-quality images, which are applicable for various depth-aware image processing. Our method is evaluated using various datasets and the performance of our depth estimation consistently outperforms state-of-the-art methods.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\MHWGQURU\Jeon et al. - 2016 - Stereo Matching with Color and Monochrome Cameras in Low-Light Conditions.pdf}
}

@inproceedings{jiaLLVIPVisibleinfraredPaired2021,
  title = {{{LLVIP}}: {{A Visible-infrared Paired Dataset}} for {{Low-light Vision}}},
  shorttitle = {{{LLVIP}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision Workshops}} ({{ICCVW}})},
  author = {Jia, Xinyu and Zhu, Chuang and Li, Minzhen and Tang, Wenqi and Zhou, Wenli},
  year = 2021,
  month = oct,
  pages = {3489--3497},
  publisher = {IEEE},
  address = {Montreal, BC, Canada},
  doi = {10.1109/ICCVW54120.2021.00389},
  urldate = {2025-05-15},
  abstract = {It is very challenging for various visual tasks such as image fusion, pedestrian detection and image-to-image translation in low light conditions due to the loss of effective target areas. In this case, infrared and visible images can be used together to provide both rich detail information and effective target areas. In this paper, we present LLVIP, a visible-infrared paired dataset for low-light vision. This dataset contains 33672 images, or 16836 pairs, most of which were taken at very dark scenes, and all of the images are strictly aligned in time and space. Pedestrians in the dataset are labeled. We compare the dataset with other visible-infrared datasets and evaluate the performance of some popular visual algorithms including image fusion, pedestrian detection and image-to-image translation on the dataset. The experimental results demonstrate the complementary effect of fusion on image information, and find the deficiency of existing algorithms of the three visual tasks in very low-light conditions. We believe the LLVIP dataset will contribute to the community of computer vision by promoting image fusion, pedestrian detection and image-to-image translation in very low-light applications. The dataset is being released in https://bupt-ai-cz.github.io/ LLVIP/.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-0191-3},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\KB8RNFD4\Jia et al. - 2021 - LLVIP A Visible-infrared Paired Dataset for Low-light Vision.pdf}
}

@article{jiangDarksegLightweightEdgeoptimized2025,
  title = {Darkseg: A Lightweight and Edge-Optimized Network for Nighttime Semantic Segmentation},
  shorttitle = {Darkseg},
  author = {Jiang, Jiao and Xu, Yang and Cao, Bin and Xiao, Ci},
  year = 2025,
  month = jul,
  journal = {Signal, Image and Video Processing},
  volume = {19},
  number = {7},
  pages = {1--9},
  publisher = {Springer London},
  issn = {1863-1711},
  doi = {10.1007/s11760-025-04108-5},
  urldate = {2025-06-03},
  abstract = {Semantic segmentation of nighttime road scenes is a critical task in autonomous driving. However, challenges such as insufficient lighting, over-exposure, and the scarcity of labeled data make this task particularly difficult. To narrow the gap between day and night domains, we introduce DarkSeg, a lightweight unsupervised domain adaptation network that offers an effective solution to these challenges. First, adversarial training techniques are utilized to achieve domain adaptation from the daytime dataset cityscapes to nighttime scenes, mitigating the issue of data scarcity. Second, the DarkSeg reduces inter-domain illumination differences through an enhanced low-light image restoration module (DarkNet), improving segmentation accuracy under low-light conditions. Third, the Polarized Self-Attention (PSA) module is integrated into the bottleneck module of the segmentation network, PASeg, to improve the representation of edge features under low-light conditions. Additionally, a boundary-based loss function (EdgeLoss) is introduced to improve the accuracy of edge pixels, thereby enhancing the overall clarity of the segmentation results. Experimental results show that DarkSeg performs significantly on the Dark Zurich and Nighttime Driving benchmark datasets, with mean Interaction over Union (mIoU) scores of 48.13 \$\$\textbackslash\%\$\$ \% and 49.72 \$\$\textbackslash\%\$\$ \% , respectively. This represents an improvement of 2.93 and 2.02 percentage points over the existing DANNet method. These results demonstrate DarkSeg's superior performance in terms of network robustness and accuracy, which renders it especially suitable for semantic segmentation tasks at night.},
  copyright = {2025 The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\GVERVQFV\Jiang et al. - 2025 - Darkseg a lightweight and edge-optimized network for nighttime semantic segmentation.pdf}
}

@article{jiangEnlightenGANDeepLight2021,
  title = {{{EnlightenGAN}}: {{Deep Light Enhancement Without Paired Supervision}}},
  shorttitle = {{{EnlightenGAN}}},
  author = {Jiang, Yifan and Gong, Xinyu and Liu, Ding and Cheng, Yu and Fang, Chen and Shen, Xiaohui and Yang, Jianchao and Zhou, Pan and Wang, Zhangyang},
  year = 2021,
  journal = {IEEE Transactions on Image Processing},
  volume = {30},
  pages = {2340--2349},
  issn = {1941-0042},
  doi = {10.1109/TIP.2021.3051462},
  urldate = {2025-03-10},
  abstract = {Deep learning-based methods have achieved remarkable success in image restoration and enhancement, but are they still competitive when there is a lack of paired training data? As one such example, this paper explores the low-light image enhancement problem, where in practice it is extremely challenging to simultaneously take a low-light and a normal-light photo of the same visual scene. We propose a highly effective unsupervised generative adversarial network, dubbed EnlightenGAN, that can be trained without low/normal-light image pairs, yet proves to generalize very well on various real-world test images. Instead of supervising the learning using ground truth data, we propose to regularize the unpaired training using the information extracted from the input itself, and benchmark a series of innovations for the low-light image enhancement problem, including a global-local discriminator structure, a self-regularized perceptual loss fusion, and the attention mechanism. Through extensive experiments, our proposed approach outperforms recent methods under a variety of metrics in terms of visual quality and subjective user study. Thanks to the great flexibility brought by unpaired training, EnlightenGAN is demonstrated to be easily adaptable to enhancing real-world images from various domains. Our codes and pre-trained models are available at: https://github.com/VITA-Group/EnlightenGAN.},
  keywords = {Adaptation models,Gallium nitride,generative adversarial networks,Generative adversarial networks,Lighting,Low-light enhancement,Training,Training data,unsupervised learning,Visualization},
  file = {C\:\\Users\\theun\\Zotero\\storage\\PJ7E5HS3\\Jiang et al. - 2021 - EnlightenGAN Deep Light Enhancement Without Paired Supervision.pdf;C\:\\Users\\theun\\Zotero\\storage\\H2AEGWV8\\9334429.html}
}

@inproceedings{jiangLightenDiffusionUnsupervisedLowLight2025,
  title = {{{LightenDiffusion}}: {{Unsupervised Low-Light Image Enhancement}} with~{{Latent-Retinex Diffusion Models}}},
  shorttitle = {{{LightenDiffusion}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2024},
  author = {Jiang, Hai and Luo, Ao and Liu, Xiaohong and Han, Songchen and Liu, Shuaicheng},
  editor = {Leonardis, Ale{\v s} and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, G{\"u}l},
  year = 2025,
  pages = {161--179},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-73195-2_10},
  abstract = {In this paper, we propose a diffusion-based unsupervised framework that incorporates physically explainable Retinex theory with diffusion models for low-light image enhancement, named LightenDiffusion. Specifically, we present a content-transfer decomposition network that performs Retinex decomposition within the latent space instead of image space as in previous approaches, enabling the encoded features of unpaired low-light and normal-light images to be decomposed into content-rich reflectance maps and content-free illumination maps. Subsequently, the reflectance map of the low-light image and the illumination map of the normal-light image are taken as input to the diffusion model for unsupervised restoration with the guidance of the low-light feature, where a self-constrained consistency loss is further proposed to eliminate the interference of normal-light content on the restored results to improve overall visual quality. Extensive experiments on publicly available real-world benchmarks show that the proposed LightenDiffusion outperforms state-of-the-art unsupervised competitors and is comparable to supervised methods while being more generalizable to various scenes. Our code is available at https://github.com/JianghaiSCU/LightenDiffusion.},
  isbn = {978-3-031-73195-2},
  langid = {english},
  keywords = {Diffusion models,Image restoration,Low-light image enhancement,Retinex theory},
  file = {C:\Users\theun\Zotero\storage\BGDSGBVZ\Jiang et al. - 2025 - LightenDiffusion Unsupervised Low-Light Image Enhancement with Latent-Retinex Diffusion Models.pdf}
}

@article{jiBoostingPerformanceLLIE2024,
  title = {Boosting the {{Performance}} of {{LLIE Methods}} via {{Unsupervised Weight Map Generation Network}}},
  author = {Ji, Shuichen and Xu, Shaoping and Xiao, Nan and Cheng, Xiaohui and Chen, Qiyu and Jiang, Xinyi},
  year = 2024,
  month = jan,
  journal = {Applied Sciences},
  volume = {14},
  number = {12},
  pages = {4962},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  doi = {10.3390/app14124962},
  urldate = {2025-10-17},
  abstract = {Over the past decade, significant advancements have been made in low-light image enhancement (LLIE) methods due to the robust capabilities of deep learning in non-linear mapping, feature extraction, and representation. However, the pursuit of a universally superior method that consistently outperforms others across diverse scenarios remains challenging. This challenge primarily arises from the inherent data bias in deep learning-based approaches, stemming from disparities in image statistical distributions between training and testing datasets. To tackle this problem, we propose an unsupervised weight map generation network aimed at effectively integrating pre-enhanced images generated from carefully selected complementary LLIE methods. Our ultimate goal is to enhance the overall enhancement performance by leveraging these pre-enhanced images, therewith culminating the enhancement workflow in a dual-stage execution paradigm. To be more specific, in the preprocessing stage, we initially employ two distinct LLIE methods, namely Night and PairLIE, chosen specifically for their complementary enhancement characteristics, to process the given input low-light image. The resultant outputs, termed pre-enhanced images, serve as dual target images for fusion in the subsequent image fusion stage. Subsequently, at the fusion stage, we utilize an unsupervised UNet architecture to determine the optimal pixel-level weight maps for merging the pre-enhanced images. This process is adeptly directed by a specially formulated loss function in conjunction with the no-reference image quality algorithm, namely the naturalness image quality evaluator (NIQE). Finally, based on a mixed weighting mechanism that combines generated pixel-level local weights with image-level global empirical weights, the pre-enhanced images are fused to produce the final enhanced image. Our experimental findings demonstrate exceptional performance across a range of datasets, surpassing various state-of-the-art methods, including two pre-enhancement methods, involved in the comparison. This outstanding performance is attributed to the harmonious integration of diverse LLIE methods, which yields robust and high-quality enhancement outcomes across various scenarios. Furthermore, our approach exhibits scalability and adaptability, ensuring compatibility with future advancements in enhancement technologies while maintaining superior performance in this rapidly evolving field.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {/unread,boosting enhancing effect,complementarity,flexibility and scalability,low-light image enhancement,two-stage hybrid strategy,unsupervised optimization strategy,weight map generation network},
  file = {C:\Users\theun\Zotero\storage\G9MUIRGA\Ji et al. - 2024 - Boosting the Performance of LLIE Methods via Unsupervised Weight Map Generation Network.pdf}
}

@article{jinimArtificialIntelligenceBasedLowLightMarine2025,
  title = {Artificial-{{Intelligence-Based Low-Light Marine Image Enhancement}} for {{Semantic Segmentation}} in {{Edge-Intelligence-Empowered Internet}} of {{Things Environment}}},
  author = {Jin Im, Su and Yun, Chaeyeong and Jae Lee, Sung and Park, Kang Ryoung},
  year = 2025,
  month = feb,
  journal = {IEEE Internet of Things Journal},
  volume = {12},
  number = {4},
  pages = {4086--4114},
  issn = {2327-4662},
  doi = {10.1109/JIOT.2024.3482453},
  urldate = {2025-06-18},
  abstract = {For accurate detection of marine life to utilize marine resources while ensuring protection of ecosystem, marine animal segmentation (MAS) has been widely researched. Furthermore, development of autonomous underwater vehicle (AUV) has expanded the scope of marine ecosystem research into deep sea where AUV utilizes artificial light sources to address the problem of low-light conditions. However, these light sources can disturb the ecosystem. In addition, extremely low-light images are acquired in areas distant from AUV due to the limitations of the light sources, such as limited field of view, resulting in poor quality of underwater images. Therefore, we propose multiscale features and residual dual attention-based low-light image enhancement network (MRLE-Net) for semantic segmentation of marine images. To preserve fine-grained information under low-light environment and reduce noise, MRLE-Net introduces dual feature extraction, multiscale feature extraction, and residual dual attention blocks. Furthermore, to improve the semantic segmentation accuracy, it employs a discrete wavelet transform-based loss function. In experiments using two open databases of MAS3K and DeepFish, the mean intersection of union values of semantic segmentation by our method are 78.72\% and 83.62\%, respectively, showing superior accuracy to the state-of-the-art methods. In addition, our MRLE-Net demonstrates its ability to operate on embedded system with low-computational resources as edge computing. From them, we confirm that it can be adopted to AUV in edge intelligence empowered Internet of Things environment by removing communication overheads caused by transmitting lots of images from AUV's camera to and receiving the segmentation result from high-computing cloud by 5G technology.},
  keywords = {Accuracy,Artificial intelligence,autonomous underwater vehicle (AUV),Autonomous underwater vehicles],Edge intelligence,Edge intelligence empowered internet of thing,edge intelligence empowered Internet of Things,Feature extraction,Image color analysis,Image edge detection,Image enhancement,Internet of Things,Invertebrates,Laser beams,Low light,Low-light image enhancement,low-light image enhancement (LLIE),Low-light images,Marine animals,Marine communication,Multi-scale features,Semantic segmentation,Semantic Segmentation,semantic segmentation of marine animal,Semantic segmentation of marine animal,Semantics,Signal to noise ratio,Transformers,Underwater imaging,Underwater photography,Wavelet transforms},
  file = {C:\Users\theun\Zotero\storage\URLKW6M6\Jin Im et al. - 2025 - Artificial-Intelligence-Based Low-Light Marine Image Enhancement for Semantic Segmentation in Edge-I.pdf}
}

@inproceedings{jinUnsupervisedNightImage2022,
  title = {Unsupervised {{Night Image Enhancement}}: {{When Layer Decomposition Meets Light-Effects Suppression}}},
  shorttitle = {Unsupervised {{Night Image Enhancement}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2022},
  author = {Jin, Yeying and Yang, Wenhan and Tan, Robby T.},
  year = 2022,
  pages = {404--421},
  publisher = {Springer, Cham},
  issn = {1611-3349},
  doi = {10.1007/978-3-031-19836-6_23},
  urldate = {2025-03-10},
  abstract = {Night images suffer not only from low light, but also from uneven distributions of light. Most existing night visibility enhancement methods focus mainly on enhancing low-light regions. This inevitably leads to over enhancement and saturation in bright regions, such...},
  isbn = {978-3-031-19836-6},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\CZHAAYRF\Jin et al. - 2022 - Unsupervised Night Image Enhancement When Layer Decomposition Meets Light-Effects Suppression.pdf}
}

@article{jiRDRFNetPyramidArchitecture2021,
  title = {{{RDRF-Net}}: {{A}} Pyramid Architecture Network with Residual-Based Dynamic Receptive Fields for Unsupervised Depth Estimation},
  shorttitle = {{{RDRF-Net}}},
  author = {Ji, Zhen-yan and Song, Xiao-jun and Song, Hou-bin and Yang, Hong and Guo, Xiao-xuan},
  year = 2021,
  month = oct,
  journal = {Neurocomputing},
  volume = {457},
  pages = {1--12},
  issn = {09252312},
  doi = {10.1016/j.neucom.2021.05.089},
  urldate = {2025-06-30},
  abstract = {Image depth estimation is a challenging problem in computer vision, especially considering both high accuracy and low run time. To save run time and maintain high accuracy, we present a new lightweight model in this paper, i.e., a Residual-based Dynamic Receptive Field Network (RDRF-Net). This model can automatically select the receptive fields suitable for different image scales to generate the depth maps with higher fitting degrees. Residual design and bottleneck layers are used to compress the network for reducing run time. Three groups of experiments are performed on the KITTI dataset to test the accuracy, computation time, and the impact of dynamic receptive fields. Experimental results show that RDRF-Net has comparable accuracy with Godard's model and significantly outperforms it in terms of run time. In addition, it performs closely to Pyd-Net in terms of run time and beats Pyd-Net's accuracy. Experiments also demonstrate the beneficial impact of dynamic receptive fields on improving depth estimation accuracy.},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\AFJEMNYK\Ji et al. - 2021 - RDRF-Net A pyramid architecture network with residual-based dynamic receptive fields for unsupervis.pdf}
}

@article{jobsonMultiscaleRetinexBridging1997,
  title = {A Multiscale Retinex for Bridging the Gap between Color Images and the Human Observation of Scenes},
  author = {Jobson, D.J. and Rahman, Z. and Woodell, G.A.},
  year = 1997,
  month = jul,
  journal = {IEEE Transactions on Image Processing},
  volume = {6},
  number = {7},
  pages = {965--976},
  issn = {1941-0042},
  doi = {10.1109/83.597272},
  urldate = {2025-01-15},
  abstract = {Direct observation and recorded color images of the same scenes are often strikingly different because human visual perception computes the conscious representation with vivid color and detail in shadows, and with resistance to spectral shifts in the scene illuminant. A computation for color images that approaches fidelity to scene observation must combine dynamic range compression, color consistency-a computational analog for human vision color constancy-and color and lightness tonal rendition. In this paper, we extend a previously designed single-scale center/surround retinex to a multiscale version that achieves simultaneous dynamic range compression/color consistency/lightness rendition. This extension fails to produce good color rendition for a class of images that contain violations of the gray-world assumption implicit to the theoretical foundation of the retinex. Therefore, we define a method of color restoration that corrects for this deficiency at the cost of a modest dilution in color consistency. Extensive testing of the multiscale retinex with color restoration on several test scenes and over a hundred images did not reveal any pathological behaviour.},
  keywords = {Analog computers,Color,Computer vision,Dynamic range,Humans,Image coding,Image restoration,Layout,Testing,Visual perception},
  file = {C\:\\Users\\theun\\Zotero\\storage\\LR7ZBKTQ\\Jobson et al. - 1997 - A multiscale retinex for bridging the gap between color images and the human observation of scenes.pdf;C\:\\Users\\theun\\Zotero\\storage\\RELUF794\\597272.html}
}

@article{jobsonPropertiesPerformanceCenter1997,
  title = {Properties and Performance of a Center/Surround Retinex},
  author = {Jobson, D.J. and Rahman, Z. and Woodell, G.A.},
  year = 1997,
  month = mar,
  journal = {IEEE Transactions on Image Processing},
  volume = {6},
  number = {3},
  pages = {451--462},
  issn = {1941-0042},
  doi = {10.1109/83.557356},
  urldate = {2025-03-06},
  abstract = {The last version of Land's (1986) retinex model for human vision's lightness and color constancy has been implemented and tested in image processing experiments. Previous research has established the mathematical foundations of Land's retinex but has not subjected his lightness theory to extensive image processing experiments. We have sought to define a practical implementation of the retinex without particular concern for its validity as a model for human lightness and color perception. We describe the trade-off between rendition and dynamic range compression that is governed by the surround space constant. Further, unlike previous results, we find that the placement of the logarithmic function is important and produces best results when placed after the surround formation. Also unlike previous results, we find the best rendition for a "canonical" gain/offset applied after the retinex operation. Various functional forms for the retinex surround are evaluated, and a Gaussian form is found to perform better than the inverse square suggested by Land. Images that violate the gray world assumptions (implicit to this retinex) are investigated to provide insight into cases where this retinex fails to produce a good rendition.},
  keywords = {Analog computers,Color,Dynamic range,Humans,Image coding,Image processing,Layout,Machine vision,NASA,Very large scale integration},
  file = {C\:\\Users\\theun\\Zotero\\storage\\VS4FBIGH\\Jobson et al. - 1997 - Properties and performance of a centersurround retinex.pdf;C\:\\Users\\theun\\Zotero\\storage\\U4MXM4G5\\557356.html}
}

@article{kabirTerrainDetectionSegmentation2025,
  title = {Terrain Detection and Segmentation for Autonomous Vehicle Navigation: {{A}} State-of-the-Art Systematic Review},
  shorttitle = {Terrain Detection and Segmentation for Autonomous Vehicle Navigation},
  author = {Kabir, Md Mohsin and Jim, Jamin Rahman and Istenes, Zolt{\'a}n},
  year = 2025,
  month = jan,
  journal = {Information Fusion},
  volume = {113},
  pages = {102644},
  issn = {15662535},
  doi = {10.1016/j.inffus.2024.102644},
  urldate = {2024-12-04},
  abstract = {This review comprehensively investigates the current state and emerging trends of autonomous vehicle terrain detection and segmentation. By systematically reviewing literature from various databases, this study outlines the evolution of detection and segmentation techniques from traditional computer vision methods to advanced machine learning and deep learning approaches. It identifies critical technological advancements, evaluates their performance, and discusses the challenges faced under various environmental conditions, data acquisition, and integration with vehicle systems. This study also highlights the need for standardized benchmarks and datasets to facilitate the development and testing of robust terrain detection systems. This review encompasses terrain detection and segmentation in structured environments, such as urban roads and highways, and unstructured environments, including rural paths and off-road terrains, to comprehensively analyze autonomous vehicle navigation challenges. By analyzing recent research findings, this review provides insights into future directions for overcoming these limitations and fostering innovation in the autonomous driving domain.},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\LN85MQBS\Kabir et al. - 2025 - Terrain detection and segmentation for autonomous vehicle navigation A state-of-the-art systematic.pdf}
}

@misc{KAISTMultiSpectralDay,
  title = {{{KAIST Multi-Spectral Day}}/{{Night Data Set}} for {{Autonomous}} and {{Assisted Driving}}},
  urldate = {2025-01-16},
  abstract = {We introduce the KAIST multi-spectral data set, which covers a great range of drivable regions, from urban to residential, for autonomous systems. Our data set provides the different perspectives of the world captured in coarse time slots (day and night), in addition to fine time slots (sunrise, morning, afternoon, sunset, night, and dawn). For all-day perception of autonomous systems, we propose the use of a different spectral sensor, i.e., a thermal imaging camera. Toward this goal, we develop a multi-sensor platform, which supports the use of a co-aligned RGB/Thermal camera, RGB stereo, 3-D LiDAR, and inertial sensors (GPS/IMU) and a related calibration technique. We design a wide range of visual perception tasks including the object detection, drivable region detection, localization, image enhancement, depth estimation, and colorization using a single/multi-spectral approach. In this paper, we provide a description of our benchmark with the recording platform, data format, development toolkits, and lessons about the progress of capturing data sets.},
  howpublished = {https://xplorestaging-ieee-org.ezproxy-f.deakin.edu.au/document/8293689},
  langid = {american},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\6VPF23TA\8293689.html}
}

@inproceedings{kaurReviewLocalBinary2021,
  title = {A {{Review}} of {{Local Binary Pattern Based}} Texture Feature Extraction},
  booktitle = {2021 9th {{International Conference}} on {{Reliability}}, {{Infocom Technologies}} and {{Optimization}} ({{Trends}} and {{Future Directions}}) ({{ICRITO}})},
  author = {Kaur, Navneet and Nazir, Nahida and {Manik}},
  year = 2021,
  month = sep,
  pages = {1--4},
  doi = {10.1109/ICRITO51393.2021.9596485},
  urldate = {2025-01-03},
  abstract = {In the sphere of image processing, image data investigation is required related to a specific application in order to extract the suggestive information and reach defined and crisp culminations. One of the most significant phase in image processing is feature extraction which is the third step following image acquisition and segmentation. The procedure of reconstructing the input image into a group of features is named as feature extraction. These features construe the textural characteristics of the image. Texture feature extraction is one such significant part of feature extraction that on majority influences the results of classification. A texture is principally based on recognizing the object or region of interest in an image. The Local Binary Pattern feature descriptor will be the pith of discussion of this paper. LBP is a texture operator that operates on an image by labeling its pixels by thresholding neighborhood of each pixel. Various quality journals have been referred in order to provide an insight into the trends in pattern recognition using LBP.},
  keywords = {/unread,Feature extraction,Image segmentation,Local Binary Pattern,Market research,Measurement,Reliability,Silver,texture features,Thresholding (Imaging)},
  file = {C:\Users\theun\Zotero\storage\748HNMY9\Kaur et al. - 2021 - A Review of Local Binary Pattern Based texture feature extraction.pdf}
}

@article{keCombiningLowLightScene2023,
  title = {Combining {{Low-Light Scene Enhancement}} for {{Fast}} and {{Accurate Lane Detection}}},
  author = {Ke, C. and Xu, Z. and Zhang, J. and Zhang, D.},
  year = 2023,
  journal = {Sensors},
  volume = {23},
  number = {10},
  publisher = {MDPI},
  issn = {14248220 (ISSN)},
  doi = {10.3390/s23104917},
  abstract = {Lane detection is a crucial task in the field of autonomous driving, as it enables vehicles to safely navigate on the road by interpreting the high-level semantics of traffic signs. Unfortunately, lane detection is a challenging problem due to factors such as low-light conditions, occlusions, and lane line blurring. These factors increase the perplexity and indeterminacy of the lane features, making them hard to distinguish and segment. To tackle these challenges, we propose a method called low-light enhancement fast lane detection (LLFLD) that integrates the automatic low-light scene enhancement network (ALLE) with the lane detection network to improve lane detection performance under low-light conditions. Specifically, we first utilize the ALLE network to enhance the input image's brightness and contrast while reducing excessive noise and color distortion. Then, we introduce symmetric feature flipping module (SFFM) and channel fusion self-attention mechanism (CFSAT) to the model, which refine the low-level features and utilize more abundant global contextual information, respectively. Moreover, we devise a novel structural loss function that leverages the inherent prior geometric constraints of lanes to optimize the detection results. We evaluate our method on the CULane dataset, a public benchmark for lane detection in various lighting conditions. Our experiments show that our approach surpasses other state of the arts in both daytime and nighttime settings, especially in low-light scenarios. \copyright{} 2023 by the authors.},
  langid = {english},
  keywords = {Arts computing,Autonomous driving,Detection networks,High level semantics,image classification,Image classification,Image enhancement,Images classification,lane detection,Lane detection,Light enhancement,Low light,Low light conditions,low-light enhancement,Low-light enhancement,semantic segmentation,Semantic segmentation,Semantic Segmentation,Semantics,Traffic signs},
  file = {C:\Users\theun\Zotero\storage\QVM4E2SP\Ke et al. - 2023 - Combining Low-Light Scene Enhancement for Fast and Accurate Lane Detection.pdf}
}

@article{khalefaMONOCULARDEPTHESTIMATION2023,
  title = {{{MONOCULAR DEPTH ESTIMATION FOR NIGHT-TIME IMAGES}}},
  author = {Khalefa, N. and {El-Sheimy}, N.},
  year = 2023,
  month = dec,
  journal = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  volume = {XLVIII-1-W2-2023},
  pages = {171--178},
  publisher = {Copernicus GmbH},
  issn = {1682-1750},
  doi = {10.5194/isprs-archives-XLVIII-1-W2-2023-171-2023},
  urldate = {2025-06-30},
  abstract = {Depth estimation plays a pivotal role in numerous computer vision applications. However, depth estimation networks trained exclusively on daytime images tend to yield poor performance when applied to nighttime scenarios due to domain differences and variations in scene characteristics. In order to address this limitation, we conducted experiments involving the creation of a synthetic nighttime dataset by employing image translation techniques through a generative network. Subsequently, we utilized the generated images to fine-tune the depth estimation network, aiming to investigate the potential for enhancing task performance using generated data. We evaluated our approach by testing with the generated data, and we observed a noticeable improvement in the depth estimation task both before and after fine-tuning. Consequently, our approach yields results that are comparable to those achieved by networks specifically designed for daytime prediction. These findings highlight the effectiveness of utilizing synthetic data to enhance the performance of depth estimation tasks, particularly in nighttime settings.},
  langid = {english},
  keywords = {Generative Adversarial Network,Image Translation,Monocular Depth Estimation,Night,Synthetic Data},
  file = {C:\Users\theun\Zotero\storage\4Q3QC7PD\Khalefa and El-Sheimy - 2023 - MONOCULAR DEPTH ESTIMATION FOR NIGHT-TIME IMAGES.pdf}
}

@article{kimDeepMonocularDepth2018,
  title = {Deep {{Monocular Depth Estimation}} via {{Integration}} of {{Global}} and {{Local Predictions}}},
  author = {Kim, Youngjung and Jung, Hyungjoo and Min, Dongbo and Sohn, Kwanghoon},
  year = 2018,
  month = aug,
  journal = {IEEE Transactions on Image Processing},
  volume = {27},
  number = {8},
  pages = {4131--4144},
  issn = {1941-0042},
  doi = {10.1109/TIP.2018.2836318},
  urldate = {2025-07-03},
  abstract = {Recent works on machine learning have greatly advanced the accuracy of single image depth estimation. However, the resulting depth images are still over-smoothed and perceptually unsatisfying. This paper casts depth prediction from single image as a parametric learning problem. Specifically, we propose a deep variational model that effectively integrates heterogeneous predictions from two convolutional neural networks (CNNs), named global and local networks. They have contrasting network architecture and are designed to capture the depth information with complementary attributes. These intermediate outputs are then combined in the integration network based on the variational framework. By unrolling the optimization steps of Split Bregman iterations in the integration network, our model can be trained in an end-to-end manner. This enables one to simultaneously learn an efficient parameterization of the CNNs and hyper-parameter in the variational method. Finally, we offer a new data set of 0.22 million RGB-D images captured by Microsoft Kinect v2. Our model generates realistic and discontinuity-preserving depth prediction without involving any low-level segmentation or superpixels. Intensive experiments demonstrate the superiority of the proposed method in a range of RGB-D benchmarks, including both indoor and outdoor scenarios.},
  keywords = {2D-to-3D conversion,Computational modeling,convolutional neural networks,Databases,Depth estimation,Estimation,Measurement,non-parametric sampling,Optimization,Predictive models,RGB-D database,Training},
  file = {C:\Users\theun\Zotero\storage\3SW484F9\Kim et al. - 2018 - Deep Monocular Depth Estimation via Integration of Global and Local Predictions.pdf}
}

@article{kimRoadShapeClassificationBased2023,
  title = {Road {{Shape Classification-Based Matching Between Lane Detection}} and {{HD Map}} for {{Robust Localization}} of {{Autonomous Cars}}},
  author = {Kim, Soyeong and Kim, Sangkwon and Seok, Jiwon and Ryu, Chorong and Hwang, Daesung and Jo, Kichun},
  year = 2023,
  month = may,
  journal = {IEEE Transactions on Intelligent Vehicles},
  volume = {8},
  number = {5},
  pages = {3431--3443},
  issn = {2379-8904},
  doi = {10.1109/TIV.2022.3218307},
  urldate = {2025-01-02},
  abstract = {Many map matching-based localization algorithms estimate the autonomous car's pose using a registration between lanes measured by a camera and the High-Definition (HD) map. However, the registration methods based on numerical optimization, such as the Iterative Closest Point (ICP) and Normal Distributions Transform (NDT), can cause underdetermined problems due to no unique solution when matching under-constrained lane shapes such as straight lines and circular arcs. This paper proposes a robust localization algorithm with centimeter-level accuracy by applying different matching techniques depending on the road shape. We proposed a road shape classification-based map matching algorithm to overcome the under-constrained problems, which have no unique solution due to insufficient constraints. The proposed algorithm classifies lane segments into line, arc, and clothoid curves considering their curvature characteristics. After that, we find correction information through a map matching and covariance estimation method using lane pairs with the same type of their shape. For the under-constrained shapes, the geometry-based map-matching algorithm and covariance estimation method are applied to avoid underdetermined results. Finally, the measurement calculated from the correction information and predicted pose of the ego-vehicle is exploited for the measurement update of the Extended Kalman Filter (EKF). The proposed method was quantitatively evaluated in the simulation environment, which contains various road shapes, and qualitatively validated for experimental data from an autonomous driving platform. The proposed algorithm shows more robust matching capability, efficient computation time, and higher accuracy than the localization system based on ICP-based lane matching.},
  keywords = {/unread,Geometry,high-definition (HD) map,localization,Location awareness,map matching,Optimization,Pose estimation,road shape classification,Roads,Sensors,Shape,Under-constraint shape},
  file = {C\:\\Users\\theun\\Zotero\\storage\\KSQVLPBP\\Kim et al. - 2023 - Road Shape Classification-Based Matching Between Lane Detection and HD Map for Robust Localization o.pdf;C\:\\Users\\theun\\Zotero\\storage\\EAZ3BGDQ\\9940999.html}
}

@article{kiransreepokkuluriEnhancingImageSegmentation2024,
  title = {Enhancing {{Image Segmentation Accuracy}} Using {{Deep Learning Techniques}}},
  author = {{Kiran Sree Pokkuluri} and {Usha Devi N. S.S.S.N} and {Martin Margala} and {Prasun Chakrabarti}},
  year = 2024,
  month = jul,
  journal = {Journal of Advanced Research in Applied Sciences and Engineering Technology},
  volume = {49},
  number = {1},
  pages = {139--148},
  issn = {24621943},
  doi = {10.37934/araset.49.1.139148},
  urldate = {2024-12-04},
  abstract = {Accurate image segmentation is a fundamental task in computer vision with applications spanning from medical imaging to autonomous vehicles. This research paper introduces a novel approach for enhancing image segmentation accuracy through the utilization of deep learning techniques. Traditional segmentation methods often struggle with complex scenes, object occlusions, and varying lighting conditions. Leveraging the power of deep learning, we propose a custom convolutional neural network (CNN) architecture named DLwCA. This architecture incorporates advanced features such as residual connections and attention mechanisms to capture fine-grained details and contextual information. The proposed approach is evaluated on benchmark datasets and compared against established methods. Quantitative metrics including Intersection over Union (IoU) and F1-score demonstrate a significant improvement in segmentation accuracy. Our approach showcases a clear potential to revolutionize image segmentation tasks, offering precise delineation of object boundaries even in challenging scenarios. This research contributes to the growing body of knowledge on leveraging deep learning for advanced computer vision tasks and establishes a strong foundation for further research in the domain of image segmentation techniques. We have compared our work with the existing literature with various parameters like F1 Score, precision and accuracy. The proposed method reports an average accuracy of 91.9\% and perming better than some baseline models.},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\5334W2JB\Kiran Sree Pokkuluri et al. - 2024 - Enhancing Image Segmentation Accuracy using Deep Learning Techniques.pdf}
}

@inproceedings{kirillovPanopticFeaturePyramid2019,
  title = {Panoptic {{Feature Pyramid Networks}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Kirillov, Alexander and Girshick, Ross and He, Kaiming and Dollar, Piotr},
  year = 2019,
  pages = {6399--6408},
  urldate = {2025-06-05},
  file = {C:\Users\theun\Zotero\storage\IVSYKD9H\Kirillov et al. - 2019 - Panoptic Feature Pyramid Networks.pdf}
}

@inproceedings{kirillovPanopticSegmentation2019,
  title = {Panoptic {{Segmentation}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Dollar, Piotr},
  year = 2019,
  pages = {9404--9413},
  urldate = {2025-06-05},
  file = {C:\Users\theun\Zotero\storage\2LAVZY7C\Kirillov et al. - 2019 - Panoptic Segmentation.pdf}
}

@inproceedings{klingnerSelfsupervisedMonocularDepth2020,
  title = {Self-Supervised {{Monocular Depth Estimation}}: {{Solving}} the {{Dynamic Object Problem}} by {{Semantic Guidance}}},
  shorttitle = {Self-Supervised {{Monocular Depth Estimation}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2020},
  author = {Klingner, Marvin and Term{\"o}hlen, Jan-Aike and Mikolajczyk, Jonas and Fingscheidt, Tim},
  year = 2020,
  pages = {582--600},
  publisher = {Springer, Cham},
  issn = {1611-3349},
  doi = {10.1007/978-3-030-58565-5_35},
  urldate = {2025-06-30},
  abstract = {Self-supervised monocular depth estimation presents a powerful method to obtain 3D scene information from single camera images, which is trainable on arbitrary image sequences without requiring depth labels, e.g., from a LiDAR sensor. In this work we present a new...},
  isbn = {978-3-030-58565-5},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\UTLJWFI7\Klingner et al. - 2020 - Self-supervised Monocular Depth Estimation Solving the Dynamic Object Problem by Semantic Guidance.pdf}
}

@inproceedings{kongKinematicDynamicVehicle2015,
  title = {Kinematic and Dynamic Vehicle Models for Autonomous Driving Control Design},
  booktitle = {2015 {{IEEE Intelligent Vehicles Symposium}} ({{IV}})},
  author = {Kong, Jason and Pfeiffer, Mark and Schildbach, Georg and Borrelli, Francesco},
  year = 2015,
  month = jun,
  pages = {1094--1099},
  issn = {1931-0587},
  doi = {10.1109/IVS.2015.7225830},
  urldate = {2025-01-10},
  abstract = {We study the use of kinematic and dynamic vehicle models for model-based control design used in autonomous driving. In particular, we analyze the statistics of the forecast error of these two models by using experimental data. In addition, we study the effect of discretization on forecast error. We use the results of the first part to motivate the design of a controller for an autonomous vehicle using model predictive control (MPC) and a simple kinematic bicycle model. The proposed approach is less computationally expensive than existing methods which use vehicle tire models. Moreover it can be implemented at low vehicle speeds where tire models become singular. Experimental results show the effectiveness of the proposed approach at various speeds on windy roads.},
  keywords = {Bicycles,Kinematics,Predictive models,Tires,Trajectory,Vehicle dynamics},
  file = {C\:\\Users\\theun\\Zotero\\storage\\U5B3YMUG\\Kong et al. - 2015 - Kinematic and dynamic vehicle models for autonomous driving control design.pdf;C\:\\Users\\theun\\Zotero\\storage\\CIAPMZQH\\7225830.html}
}

@article{kongLiteratureReviewHistogram2013,
  title = {A {{Literature Review}} on {{Histogram Equalization}} and {{Its Variations}} for {{Digital Image Enhancement}}},
  author = {Kong, Nicholas},
  year = 2013,
  journal = {International Journal of Innovation, Management and Technology},
  issn = {20100248},
  doi = {10.7763/IJIMT.2013.V4.426},
  urldate = {2025-05-28},
  abstract = {Global Histogram Equalization (GHE) is a wellknown image enhancement method. Despite of its simplicity and popularity, GHE still has limitations. GHE usually causes the shifting of the mean luminance of the image, produces artifacts and unnatural enhancements, and does not consider local information in its process. Therefore, these limitations lead to the development of several histogram equalization (HE) methods. This paper surveys some of HE based methods. These methods generally fall into three main families of HE, namely Mean Brightness Preserving HE (MBPHE), Bin Modified HE (BMHE), and Local HE (LHE).},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\PT664NAU\Kong - 2013 - A Literature Review on Histogram Equalization and Its Variations for Digital Image Enhancement.pdf}
}

@inproceedings{krahenbuhlEfficientInferenceFully2011,
  title = {Efficient {{Inference}} in {{Fully Connected CRFs}} with {{Gaussian Edge Potentials}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kr{\"a}henb{\"u}hl, Philipp and Koltun, Vladlen},
  year = 2011,
  volume = {24},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-06-09},
  abstract = {Most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions. While region- level models often feature dense pairwise connectivity, pixel-level models are con- siderably larger and have only permitted sparse graph structures. In this paper, we consider fully connected CRF models defined on the complete set of pixels in an image. The resulting graphs have billions of edges, making traditional inference algorithms impractical. Our main contribution is a highly efficient approximate inference algorithm for fully connected CRF models in which the pairwise edge potentials are defined by a linear combination of Gaussian kernels. Our experi- ments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy.},
  file = {C:\Users\theun\Zotero\storage\KJGG5TUK\Krähenbühl and Koltun - 2011 - Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials.pdf}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = 2012,
  volume = {25},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-06-10},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\textbackslash\% and 18.9\textbackslash\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  file = {C:\Users\theun\Zotero\storage\9N2D9S8T\Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf}
}

@article{kuuttiSurveyDeepLearning2021,
  title = {A {{Survey}} of {{Deep Learning Applications}} to {{Autonomous Vehicle Control}}},
  author = {Kuutti, Sampo and Bowden, Richard and Jin, Yaochu and Barber, Phil and Fallah, Saber},
  year = 2021,
  month = feb,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {22},
  number = {2},
  pages = {712--733},
  issn = {1558-0016},
  doi = {10.1109/TITS.2019.2962338},
  urldate = {2025-01-10},
  abstract = {Designing a controller for autonomous vehicles capable of providing adequate performance in all driving scenarios is challenging due to the highly complex environment and inability to test the system in the wide variety of scenarios which it may encounter after deployment. However, deep learning methods have shown great promise in not only providing excellent performance for complex and non-linear control problems, but also in generalising previously learned rules to new scenarios. For these reasons, the use of deep learning for vehicle control is becoming increasingly popular. Although important advancements have been achieved in this field, these works have not been fully summarised. This paper surveys a wide range of research works reported in the literature which aim to control a vehicle through deep learning methods. Although there exists overlap between control and perception, the focus of this paper is on vehicle control, rather than the wider perception problem which includes tasks such as semantic segmentation and object detection. The paper identifies the strengths and limitations of available deep learning methods through comparative analysis and discusses the research challenges in terms of computation, architecture selection, goal specification, generalisation, verification and validation, as well as safety. Overall, this survey brings timely and topical information to a rapidly evolving field relevant to intelligent transportation systems.},
  keywords = {advanced driver assistance,autonomous vehicles,Autonomous vehicles,computer vision,Deep learning,intelligent control,Machine learning,neural networks,Neural networks,Reinforcement learning,Sensors,Task analysis,Training},
  file = {C:\Users\theun\Zotero\storage\M3NPKZAK\Kuutti et al. - 2021 - A Survey of Deep Learning Applications to Autonomous Vehicle Control.pdf}
}

@article{lagaSurveyDeepLearning2022,
  title = {A {{Survey}} on {{Deep Learning Techniques}} for {{Stereo-Based Depth Estimation}}},
  author = {Laga, Hamid and Jospin, Laurent Valentin and Boussaid, Farid and Bennamoun, Mohammed},
  year = 2022,
  month = apr,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {4},
  pages = {1738--1764},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2020.3032602},
  urldate = {2025-03-19},
  abstract = {Estimating depth from RGB images is a long-standing ill-posed problem, which has been explored for decades by the computer vision, graphics, and machine learning communities. Among the existing techniques, stereo matching remains one of the most widely used in the literature due to its strong connection to the human binocular system. Traditionally, stereo-based depth estimation has been addressed through matching hand-crafted features across multiple images. Despite the extensive amount of research, these traditional techniques still suffer in the presence of highly textured areas, large uniform regions, and occlusions. Motivated by their growing success in solving various 2D and 3D vision problems, deep learning for stereo-based depth estimation has attracted a growing interest from the community, with more than 150 papers published in this area between 2014 and 2019. This new generation of methods has demonstrated a significant leap in performance, enabling applications such as autonomous driving and augmented reality. In this paper, we provide a comprehensive survey of this new and continuously growing field of research, summarize the most commonly used pipelines, and discuss their benefits and limitations. In retrospect of what has been achieved so far, we also conjecture what the future may hold for deep learning-based stereo for depth estimation research.},
  keywords = {/unread,3D reconstruction,Australia,CNN,deep learning,Deep learning,disparity estimation,Estimation,feature leaning,feature matching,multi-view stereo,Pipelines,stereo matching,Three-dimensional displays,Training,Videos},
  file = {C\:\\Users\\theun\\Zotero\\storage\\27X86WBX\\Laga et al. - 2022 - A Survey on Deep Learning Techniques for Stereo-Based Depth Estimation.pdf;C\:\\Users\\theun\\Zotero\\storage\\QELERWRQ\\9233988.html}
}

@inproceedings{lambaRealTimeRestorationDark2023,
  title = {Real-{{Time Restoration}} of {{Dark Stereo Images}}},
  booktitle = {2023 {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Lamba, Mohit and Suhas Kumar, M V A and Mitra, Kaushik},
  year = 2023,
  month = jan,
  pages = {4903--4913},
  issn = {2642-9381},
  doi = {10.1109/WACV56688.2023.00489},
  urldate = {2025-03-15},
  abstract = {Low-light image enhancement has been an actively researched area for decades and has produced excellent night-time single-image, video, and Light Field restoration methods. Despite these advances, the problem of extreme low-light stereo image restoration has been mostly ignored and addressing it can enable night-time capabilities to several applications such as smartphones and self-driving cars. We propose an especially light-weight and fast hybrid U-net architecture for extreme low-light stereo image restoration. In the initial few scale spaces, we process the left and right features individually, because the two features do not align well due to large disparity. At coarser scale-spaces, the disparity between left and right features decreases and the network's receptive field increases. We use this fact to reduce computations by simultaneously processing the left and right features, which also benefits epipole preservation. As our architecture does not use any 3D convolution for fast inference, we use a Depth-Aware loss module to train our network. This module computes quick and coarse depth estimates to better enforce the stereo epipolar constraints. Extensive benchmarking in terms of visual enhancement and downstream depth estimation shows that our architecture not only restores dark stereo images faithfully but also offers 4-60\texttimes{} speed-up with 15-100\texttimes{} lower floating point operations, necessary for real-world applications.},
  keywords = {Applications: Commercial/retail,Computer architecture,Convolution,Embedded sensing/real-time techniques,Estimation,Light fields,Real-time systems,Robotics,Three-dimensional displays,Visualization},
  file = {C\:\\Users\\theun\\Zotero\\storage\\AND7IDJS\\Lamba et al. - 2023 - Real-Time Restoration of Dark Stereo Images.pdf;C\:\\Users\\theun\\Zotero\\storage\\QP3YZF8H\\Lamba_Real-Time_Restoration_of_WACV_2023_supplemental.pdf;C\:\\Users\\theun\\Zotero\\storage\\H6EWGDKU\\10030097.html}
}

@inproceedings{lambaRestoringExtremelyDark2021,
  title = {Restoring {{Extremely Dark Images}} in {{Real Time}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Lamba, Mohit and Mitra, Kaushik},
  year = 2021,
  month = jun,
  pages = {3486--3496},
  issn = {2575-7075},
  doi = {10.1109/CVPR46437.2021.00349},
  urldate = {2025-03-15},
  abstract = {A practical low-light enhancement solution must be computationally fast, memory-efficient, and achieve a visually appealing restoration. Most of the existing methods target restoration quality and thus compromise on speed and memory requirements, raising concerns about their real-world deployability. We propose a new deep learning architecture for extreme low-light single image restoration, which despite its fast \& lightweight inference, produces a restoration that is perceptually at par with state-of-the-art computationally intense models. To achieve this, we do most of the processing in the higher scale-spaces, skipping the intermediate-scales wherever possible. Also unique to our model is the potential to process all the scale-spaces concurrently, offering an additional 30\% speedup without compromising the restoration quality. Pre-amplification of the dark raw-image is an important step in extreme lowlight image enhancement. Most of the existing state of the art methods need GT exposure value to estimate the pre-amplification factor, which is not practically feasible. Thus, we propose an amplifier module that estimates the amplification factor using only the input raw image and can be used "off-the-shelf" with pre-trained models without any fine-tuning. We show that our model can restore an ultrahigh-definition 4K resolution image in just 1 sec. on a CPU and at 32 fps on a GPU and yet maintain a competitive restoration quality. We also show that our proposed model, without any fine-tuning, generalizes well to cameras not seen during training and to subsequent tasks such as object detection.},
  keywords = {Cameras,Computational modeling,Deep learning,Graphics processing units,Image resolution,Object detection,Training},
  file = {C\:\\Users\\theun\\Zotero\\storage\\GAJ7EI6U\\Lamba and Mitra - 2021 - Restoring Extremely Dark Images in Real Time.pdf;C\:\\Users\\theun\\Zotero\\storage\\KDNZ26VU\\9577341.html}
}

@article{landLightnessRetinexTheory1971,
  title = {Lightness and {{Retinex Theory}}},
  author = {Land, Edwin H. and McCann, John J.},
  year = 1971,
  month = jan,
  journal = {Journal of the Optical Society of America},
  volume = {61},
  number = {1},
  pages = {1},
  issn = {0030-3941},
  doi = {10.1364/JOSA.61.000001},
  urldate = {2025-05-29},
  copyright = {https://doi.org/10.1364/OA\_License\_v1\#VOR},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\TFAUSWKD\Land and McCann - 1971 - Lightness and Retinex Theory.pdf}
}

@incollection{landRecentAdvancesRetinex1985,
  title = {Recent {{Advances}} in {{Retinex Theory}}},
  booktitle = {Central and {{Peripheral Mechanisms}} of {{Colour Vision}}},
  author = {Land, E. H.},
  editor = {Ottoson, David and Zeki, Semir},
  year = 1985,
  pages = {5--17},
  publisher = {Palgrave Macmillan UK},
  address = {London},
  doi = {10.1007/978-1-349-08020-5_2},
  urldate = {2025-03-04},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-1-349-08022-9 978-1-349-08020-5},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\HJ8S8YT3\Land - 1985 - Recent Advances in Retinex Theory.pdf}
}

@article{landRetinexTheoryColor1977,
  title = {The {{Retinex Theory}} of {{Color Vision}}},
  author = {Land, Edwin H.},
  year = 1977,
  journal = {Scientific American},
  volume = {237},
  number = {6},
  eprint = {24953876},
  eprinttype = {jstor},
  pages = {108--129},
  publisher = {Scientific American, a division of Nature America, Inc.},
  issn = {0036-8733},
  urldate = {2025-01-15},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\JS3B8W7X\Land - 1977 - The Retinex Theory of Color Vision.pdf}
}

@incollection{lawsonHypotheticodeductiveMethod2015,
  title = {Hypothetico-Deductive {{Method}}},
  booktitle = {Encyclopedia of {{Science Education}}},
  author = {Lawson, Anton E.},
  editor = {Gunstone, Richard},
  year = 2015,
  pages = {471--472},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-007-2150-0_260},
  urldate = {2025-03-12},
  isbn = {978-94-007-2150-0},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\MZY3NCTQ\Lawson - 2015 - Hypothetico-deductive Method.pdf}
}

@article{leeContrastEnhancementBased2013,
  title = {Contrast {{Enhancement Based}} on {{Layered Difference Representation}} of {{2D Histograms}}},
  author = {Lee, Chulwoo and Lee, Chul and Kim, Chang-Su},
  year = 2013,
  month = dec,
  journal = {IEEE Transactions on Image Processing},
  volume = {22},
  number = {12},
  pages = {5372--5384},
  issn = {1941-0042},
  doi = {10.1109/TIP.2013.2284059},
  urldate = {2025-03-11},
  abstract = {A novel contrast enhancement algorithm based on the layered difference representation of 2D histograms is proposed in this paper. We attempt to enhance image contrast by amplifying the gray-level differences between adjacent pixels. To this end, we obtain the 2D histogram h(k, k+l) from an input image, which counts the pairs of adjacent pixels with gray-levels k and k+l, and represent the gray-level differences in a tree-like layered structure. Then, we formulate a constrained optimization problem based on the observation that the gray-level differences, occurring more frequently in the input image, should be more emphasized in the output image. We first solve the optimization problem to derive the transformation function at each layer. We then combine the transformation functions at all layers into the unified transformation function, which is used to map input gray-levels to output gray-levels. Experimental results demonstrate that the proposed algorithm enhances images efficiently in terms of both objective quality and subjective quality.},
  keywords = {2D histogram,constrained optimization,contrast enhancement,Dynamic range,Educational institutions,Equations,Heuristic algorithms,histogram equalization,Histograms,Image enhancement,layered difference representation,Optimization,Vectors},
  file = {C\:\\Users\\theun\\Zotero\\storage\\DWCTF8BI\\Lee et al. - 2013 - Contrast Enhancement Based on Layered Difference Representation of 2D Histograms.pdf;C\:\\Users\\theun\\Zotero\\storage\\CJJJU2QB\\6615961.html}
}

@article{leeFastRoadDetection2021,
  title = {Fast {{Road Detection}} by {{CNN-Based Camera}}--{{Lidar Fusion}} and {{Spherical Coordinate Transformation}}},
  author = {Lee, Jae-Seol and Park, Tae-Hyoung},
  year = 2021,
  month = sep,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {22},
  number = {9},
  pages = {5802--5810},
  issn = {1558-0016},
  doi = {10.1109/TITS.2020.2988302},
  urldate = {2025-01-02},
  abstract = {We propose a new camera-lidar fusion method for road detection where the spherical coordinate transformation is introduced to decrease the gap between the point cloud of 3D lidar data. The camera's color data and the 3D lidar's height data are transformed into the same spherical coordinate, and then input to the convolution neural network for segmentation. Faster segmentation is possible due to the reduced size of input data. To increase the detection accuracy, this modified SegNet expands the receptive field of the network. Using the KITTI dataset, we present the experimental results to show the usefulness of the proposed method.},
  keywords = {/unread,autonomous vehicles,Autonomous vehicles,Cameras,convolution neural network,Convolutional neural networks,Image segmentation,Laser radar,lidar and camera fusion,Road detection,segmentation,Sensor fusion,spherical coordinate transformation},
  file = {C\:\\Users\\theun\\Zotero\\storage\\QJFFMAS7\\Lee and Park - 2021 - Fast Road Detection by CNN-Based Camera–Lidar Fusion and Spherical Coordinate Transformation.pdf;C\:\\Users\\theun\\Zotero\\storage\\QD2ZYX8P\\9084150.html}
}

@inproceedings{leeGPSGLASSLearningNighttime2023,
  title = {{{GPS-GLASS}}: {{Learning Nighttime Semantic Segmentation Using Daytime Video}} and {{GPS Data}}},
  shorttitle = {{{GPS-GLASS}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Lee, Hongjae and Han, Changwoo and Yoo, Jun-Sang and Jung, Seung-Won},
  year = 2023,
  pages = {4001--4010},
  urldate = {2025-06-16},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\ET4KK6AH\Lee et al. - 2023 - GPS-GLASS Learning Nighttime Semantic Segmentation Using Daytime Video and GPS Data.pdf}
}

@article{lepchaDeepJourneyImage2023,
  title = {A Deep Journey into Image Enhancement: {{A}} Survey of Current and Emerging Trends},
  shorttitle = {A Deep Journey into Image Enhancement},
  author = {Lepcha, Dawa Chyophel and Goyal, Bhawna and Dogra, Ayush and Sharma, Kanta Prasad and Gupta, Deena Nath},
  year = 2023,
  month = may,
  journal = {Information Fusion},
  volume = {93},
  pages = {36--76},
  issn = {15662535},
  doi = {10.1016/j.inffus.2022.12.012},
  urldate = {2025-01-16},
  abstract = {Image captured under poor-illumination conditions often display attributes of having poor contrasts, low brightness, a narrow gray range, colour distortions and considerable interference, which seriously affect the qualitative visual effects on human eyes and severely restrict the efficiency of several machine vision systems. In addition, underwater images often suffer from colour shift and contrast degradation because of an absorption and scattering of light while travelling in water. These unpleasant effects limits visibility, reduce contrast and even generate colour casts that limits the use of underwater images and videos in marine archaeology and biology. In medical imaging applications, medical images are important tools for detecting and diagnosing several medical conditions and ailments. However, the quality of medical images can often be degraded during image acquisition due to factors such as noise interference, artefacts, and poor illumination. This may lead to the misdiagnosis of medical conditions, which can further aggravate life threatening situations. Image enhancement is one of the most important technologies in the field of image processing, and its purpose is to improve the quality of images for specific applications. In general, the basic principle of image enhancement is to improve the quality and visual interpretability of an image so that it is more suitable for the specific applications and the observers. Over the last few decades, numerous image enhancement techniques have been proposed in the literature This study covers a systematic survey on existing state-of-the-art image enhancement techniques into broad classification of their algorithms. In addition, this paper summarises the datasets utilised in the literature for performing the experiments. Furthermore, an attention has been drawn towards several evaluation parameters for quantitative evaluation and compared different state-of-the-art algorithms for performance analysis on benchmark datasets. In addition, we discussed the recent areas of applications in image enhancement in detail. Lastly, we have also discussed numerous unresolved open problems and suggested possible future research directions. We believe that by putting forth all our efforts this study may presents a comprehensive resource for the future research.},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\N34QHYLU\Lepcha et al. - 2023 - A deep journey into image enhancement A survey of current and emerging trends.pdf}
}

@inproceedings{liangIterativePromptLearning2023,
  title = {Iterative {{Prompt Learning}} for {{Unsupervised Backlit Image Enhancement}}},
  booktitle = {2023 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Liang, Zhexin and Li, Chongyi and Zhou, Shangchen and Feng, Ruicheng and Loy, Chen Change},
  year = 2023,
  month = oct,
  pages = {8060--8069},
  publisher = {IEEE},
  address = {Paris, France},
  doi = {10.1109/ICCV51070.2023.00743},
  urldate = {2025-03-10},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-0718-4},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\FZ4MIDHU\Liang et al. - 2023 - Iterative Prompt Learning for Unsupervised Backlit Image Enhancement.pdf}
}

@inproceedings{liangLightDarkNovelLightweight2024,
  title = {Light-{{Dark}}: {{A Novel Lightweight Self-supervised Monocular Depth Estimation}} in the {{Dark}}},
  booktitle = {Lect. {{Notes Comput}}. {{Sci}}.},
  author = {Liang, Q. and Wang, L. and Wang, L. and Liu, X. and Wang, G.},
  editor = {{Huang D.-S.} and {Zhang Q.} and {Zhang C.}},
  year = 2024,
  volume = {14868 LNCS},
  pages = {3--14},
  publisher = {{Springer Science and Business Media Deutschland GmbH}},
  doi = {10.1007/978-981-97-5600-1_1},
  abstract = {Self-supervised monocular depth estimation has been widely studied in recent years. In nighttime scenes where the photometric consistency assumption is not met, several solutions have emerged to address this challenge. However, existing monocular depth estimation algorithms for nighttime often require a large-scale model and a significant number of floating-point operations, making it challenging to apply them to practical problems such as autonomous driving. In this paper, we propose a lightweight monocular depth estimation algorithm tailored for nighttime scenes, named Light-Dark. Specifically, we design a lightweight DepthNet incorporating Feature-Fusion blocks and Cross-connections. Additionally, in response to the low-light and high-noise issues in nighttime scenes, we introduce a Noise-Constrained Adaptive Image Enhancement (NCAIE) module. We deploy our model on the edge device Jetson AGX Orin to validate its real-world performance. A series of experiments conducted on nighttime datasets, RobotCar and nuScenes, indicate the effectiveness of our proposed Light-Dark and the equilibrium between lightweight design and accuracy. \copyright{} The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.},
  isbn = {03029743 (ISSN); 978-981975599-8 (ISBN)},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\NSBSYEU8\Liang et al. - 2024 - Light-Dark A Novel Lightweight Self-supervised Monocular Depth Estimation in the Dark.pdf}
}

@article{liaoStereoLowlightEnhancement2023,
  title = {Stereo Low-Light Enhancement Based on Feature Fusion and Consistency Loss},
  shorttitle = {基 于 特 征 融 合 和 一 致 性 损 失 的 双 目 低 光 照 增 强},
  author = {Liao, J.-W. and Pang, Y.-W. and Nie, J. and Sun, H.-Q. and Cao, J.-L.},
  year = 2023,
  journal = {Zhejiang Daxue Xuebao (Gongxue Ban)/Journal of Zhejiang University (Engineering Science)},
  volume = {57},
  number = {12},
  pages = {2456--2466},
  doi = {10.3785/j.issn.1008-973X.2023.12.013},
  abstract = {A large scale real scene stereo low-light image dataset SLL10K was proposed. There were 12 658 pairs of unreferenced stereo low-illumination images and 205 pairs of referenced stereo images contained in the dataset. The images in the SLL10K dataset cover a wealth of lighting, time, and scene. FCNet, a stereo low-light image enhancement network based on feature fusion and consistency loss was proposed. The feature fusion module was used to fully integrate intra-monocular and inter-stereo features, and the consistency loss function was used to maintain the consistency between images before and after enhancement. Experiments on the SLL10K dataset and darkening KITTI dataset show that the images with FCNet obtain better performance on low-light image enhancement and object detection than the monocular enhancement method. \copyright{} 2023 Zhejiang University. All rights reserved.},
  keywords = {feature fusion,image enhancement,low-light,no-reference image enhancement,stereo dataset},
  file = {C:\Users\theun\Zotero\storage\GM8P5VJB\display.html}
}

@article{liDeepLearningBased2021,
  title = {A Deep Learning Based Image Enhancement Approach for Autonomous Driving at Night},
  author = {Li, Guofa and Yang, Yifan and Qu, Xingda and Cao, Dongpu and Li, Keqiang},
  year = 2021,
  month = feb,
  journal = {Knowledge-Based Systems},
  volume = {213},
  pages = {106617},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2020.106617},
  urldate = {2025-01-16},
  abstract = {Images of road scenes in low-light situations are lack of details which could increase crash risk of connected autonomous vehicles (CAVs). Therefore, an effective and efficient image enhancement model for low-light images is necessary for safe CAV driving. Though some efforts have been made, image enhancement still cannot be well addressed especially in extremely low light situations (e.g., in rural areas at night without street light). To address this problem, we developed a light enhancement net (LE-net) based on the convolutional neural network. Firstly, we proposed a generation pipeline to transform daytime images to low-light images, and then used them to construct image pairs for model development. Our proposed LE-net was then trained and validated on the generated low-light images. Finally, we examined the effectiveness of our LE-net in real night situations at various low-light levels. Results showed that our LE-net was superior to the compared models, both qualitatively and quantitatively.},
  keywords = {Autonomous vehicles,Deep learning,Driver assistance systems,Driving safety,Image enhancement},
  file = {C\:\\Users\\theun\\Zotero\\storage\\PPEXC2B7\\Li et al. - 2021 - A deep learning based image enhancement approach for autonomous driving at night.pdf;C\:\\Users\\theun\\Zotero\\storage\\49YAQEV5\\S0950705120307462.html}
}

@inproceedings{liDurLARHighfidelity128channel2021,
  title = {{{DurLAR}}: {{A High-fidelity}} 128-Channel {{LiDAR Dataset}} with {{Panoramic Ambient}} and {{Reflectivity Imagery}} for {{Multi-modal Autonomous Driving Applications}}},
  shorttitle = {{{DurLAR}}},
  author = {Li, Li and Ismail, Khalid N. and Shum, Hubert P. H. and Breckon, Toby P.},
  year = 2021,
  month = dec,
  eprint = {2406.10068},
  primaryclass = {cs},
  doi = {10.1109/3DV53792.2021.00130},
  urldate = {2025-07-11},
  abstract = {We present DurLAR, a high-fidelity 128-channel 3D LiDAR dataset with panoramic ambient (near infrared) and reflectivity imagery, as well as a sample benchmark task using depth estimation for autonomous driving applications. Our driving platform is equipped with a high resolution 128 channel LiDAR, a 2MPix stereo camera, a lux meter and a GNSS/INS system. Ambient and reflectivity images are made available along with the LiDAR point clouds to facilitate multi-modal use of concurrent ambient and reflectivity scene information. Leveraging DurLAR, with a resolution exceeding that of prior benchmarks, we consider the task of monocular depth estimation and use this increased availability of higher resolution, yet sparse ground truth scene depth information to propose a novel joint supervised/self-supervised loss formulation. We compare performance over both our new DurLAR dataset, the established KITTI benchmark and the Cityscapes dataset. Our evaluation shows our joint use supervised and self-supervised loss terms, enabled via the superior ground truth resolution and availability within DurLAR improves the quantitative and qualitative performance of leading contemporary monocular depth estimation approaches (RMSE=3.639, Sq Rel=0.936).},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C\:\\Users\\theun\\Zotero\\storage\\WMFVKW6R\\Li et al. - 2021 - DurLAR A High-fidelity 128-channel LiDAR Dataset with Panoramic Ambient and Reflectivity Imagery fo.pdf;C\:\\Users\\theun\\Zotero\\storage\\2BA35CNM\\2406.html}
}

@article{liEmergentVisualSensors2023,
  title = {Emergent {{Visual Sensors}} for {{Autonomous Vehicles}}},
  author = {Li, You and Moreau, Julien and {Ibanez-Guzman}, Javier},
  year = 2023,
  month = may,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {24},
  number = {5},
  pages = {4716--4737},
  issn = {1558-0016},
  doi = {10.1109/TITS.2023.3248483},
  urldate = {2025-01-10},
  abstract = {For vehicles to navigate autonomously, they need to perceive and understand their immediate surroundings. Currently, cameras are the preferred sensors, due to their high performance and relatively low-cost compared with other sensors like LiDARs and Radars. However, their performance is limited by inherent imaging constraints, a standard RGB camera may perform poorly in extreme conditions, including low illumination, high contrast, bad weather (e.g. fog, rain, snow, etc.), glare, etc. Further, when using monocular cameras, it is more challenging to determine spatial distances than when using active range sensors such as LiDARs or Radars. Over the past years, novel image sensors, namely, infrared cameras, range-gated cameras, polarization cameras, and event cameras, have demonstrated strong potential. Some of them could be game-changers for future autonomous vehicles, they are the result of progress in sensor technology and the development of the accompanying perception algorithms. This paper presents in a systematic manner their principles, comparative advantages, data processing algorithms, and related applications. The purpose is to provide practitioners with an in-depth overview of novel sensing technologies that can contribute to the safe deployment of autonomous vehicles.},
  keywords = {Autonomous vehicles,Cameras,event camera,gated imaging sensor,Image sensor,infrared (IR) imaging sensor,Laser radar,polarization imaging sensor,Radar imaging,Reflection,Sensors,Visualization},
  file = {C\:\\Users\\theun\\Zotero\\storage\\6F7MYHEW\\Li et al. - 2023 - Emergent Visual Sensors for Autonomous Vehicles.pdf;C\:\\Users\\theun\\Zotero\\storage\\JCGLA4K8\\10092278.html}
}

@inproceedings{liFastLLVERealTimeLowLight2023,
  title = {{{FastLLVE}}: {{Real-Time Low-Light Video Enhancement}} with {{Intensity-Aware Lookup Table}}},
  shorttitle = {{{FastLLVE}}},
  booktitle = {Proceedings of the 31st {{ACM International Conference}} on {{Multimedia}}},
  author = {Li, Wenhao and Wu, Guangyang and Wang, Wenyi and Ren, Peiran and Liu, Xiaohong},
  year = 2023,
  month = oct,
  eprint = {2308.06749},
  primaryclass = {cs},
  pages = {8134--8144},
  doi = {10.1145/3581783.3611933},
  urldate = {2024-12-04},
  abstract = {Low-Light Video Enhancement (LLVE) has received considerable attention in recent years. One of the critical requirements of LLVE is inter-frame brightness consistency, which is essential for maintaining the temporal coherence of the enhanced video. However, most existing single-image-based methods fail to address this issue, resulting in flickering effect that degrades the overall quality after enhancement. Moreover, 3D Convolution Neural Network (CNN)-based methods, which are designed for video to maintain inter-frame consistency, are computationally expensive, making them impractical for real-time applications. To address these issues, we propose an efficient pipeline named FastLLVE that leverages the Look-Up-Table (LUT) technique to maintain inter-frame brightness consistency effectively. Specifically, we design a learnable Intensity-Aware LUT (IA-LUT) module for adaptive enhancement, which addresses the low-dynamic problem in low-light scenarios. This enables FastLLVE to perform low-latency and low-complexity enhancement operations while maintaining high-quality results. Experimental results on benchmark datasets demonstrate that our method achieves the State-Of-The-Art (SOTA) performance in terms of both image quality and inter-frame brightness consistency. More importantly, our FastLLVE can process 1,080p videos at 50+ Frames Per Second (FPS), which is 2\texttimes{} faster than SOTA CNN-based methods in inference time, making it a promising solution for real-time applications. The code is available at https://github.com/Wenhao-Li777/FastLLVE.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C:\Users\theun\Zotero\storage\RGMBLC3P\Li et al. - 2023 - FastLLVE Real-Time Low-Light Video Enhancement with Intensity-Aware Lookup Table.pdf}
}

@article{liLearningDepthLeveraging2023,
  title = {Learning Depth via Leveraging Semantics: {{Self-supervised}} Monocular Depth Estimation with Both Implicit and Explicit Semantic Guidance},
  shorttitle = {Learning Depth via Leveraging Semantics},
  author = {Li, Rui and Xue, Danna and Su, Shaolin and He, Xiantuo and Mao, Qing and Zhu, Yu and Sun, Jinqiu and Zhang, Yanning},
  year = 2023,
  month = may,
  journal = {Pattern Recognition},
  volume = {137},
  pages = {109297},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2022.109297},
  urldate = {2025-06-30},
  abstract = {Self-supervised monocular depth estimation has shown great success in learning depth using only images for supervision. In this paper, we propose to enhance self-supervised depth estimation with semantics and propose a novel learning scheme, which incorporates both implicit and explicit semantic guidances. Specifically, we propose to relate depth distributions to the semantic category information by proposing a Semantic-aware Spatial Feature Modulation (SSFM) scheme, which implicitly modulates the semantic and depth features in a joint learning framework. The modulation parameters are generated from semantic labels to acquire category-level guidance. Meanwhile, a semantic-guided ranking loss is proposed to explicitly constrain the estimated depth borders using the corresponding segmentation labels. To avoid the impact brought by erroneous segmentation labels, both robust sampling strategy and prediction uncertainty weighting are proposed for the ranking loss. Extensive experimental results show that our method produces high-quality depth maps with semantically consistent depth distributions and accurate depth edges, outperforming the state-of-the-art methods by significant margins.},
  keywords = {Robust point pair sampling,Semantic-aware spatial feature modulation,Semantic-guided ranking loss,Semantic-guided self-supervised depth estimation,Uncertainty weighting},
  file = {C\:\\Users\\theun\\Zotero\\storage\\UD8YCUV4\\Li et al. - 2023 - Learning depth via leveraging semantics Self-supervised monocular depth estimation with both implic.pdf;C\:\\Users\\theun\\Zotero\\storage\\TMKM7YZP\\S0031320322007762.html}
}

@misc{liLearningEnhanceLowLight2021,
  title = {Learning to {{Enhance Low-Light Image}} via {{Zero-Reference Deep Curve Estimation}}},
  author = {Li, Chongyi and Guo, Chunle and Loy, Chen Change},
  year = 2021,
  month = mar,
  number = {arXiv:2103.00860},
  eprint = {2103.00860},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.00860},
  urldate = {2025-01-19},
  abstract = {This paper presents a novel method, Zero-Reference Deep Curve Estimation (Zero-DCE), which formulates light enhancement as a task of image-specific curve estimation with a deep network. Our method trains a lightweight deep network, DCE-Net, to estimate pixel-wise and high-order curves for dynamic range adjustment of a given image. The curve estimation is specially designed, considering pixel value range, monotonicity, and differentiability. Zero-DCE is appealing in its relaxed assumption on reference images, i.e., it does not require any paired or even unpaired data during training. This is achieved through a set of carefully formulated non-reference loss functions, which implicitly measure the enhancement quality and drive the learning of the network. Despite its simplicity, we show that it generalizes well to diverse lighting conditions. Our method is efficient as image enhancement can be achieved by an intuitive and simple nonlinear curve mapping. We further present an accelerated and light version of Zero-DCE, called Zero-DCE++, that takes advantage of a tiny network with just 10K parameters. Zero-DCE++ has a fast inference speed (1000/11 FPS on a single GPU/CPU for an image of size 1200*900*3) while keeping the enhancement performance of Zero-DCE. Extensive experiments on various benchmarks demonstrate the advantages of our method over state-of-the-art methods qualitatively and quantitatively. Furthermore, the potential benefits of our method to face detection in the dark are discussed. The source code will be made publicly available at https://li-chongyi.github.io/Proj\_Zero-DCE++.html.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\DQPAWD5J\\Li et al. - 2021 - Learning to Enhance Low-Light Image via Zero-Reference Deep Curve Estimation.pdf;C\:\\Users\\theun\\Zotero\\storage\\QGPX9GV2\\2103.html}
}

@article{liLightenNetConvolutionalNeural2018,
  title = {{{LightenNet}}: {{A Convolutional Neural Network}} for Weakly Illuminated Image Enhancement},
  shorttitle = {{{LightenNet}}},
  author = {Li, Chongyi and Guo, Jichang and Porikli, Fatih and Pang, Yanwei},
  year = 2018,
  month = mar,
  journal = {Pattern Recognition Letters},
  volume = {104},
  pages = {15--22},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2018.01.010},
  urldate = {2025-03-11},
  abstract = {Weak illumination or low light image enhancement as pre-processing is needed in many computer vision tasks. Existing methods show limitations when they are used to enhance weakly illuminated images, especially for the images captured under diverse illumination circumstances. In this letter, we propose a trainable Convolutional Neural Network (CNN) for weakly illuminated image enhancement, namely LightenNet, which takes a weakly illuminated image as input and outputs its illumination map that is subsequently used to obtain the enhanced image based on Retinex model. The proposed method produces visually pleasing results without over or under-enhanced regions. Qualitative and quantitative comparisons are conducted to evaluate the performance of the proposed method. The experimental results demonstrate that the proposed method achieves superior performance than existing methods. Additionally, we propose a new weakly illuminated image synthesis approach, which can be use as a guide for weakly illuminated image enhancement networks training and full-reference image quality assessment.},
  keywords = {/unread,CNNs,Image degradation,Low light image enhancement,Weak illumination image enhancement},
  file = {C\:\\Users\\theun\\Zotero\\storage\\SB2LBQMT\\Li et al. - 2018 - LightenNet A Convolutional Neural Network for weakly illuminated image enhancement.pdf;C\:\\Users\\theun\\Zotero\\storage\\9ILZKZZY\\S0167865518300163.html}
}

@inproceedings{liLightNightMultiCondition2024,
  title = {Light the {{Night}}: {{A Multi-Condition Diffusion Framework}} for {{Unpaired Low-Light Enhancement}} in {{Autonomous Driving}}},
  shorttitle = {Light the {{Night}}},
  booktitle = {2024 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Li, Jinlong and Li, Baolu and Tu, Zhengzhong and Liu, Xinyu and Guo, Qing and {Juefei-Xu}, Felix and Xu, Runsheng and Yu, Hongkai},
  year = 2024,
  month = jun,
  pages = {15205--15215},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR52733.2024.01440},
  urldate = {2025-01-18},
  abstract = {Vision-centric perception systems for autonomous driving have gained considerable attention recently due to their cost-effectiveness and scalability, especially compared to LiDAR-based systems. However, these systems often struggle in low-light conditions, potentially compromising their performance and safety. To address this, our paper introduces LightDiff, a domain-tailored framework designed to enhance the low-light image quality for autonomous driving applications. Specifically, we employ a multicondition controlled diffusion model. LightDiff works without any human-collected paired data, leveraging a dynamic data degradation process instead. It incorporates a novel multi-condition adapter that adaptively controls the input weights from different modalities, including depth maps, RGB images, and text captions, to effectively illuminate dark scenes while maintaining context consistency. Furthermore, to align the enhanced images with the detection model's knowledge, LightDiff employs perception-specific scores as rewards to guide the diffusion training process through reinforcement learning. Extensive experiments on the nuScenes datasets demonstrate that LightDiff can significantly improve the performance of several state-of-the-art 3D detectors in night-time conditions while achieving high visual quality scores, highlighting its potential to safeguard autonomous driving.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-5300-6},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\EMUHMGBE\Li et al. - 2024 - Light the Night A Multi-Condition Diffusion Framework for Unpaired Low-Light Enhancement in Autonom.pdf}
}

@article{liLowLightImageVideo2022,
  title = {Low-{{Light Image}} and {{Video Enhancement Using Deep Learning}}: {{A Survey}}},
  shorttitle = {Low-{{Light Image}} and {{Video Enhancement Using Deep Learning}}},
  author = {Li, Chongyi and Guo, Chunle and Han, Linghao and Jiang, Jun and Cheng, Ming-Ming and Gu, Jinwei and Loy, Chen Change},
  year = 2022,
  month = dec,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {12},
  pages = {9396--9416},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2021.3126387},
  urldate = {2024-12-04},
  abstract = {Low-light image enhancement (LLIE) aims at improving the perception or interpretability of an image captured in an environment with poor illumination. Recent advances in this area are dominated by deep learning-based solutions, where many learning strategies, network structures, loss functions, training data, etc. have been employed. In this paper, we provide a comprehensive survey to cover various aspects ranging from algorithm taxonomy to unsolved open issues. To examine the generalization of existing methods, we propose a low-light image and video dataset, in which the images and videos are taken by different mobile phones' cameras under diverse illumination conditions. Besides, for the first time, we provide a unified online platform that covers many popular LLIE methods, of which the results can be produced through a user-friendly web interface. In addition to qualitative and quantitative evaluation of existing methods on publicly available and our proposed datasets, we also validate their performance in face detection in the dark. This survey together with the proposed dataset and online platform could serve as a reference source for future study and promote the development of this research field. The proposed platform and dataset as well as the collected methods, datasets, and evaluation metrics are publicly available and will be regularly updated. Project page: https://www. mmlab-ntu.com/project/lliv\_survey/index.html.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\44SFELJM\Li et al. - 2022 - Low-Light Image and Video Enhancement Using Deep Learning A Survey.pdf}
}

@article{liLuminanceDomainguidedLowlight2024,
  title = {Luminance Domain-Guided Low-Light Image Enhancement},
  author = {Li, Yuhang and Wang, Chao and Liang, Bing and Cai, Feifan and Ding, Youdong},
  year = 2024,
  month = jul,
  journal = {Neural Computing and Applications},
  volume = {36},
  number = {21},
  pages = {13187--13203},
  issn = {0941-0643, 1433-3058},
  doi = {10.1007/s00521-024-09687-x},
  urldate = {2025-02-27},
  abstract = {Images captured under low-light conditions often suffer from low contrast, high noise, and uneven brightness due to nightlight, backlight, and shadow. These challenges make it difficult to use them as high-quality inputs for visual tasks. Existing low-light enhancement methods tend to increase overall image brightness, which can cause overexposure of normal-light areas after enhancement. To solve this problem, this paper proposes an Uneven Dark Vision Network (UDVN) that consists of two sub-networks. The Luminance Domain Network (LDN) uses Direction-aware Spatial Context (DSC) and Feature Enhancement Module (FEM) to segment different light regions in the image and output the luminance domain mask. Guided by this mask, the Light Enhancement Network (LEN) uses the Cross-Domain Transformation Residual block (CDTR) to adaptively illuminate different regions with various lights. We also introduce a new region loss function to constrain the LEN to better enhance the quality of different light regions. In addition, we have constructed a new low-light synthesis dataset (UDL) that is larger, more diverse, and includes uneven lighting states in the real world. Extensive experiments on several benchmark datasets demonstrate that our proposed method is highly competitive with state-of-the-art (SOTA) methods. Specifically, it outperforms other methods in light recovery and detail preservation when processing uneven low-light images. The UDL dataset is publicly available at: https://github.com/ YuhangLi-li/UDVN.},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\Q48KFPTS\Li et al. - 2024 - Luminance domain-guided low-light image enhancement.pdf}
}

@inproceedings{liMegaDepthLearningSingleView2018,
  title = {{{MegaDepth}}: {{Learning Single-View Depth Prediction From Internet Photos}}},
  shorttitle = {{{MegaDepth}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Li, Zhengqi and Snavely, Noah},
  year = 2018,
  pages = {2041--2050},
  urldate = {2025-07-03},
  file = {C:\Users\theun\Zotero\storage\XPIEYNV5\Li and Snavely - 2018 - MegaDepth Learning Single-View Depth Prediction From Internet Photos.pdf}
}

@inproceedings{linEnhancingLowlightInstance2025,
  title = {Enhancing Low-Light Instance Segmentation through Feature-Level Denoising},
  booktitle = {Machine {{Learning}} from {{Challenging Data}} 2025},
  author = {Lin, Joanne and Bull, David and Anantrasirichai, Nantheera},
  editor = {Sklivanitis, George and Markopoulos, Panagiotis (. and Ouyang, Bing},
  year = 2025,
  month = may,
  pages = {13},
  publisher = {SPIE},
  address = {Orlando, United States},
  doi = {10.1117/12.3054001},
  urldate = {2025-06-04},
  abstract = {Instance segmentation accurately delineates the precise boundaries of each distinct object in an image or video. However, performing this task in low-light conditions presents challenges due to issues such as shot noise from low photon counts, color distortions, and reduced contrast. In this work, we propose a plug-and-play solution designed to address these complexities. Our approach integrates weighted non-local blocks (wNLB) into the feature extractor, enabling inherent denoising at the feature level. The proposed method incorporates learnable weights at each layer, allowing the network to adapt to the varying noise characteristics across different feature scales. We demonstrate that our wNLB improves the performance of object detectors and trackers when compared to pretrained networks.},
  isbn = {978-1-5106-8709-7 978-1-5106-8710-3},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\EB9UBX2B\Lin et al. - 2025 - Enhancing low-light instance segmentation through feature-level denoising.pdf}
}

@article{linFocalLossDense,
  title = {Focal {{Loss}} for {{Dense Object Detection}}},
  author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollar, Piotr},
  abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors.},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\5PCTB33T\Lin et al. - Focal Loss for Dense Object Detection.pdf}
}

@misc{linFocalLossDense2018,
  title = {Focal {{Loss}} for {{Dense Object Detection}}},
  author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  year = 2018,
  month = feb,
  number = {arXiv:1708.02002},
  eprint = {1708.02002},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1708.02002},
  urldate = {2025-10-23},
  abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\KYLV5WQS\\Lin et al. - 2018 - Focal Loss for Dense Object Detection.pdf;C\:\\Users\\theun\\Zotero\\storage\\MFSIH5C4\\1708.html}
}

@article{linNoreferenceQualityAssessment2023,
  title = {No-Reference Quality Assessment for Low-Light Image Enhancement: {{Subjective}} and Objective Methods},
  shorttitle = {No-Reference Quality Assessment for Low-Light Image Enhancement},
  author = {Lin, Weitao and Wu, Yuxuan and Xu, Lishi and Chen, Weiling and Zhao, Tiesong and Wei, Hongan},
  year = 2023,
  month = jul,
  journal = {Displays},
  volume = {78},
  pages = {102432},
  issn = {01419382},
  doi = {10.1016/j.displa.2023.102432},
  urldate = {2024-12-04},
  abstract = {No-Reference (NR) quality assessment is a crucial approach for evaluating the quality of low-light enhanced images, as it is often difficult to acquire high-quality reference images in applications such as night-time automatic driving. However, current NR evaluation methods for low-light enhanced images often lack consideration of important characteristics such as color, structure, and naturalness. This paper proposes a novel NR quality assessment method for NR low-light enhanced images from both subjective and objective aspects. On the subjective side, we construct the Low-light Enhanced Images Subjective Dataset (LEISD) containing 2040 images with 255 different image contents. Each image was evaluated based on the Single Stimulus (SS) method by 20 subjects. On the objective side, we propose Multi-Features Reconciliation-based Quality Assessment (MFRQA) methods for low-light enhanced images by observing the low-light enhanced images. The MFRQA summarized four key feature perspectives: brightness, color, structure and naturalness, and employed the traditional machine learning model to reconcile the multi-features. Experimental results on the LEISD dataset demonstrate competitive performance and low complexity of our method compared to the representative quality metrics.},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\AFAZHARV\Lin et al. - 2023 - No-reference quality assessment for low-light image enhancement Subjective and objective methods.pdf}
}

@article{linRealTimeMonocularVision2021,
  title = {Real-{{Time Monocular Vision System}} for {{UAV Autonomous Landing}} in {{Outdoor Low-Illumination Environments}}},
  author = {Lin, Shanggang and Jin, Lianwen and Chen, Ziwei},
  year = 2021,
  month = jan,
  journal = {Sensors},
  volume = {21},
  number = {18},
  pages = {6226},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s21186226},
  urldate = {2025-01-03},
  abstract = {Landing an unmanned aerial vehicle (UAV) autonomously and safely is a challenging task. Although the existing approaches have resolved the problem of precise landing by identifying a specific landing marker using the UAV's onboard vision system, the vast majority of these works are conducted in either daytime or well-illuminated laboratory environments. In contrast, very few researchers have investigated the possibility of landing in low-illumination conditions by employing various active light sources to lighten the markers. In this paper, a novel vision system design is proposed to tackle UAV landing in outdoor extreme low-illumination environments without the need to apply an active light source to the marker. We use a model-based enhancement scheme to improve the quality and brightness of the onboard captured images, then present a hierarchical-based method consisting of a decision tree with an associated light-weight convolutional neural network (CNN) for coarse-to-fine landing marker localization, where the key information of the marker is extracted and reserved for post-processing, such as pose estimation and landing control. Extensive evaluations have been conducted to demonstrate the robustness, accuracy, and real-time performance of the proposed vision system. Field experiments across a variety of outdoor nighttime scenarios with an average luminance of 5 lx at the marker locations have proven the feasibility and practicability of the system.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {autonomous landing,low-illumination,marker detection,real-time,unmanned aerial vehicle},
  file = {C:\Users\theun\Zotero\storage\8MMZ2WL7\Lin et al. - 2021 - Real-Time Monocular Vision System for UAV Autonomous Landing in Outdoor Low-Illumination Environment.pdf}
}

@inproceedings{linRefineNetMultiPathRefinement2017,
  title = {{{RefineNet}}: {{Multi-Path Refinement Networks}} for {{High-Resolution Semantic Segmentation}}},
  shorttitle = {{{RefineNet}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Lin, Guosheng and Milan, Anton and Shen, Chunhua and Reid, Ian},
  year = 2017,
  pages = {1925--1934},
  urldate = {2025-06-04},
  file = {C:\Users\theun\Zotero\storage\2ZIZ593N\Lin et al. - 2017 - RefineNet Multi-Path Refinement Networks for High-Resolution Semantic Segmentation.pdf}
}

@inproceedings{liPointCNNConvolutionXTransformed2018,
  title = {{{PointCNN}}: {{Convolution On X-Transformed Points}}},
  shorttitle = {{{PointCNN}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Li, Yangyan and Bu, Rui and Sun, Mingchao and Wu, Wei and Di, Xinhan and Chen, Baoquan},
  year = 2018,
  volume = {31},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-01-03},
  abstract = {We present a simple and general framework for feature learning from point cloud. The key to the success of CNNs is the convolution operator that is capable of leveraging spatially-local correlation in data represented densely in grids (e.g. images). However, point cloud are irregular and unordered, thus a direct convolving of kernels against the features associated with the points will result in deserting the shape information while being variant to the orders. To address these problems, we propose to learn a X-transformation from the input points, which is used for simultaneously weighting the input features associated with the points and permuting them into latent potentially canonical order. Then element-wise product and sum operations of typical convolution operator are applied on the X-transformed features. The proposed method is a generalization of typical CNNs into learning features from point cloud, thus we call it PointCNN. Experiments show that PointCNN achieves on par or better performance than state-of-the-art methods on multiple challenging benchmark datasets and tasks.},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\5JZTER3T\Li et al. - 2018 - PointCNN Convolution On X-Transformed Points.pdf}
}

@article{liSegmentingObjectsDay2021,
  title = {Segmenting {{Objects}} in {{Day}} and {{Night}}: {{Edge-Conditioned CNN}} for {{Thermal Image Semantic Segmentation}}},
  shorttitle = {Segmenting {{Objects}} in {{Day}} and {{Night}}},
  author = {Li, Chenglong and Xia, Wei and Yan, Yan and Luo, Bin and Tang, Jin},
  year = 2021,
  month = jul,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {32},
  number = {7},
  pages = {3069--3082},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2020.3009373},
  urldate = {2025-05-05},
  abstract = {Despite much research progress in image semantic segmentation, it remains challenging under adverse environmental conditions caused by imaging limitations of the visible spectrum, while thermal infrared cameras have several advantages over cameras for the visible spectrum, such as operating in total darkness, insensitive to illumination variations, robust to shadow effects, and strong ability to penetrate haze and smog. These advantages of thermal infrared cameras make the segmentation of semantic objects in day and night. In this article, we propose a novel network architecture, called edge-conditioned convolutional neural network (EC-CNN), for thermal image semantic segmentation. Particularly, we elaborately design a gated featurewise transform layer in EC-CNN to adaptively incorporate edge prior knowledge. The whole EC-CNN is end-to-end trained and can generate high-quality segmentation results with edge guidance. Meanwhile, we also introduce a new benchmark data set named ``Segmenting Objects in Day And night'' (SODA) for comprehensive evaluations in thermal image semantic segmentation. SODA contains over 7168 manually annotated and synthetically generated thermal images with 20 semantic region labels and from a broad range of viewpoints and scene complexities. Extensive experiments on SODA demonstrate the effectiveness of the proposed EC-CNN against state-of-the-art methods.},
  keywords = {/unread,Benchmark data set,Benchmark testing,Cameras,edge prior knowledge,featurewise affine transform,Image edge detection,Image segmentation,semantic segmentation,Semantics,Task analysis,thermal infrared image},
  file = {C:\Users\theun\Zotero\storage\8TUVVS5V\Li et al. - 2021 - Segmenting Objects in Day and Night Edge-Conditioned CNN for Thermal Image Semantic Segmentation.pdf}
}

@incollection{liSemanticFlowFast2020,
  title = {Semantic {{Flow}} for {{Fast}} and {{Accurate Scene Parsing}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2020},
  author = {Li, Xiangtai and You, Ansheng and Zhu, Zhen and Zhao, Houlong and Yang, Maoke and Yang, Kuiyuan and Tan, Shaohua and Tong, Yunhai},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year = 2020,
  volume = {12346},
  pages = {775--793},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-58452-8_45},
  urldate = {2025-05-02},
  abstract = {In this paper, we focus on designing effective method for fast and accurate scene parsing. A common practice to improve the performance is to attain high resolution feature maps with strong semantic representation. Two strategies are widely used---atrous convolutions and feature pyramid fusion, are either computation intensive or ineffective. Inspired by the Optical Flow for motion alignment between adjacent video frames, we propose a Flow Alignment Module (FAM) to learn Semantic Flow between feature maps of adjacent levels, and broadcast high-level features to high resolution features effectively and efficiently. Furthermore, integrating our module to a common feature pyramid structure exhibits superior performance over other real-time methods even on light-weight backbone networks, such as ResNet-18. Extensive experiments are conducted on several challenging datasets, including Cityscapes, PASCAL Context, ADE20K and CamVid. Especially, our network is the first to achieve 80.4\% mIoU on Cityscapes with a frame rate of 26 FPS. The code is available at https://github.com/ donnyyou/torchcv.},
  isbn = {978-3-030-58451-1 978-3-030-58452-8},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\VFVBZMZJ\Li et al. - 2020 - Semantic Flow for Fast and Accurate Scene Parsing.pdf}
}

@article{liStructureRevealingLowLightImage2018,
  title = {Structure-{{Revealing Low-Light Image Enhancement Via Robust Retinex Model}}},
  author = {Li, Mading and Liu, Jiaying and Yang, Wenhan and Sun, Xiaoyan and Guo, Zongming},
  year = 2018,
  month = jun,
  journal = {IEEE Transactions on Image Processing},
  volume = {27},
  number = {6},
  pages = {2828--2841},
  issn = {1941-0042},
  doi = {10.1109/TIP.2018.2810539},
  urldate = {2025-03-11},
  abstract = {Low-light image enhancement methods based on classic Retinex model attempt to manipulate the estimated illumination and to project it back to the corresponding reflectance. However, the model does not consider the noise, which inevitably exists in images captured in low-light conditions. In this paper, we propose the robust Retinex model, which additionally considers a noise map compared with the conventional Retinex model, to improve the performance of enhancing low-light images accompanied by intensive noise. Based on the robust Retinex model, we present an optimization function that includes novel regularization terms for the illumination and reflectance. Specifically, we use {$\ell$}1 norm to constrain the piece-wise smoothness of the illumination, adopt a fidelity term for gradients of the reflectance to reveal the structure details in low-light images, and make the first attempt to estimate a noise map out of the robust Retinex model. To effectively solve the optimization problem, we provide an augmented Lagrange multiplier based alternating direction minimization algorithm without logarithmic transformation. Experimental results demonstrate the effectiveness of the proposed method in low-light image enhancement. In addition, the proposed method can be generalized to handle a series of similar problems, such as the image enhancement for underwater or remote sensing and in hazy or dusty conditions.},
  keywords = {Image color analysis,Image enhancement,Lighting,Low-light image enhancement,Noise reduction,noise suppression,Optimization,Retinex model,Robustness,structure-revealing,Task analysis},
  file = {C\:\\Users\\theun\\Zotero\\storage\\57NRXGKD\\Li et al. - 2018 - Structure-Revealing Low-Light Image Enhancement Via Robust Retinex Model.pdf;C\:\\Users\\theun\\Zotero\\storage\\9M8UANSR\\8304597.html}
}

@article{liuASABiSeNetImprovedRealtime2023,
  title = {{{ASA-BiSeNet}}: Improved Real-Time Approach for Road Lane Semantic Segmentation of Low-Light Autonomous Driving Road Scenes},
  author = {Liu, Y. and Yi, F. and Ma, Y. and Wang, Y.},
  year = 2023,
  journal = {Applied Optics},
  volume = {62},
  number = {19},
  pages = {5224--5235},
  publisher = {Optica Publishing Group (formerly OSA)},
  issn = {1559128X (ISSN)},
  doi = {10.1364/AO.486302},
  abstract = {The solution to the problem of road environmental perception is one of the essential prerequisites to realizing the autonomous driving of intelligent vehicles, and road lane detection plays a crucial role in road environmental perception. However, road lane detection in complex road scenes is challenging due to poor illumination conditions, the occlusion of other objects, and the influence of unrelated road markings. It also hinders the commercial application of autonomous driving technology in various road scenes. In order to minimize the impact of illumination factors on road lane detection tasks, researchers use deep learning (DL) technology to enhance low-light images. In this study, road lane detection is regarded as an image segmentation problem, and road lane detection is studied based on the DL approach to meet the challenge of rapid environmental changes during driving. First, the Zero- DCE++ approach is used to enhance the video frame of the road scene under low-light conditions. Then, based on the bilateral segmentation network (BiSeNet) approach, the approach of associate self-attention with BiSeNet (ASA-BiSeNet) integrating two attention mechanisms is designed to improve the road lane detection ability. Finally, the ASA-BiSeNet approach is trained based on the self-made road lane dataset for the road lane detection task. At the same time, the approach based on the BiSeNet approach is compared with the ASA-BiSeNet approach. The experimental results show that the frames per second (FPS) of the ASA-BiSeNet approach is about 152.5 FPS, and its mean intersection over union is 71.39\%, which can meet the requirements of real-time autonomous driving.  \copyright 2023 Optica Publishing Group.},
  langid = {english},
  keywords = {article,attention,Autonomous driving,Autonomous vehicles,deep learning,Deep learning,Detection tasks,environmental change,Environmental perceptions,Frames per seconds,illumination,Illumination conditions,Image enhancement,image segmentation,Intelligent roads,Low light,Real- time,Road and street markings,Road lane detections,Roads and streets,Semantic segmentation,Semantic Segmentation,Semantics,videorecording}
}

@article{liuBenchmarkingLowLightImage2021,
  title = {Benchmarking {{Low-Light Image Enhancement}} and {{Beyond}}},
  author = {Liu, Jiaying and Xu, Dejia and Yang, Wenhan and Fan, Minhao and Huang, Haofeng},
  year = 2021,
  month = apr,
  journal = {International Journal of Computer Vision},
  volume = {129},
  number = {4},
  pages = {1153--1184},
  publisher = {Springer US},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-020-01418-8},
  urldate = {2025-01-16},
  abstract = {In this paper, we present a systematic review and evaluation of existing single-image low-light enhancement algorithms. Besides the commonly used low-level vision oriented evaluations, we additionally consider measuring machine vision performance in the low-light condition via face detection task to explore the potential of joint optimization of high-level and low-level vision enhancement. To this end, we first propose a large-scale low-light image dataset serving both low/high-level vision with diversified scenes and contents as well as complex degradation in real scenarios, called Vision Enhancement in the LOw-Light condition (VE-LOL). Beyond paired low/normal-light images without annotations, we additionally include the analysis resource related to human, i.e. face images in the low-light condition with annotated face bounding boxes. Then, efforts are made on benchmarking from the perspective of both human and machine visions. A rich variety of criteria is used for the low-level vision evaluation, including full-reference, no-reference, and semantic similarity metrics. We also measure the effects of the low-light enhancement on face detection in the low-light condition. State-of-the-art face detection methods are used in the evaluation. Furthermore, with the rich material of VE-LOL, we explore the novel problem of joint low-light enhancement and face detection. We develop an enhanced face detector to apply low-light enhancement and face detection jointly. The features extracted by the enhancement module are fed to the successive layer with the same resolution of the detection module. Thus, these features are intertwined together to unitedly learn useful information across two phases, i.e. enhancement and detection. Experiments on VE-LOL provide a comparison of state-of-the-art low-light enhancement algorithms, point out their limitations, and suggest promising future directions. Our dataset has supported the Track ``Face Detection in Low Light Conditions'' of CVPR UG2+ Challenge (2019--2020) ( http://cvpr2020.ug2challenge.org/ ).},
  copyright = {2021 The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\B4S62HK2\Liu et al. - 2021 - Benchmarking Low-Light Image Enhancement and Beyond.pdf}
}

@article{liuChannelInteractionTransformer2024,
  title = {Channel {{Interaction}} and {{Transformer Depth Estimation Network}}: {{Robust Self-Supervised Depth Estimation Under Varied Weather Conditions}}},
  shorttitle = {Channel {{Interaction}} and {{Transformer Depth Estimation Network}}},
  author = {Liu, Jianqiang and Guo, Zhengyu and Ping, Peng and Zhang, Hao and Shi, Quan},
  year = 2024,
  month = jan,
  journal = {Sustainability},
  volume = {16},
  number = {20},
  pages = {9131},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2071-1050},
  doi = {10.3390/su16209131},
  urldate = {2025-06-30},
  abstract = {Monocular depth estimation provides low-cost environmental information for intelligent systems such as autonomous vehicles and robots, supporting sustainable development by reducing reliance on expensive, energy-intensive sensors and making technology more accessible and efficient. However, in practical applications, monocular vision is highly susceptible to adverse weather conditions, significantly reducing depth perception accuracy and limiting its ability to deliver reliable environmental information. To improve the robustness of monocular depth estimation in challenging weather, this paper first utilizes generative models to adjust image exposure and generate synthetic images of rainy, foggy, and nighttime scenes, enriching the diversity of the training data. Next, a channel interaction module and Multi-Scale Fusion Module are introduced. The former enhances information exchange between channels, while the latter effectively integrates multi-level feature information. Finally, an enhanced consistency loss is added to the loss function to prevent the depth estimation bias caused by data augmentation. Experiments on datasets such as DrivingStereo, Foggy CityScapes, and NuScenes-Night demonstrate that our method, CIT-Depth, exhibits superior generalization across various complex conditions.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {deep learning,feature enhancement,self-supervised depth estimation},
  file = {C:\Users\theun\Zotero\storage\R5D7A6KX\Liu et al. - 2024 - Channel Interaction and Transformer Depth Estimation Network Robust Self-Supervised Depth Estimatio.pdf}
}

@inproceedings{liuConvNet2020s2022,
  title = {A {{ConvNet}} for the 2020s},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  year = 2022,
  pages = {11976--11986},
  urldate = {2025-06-05},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\M2P9GGVH\Liu et al. - 2022 - A ConvNet for the 2020s.pdf}
}

@article{liuDualBranchMultiscaleOptimization2025,
  title = {Dual-{{Branch Multiscale Optimization Network}} for {{Enhancing Low-Light Images}} in {{Rail Transit Obstacle Detection}} and {{Segmentation}}},
  author = {Liu, Qi and He, Deqiang and Zhang, Mingchao and Wu, Jinxin},
  year = 2025,
  month = feb,
  journal = {IEEE Sensors Journal},
  volume = {25},
  number = {3},
  pages = {5697--5710},
  issn = {1558-1748},
  doi = {10.1109/JSEN.2024.3511554},
  urldate = {2025-06-18},
  abstract = {The existing deep learning-based low-light image enhancement models have demonstrated their validity on many benchmark datasets. However, it is not easy to enhance the brightness, color, contrast, and other information of images while maintaining the quality of image details with these models. To enhance the visual perception of images under low-light conditions at night, improve driving visibility, and increase the accuracy of obstacle detection and segmentation, a dual-branch multiscale optimization (DBMO) network is proposed in this article. The global branch employs a multiscale network based on the transformer, which preserves high-resolution input images while effectively integrating multiscale features to capture global image information. Meanwhile, the detailed branch introduces an adjustment denoising network based on wavelet transform. It performs noise suppression and detail enhancement on the high- and low-frequency information obtained from wavelet decomposition, thereby enhancing image details while balancing brightness and contrast. Finally, the feature information extracted from both branches is adaptively weighted and fused to produce the final enhanced image. The experimental results on a real rail transit obstacle dataset demonstrate that DBMO significantly improves images' overall brightness and color balance and achieves the highest accuracy improvement in rail transit obstacle detection and segmentation. Compared to baseline models YOLO-v8 and Deeplab-v3+, obstacle target detection and semantic segmentation accuracy improved by 3.6\% and 7.65\%, respectively. This model can be applied to nighttime train assistance systems and shows promising potential in obstacle detection and segmentation tasks.},
  keywords = {Accuracy,Benchmarking,Brightness,Dual-branch network,Feature extraction,Features fusions,Image denoising,Image enhancement,Laser beams,Light rail transit,Lighting,low-light image enhancement,Low-light image enhancement,Low-light images,Multi-scale feature fusion,Multi-scale features,Multi-scale optimization,multiscale feature fusion,Noise reduction,obstacle detection,Obstacle detectors,Obstacle segmentation,Obstacles detection,Optimization,Railroad transportation,Semantic segmentation,Semantic Segmentation,transformer,Transformer,Transformers,Visual perception,Wavelet transforms},
  file = {C:\Users\theun\Zotero\storage\F92RKU6R\Liu et al. - 2025 - Dual-Branch Multiscale Optimization Network for Enhancing Low-Light Images in Rail Transit Obstacle.pdf}
}

@article{liuEfficientRetinexBasedFramework2025,
  title = {Efficient {{Retinex-Based Framework}} for {{Low-Light Image Enhancement}} without {{Additional Networks}}},
  author = {Liu, Chunxiao and Wang, Zelong and Birch, Philip and Wang, Xun},
  year = 2025,
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  pages = {1--1},
  issn = {1051-8215, 1558-2205},
  doi = {10.1109/TCSVT.2024.3520802},
  urldate = {2025-02-27},
  abstract = {Images captured in low-light environments often suffer from significant degradation. However, most existing Retinexbased methods require an additional decomposition network and overlook the degradation caused by the illumination adjustment process, which results in the consumption of significant computational resources to achieve only average performance. To address the above issues, this paper proposes a more efficient Retinexbased approach named RetinexMac that allows training without an additional decomposition network or regularization functions. RetinexMac first employs an illumination coefficient estimation network to estimate the transform map and light up the global illumination and the local contrast of input images, then a multiscale degradation estimation network is used to suppress the degradation amplified by the illumination adjustment. In order to accurately estimate the degradation, a convolution and attention mixed module integrates the global and local spatial information. This is shown to also significantly improve the performance of other previous Retinex-based methods. Extensive experiments on several representative datasets show that our RetinexMac achieves both current state-of-the-art (SOTA) performance and more ideal visual appearance in terms of illumination and detail, as well as computational efficiency.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\DEP7DL92\Liu et al. - 2025 - Efficient Retinex-Based Framework for Low-Light Image Enhancement without Additional Networks.pdf}
}

@article{liuImprovingNighttimeDrivingScene2023,
  title = {Improving {{Nighttime Driving-Scene Segmentation}} via {{Dual Image-Adaptive Learnable Filters}}},
  author = {Liu, Wenyu and Li, Wentong and Zhu, Jianke and Cui, Miaomiao and Xie, Xuansong and Zhang, Lei},
  year = 2023,
  month = oct,
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {33},
  number = {10},
  pages = {5855--5867},
  issn = {1558-2205},
  doi = {10.1109/TCSVT.2023.3260240},
  urldate = {2025-06-25},
  abstract = {Semantic segmentation on driving-scene images is vital for autonomous driving. Although encouraging performance has been achieved on daytime images, the performance on nighttime images are less satisfactory due to the insufficient exposure and the lack of labeled data. To address these issues, we present an add-on module called dual image-adaptive learnable filters (DIAL-Filters) to improve the semantic segmentation in nighttime driving conditions, aiming at exploiting the intrinsic features of driving-scene images under different illuminations. DIAL-Filters consist of two parts, including an image-adaptive processing module (IAPM) and a learnable guided filter (LGF). With DIAL-Filters, we design both unsupervised and supervised frameworks for nighttime driving-scene segmentation, which can be trained in an end-to-end manner. Specifically, the IAPM module consists of a small convolutional neural network with a set of differentiable image filters, where each image can be adaptively enhanced for better segmentation with respect to the different illuminations. The LGF is employed to enhance the output of segmentation network to get the final segmentation result. The DIAL-Filters are light-weight and efficient and they can be readily applied for both daytime and nighttime images. Our experiments show that DAIL-Filters can significantly improve the supervised segmentation performance on ACDC\_Night and NightCity datasets, while it demonstrates the state-of-the-art performance on unsupervised nighttime semantic segmentation on Dark Zurich and Nighttime Driving testbeds. Codes and models are available at https://github.com/wenyyu/IA-Seg.},
  keywords = {Adaptation models,Autonomous driving,Convolutional neural networks,differentiable filter,Image resolution,Lighting,nighttime vision,semantic segmentation,Semantic segmentation,Task analysis,Training},
  file = {C:\Users\theun\Zotero\storage\8LIGCTGA\Liu et al. - 2023 - Improving Nighttime Driving-Scene Segmentation via Dual Image-Adaptive Learnable Filters.pdf}
}

@article{liuLaneDetectionBased2024,
  title = {Lane Detection Based on Real-Time Semantic Segmentation for End-to-End Autonomous Driving under Low-Light Conditions},
  author = {Liu, Yang and Wang, Yongfu and Li, Qiansheng},
  year = 2024,
  month = dec,
  journal = {Digital Signal Processing},
  volume = {155},
  pages = {104752},
  issn = {1051-2004},
  doi = {10.1016/j.dsp.2024.104752},
  urldate = {2025-01-19},
  abstract = {Lane detection is one of the important modules of the autonomous driving system for environmental perception. In recent years, lane detection based on the semantic segmentation method has effectively promoted the development of autonomous driving technology. It provides lane information of complex driving scenes for the Deep Reinforcement Learning (DRL) method as a decision-making reference, which is the key technology to achieve vehicle-assisted driving and even autonomous driving. Aiming at the problems of feature extraction difficulty and low utilization rate of semantic feature information in lane detection method based on semantic segmentation in low-light scenes, which lead to miss-detection or false-alarm, this study designs a real-time lane segmentation method based on the encoding-decoding mechanism in low-light scenes, namely UET-STDC. The method first uses the ZERO-DCE++ method to improve the quality of low-light road video frames and then designs a multi-scale feature extraction module combined with deformable convolution to improve the method's ability to extract lane semantic features in complex environments and uses skip connections to promote the fusion of shallow semantic features and deep semantic features. Finally, some of the up-sampling modules in the encoder structure are simplified, and the depthwise separable convolution is introduced to replace the convolution to reduce the computational complexity of the UET-STDC method. In addition, in order to obtain a better representation of lane features, this study combines attention mechanisms in the encoder structure. A large number of experiments have been carried out on the self-made lane semantic segmentation dataset. The results show that the UET-STDC method can significantly improve the accuracy of lane segmentation under low-light conditions and has a good segmentation effect and robustness under various illumination conditions.},
  keywords = {Autonomous driving,Autonomous driving system,Deep learning,Deep reinforcement learning,Driving systems,End to end,End-to-end,Image coding,Image enhancement,Lane detection,Laser beams,Low light,Real- time,Real-time,Reinforcement learning,Semantic features,Semantic segmentation,Semantic Segmentation,STDC},
  file = {C\:\\Users\\theun\\Zotero\\storage\\FLFVEJLH\\Liu et al. - 2024 - Lane detection based on real-time semantic segmentation for end-to-end autonomous driving under low-.pdf;C\:\\Users\\theun\\Zotero\\storage\\PVCQP4J5\\S1051200424003774.html}
}

@article{liuLearningNestedScene2023,
  title = {Learning {{With Nested Scene Modeling}} and {{Cooperative Architecture Search}} for {{Low-Light Vision}}},
  author = {Liu, Risheng and Ma, Long and Ma, Tengyu and Fan, Xin and Luo, Zhongxuan},
  year = 2023,
  month = may,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {45},
  number = {5},
  pages = {5953--5969},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2022.3212995},
  urldate = {2025-02-27},
  abstract = {Images captured from low-light scenes often suffer from severe degradations, including low visibility, color casts, intensive noises, etc. These factors not only degrade image qualities, but also affect the performance of downstream Low-Light Vision (LLV) applications. A variety of deep networks have been proposed to enhance the visual quality of low-light images. However, they mostly rely on significant architecture engineering and often suffer from the high computational burden. More importantly, it still lacks an efficient paradigm to uniformly handle various tasks in the LLV scenarios. To partially address the above issues, we establish Retinex-inspired Unrolling with Architecture Search (RUAS), a general learning framework, that can address low-light enhancement task, and has the flexibility to handle other challenging downstream vision tasks. Specifically, we first establish a nested optimization formulation, together with an unrolling strategy, to explore underlying principles of a series of LLV tasks. Furthermore, we design a differentiable strategy to cooperatively search specific scene and task architectures for RUAS. Last but not least, we demonstrate how to apply RUAS for both low- and high-level LLV applications (e.g., enhancement, detection and segmentation). Extensive experiments verify the flexibility, effectiveness, and efficiency of RUAS.},
  keywords = {Computer architecture,cooperative architecture search,Deep learning,Image enhancement,Low-light vision,nested optimization,Noise reduction,Retinex-inspired unrolling,Task analysis,Training,Visualization},
  file = {C\:\\Users\\theun\\Zotero\\storage\\FWWEIQQ6\\Liu et al. - 2023 - Learning With Nested Scene Modeling and Cooperative Architecture Search for Low-Light Vision.pdf;C\:\\Users\\theun\\Zotero\\storage\\KIDV7IXW\\9914672.html}
}

@article{liuMultiViewThermalVisible2023,
  title = {A {{Multi-View Thermal}}--{{Visible Image Dataset}} for {{Cross-Spectral Matching}}},
  author = {Liu, Yuxiang and Liu, Yu and Yan, Shen and Chen, Chen and Zhong, Jikun and Peng, Yang and Zhang, Maojun},
  year = 2023,
  month = jan,
  journal = {Remote Sensing},
  volume = {15},
  number = {1},
  pages = {174},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2072-4292},
  doi = {10.3390/rs15010174},
  urldate = {2025-05-15},
  abstract = {Cross-spectral local feature matching between visual and thermal images benefits many vision tasks in low-light environments, including image-to-image fusion and camera re-localization. An essential prerequisite for unleashing the potential of supervised deep learning algorithms in the area of visible--thermal matching is the availability of large-scale and high-quality annotated datasets. However, publicly available datasets are either in relative small quantity scales or have limited pose annotations due to the expensive cost of data acquisition and annotation, which severely hinders the development of this field. In this paper, we proposed a multi-view thermal--visible image dataset for large-scale cross-spectral matching. We first recovered a 3D reference model from a group of collected RGB images, in which a certain image (bridge) shares almost the same pose as the thermal query. We then effectively registered the thermal image to the model based on manually annotating a 2D-2D tie point between the bridge and the thermal. In this way, through simply annotating one same viewpoint image pair, numerous overlapping image pairs between thermal and visible could be available. We also proposed a semi-automatic approach for generating accurate supervision for training multi-view cross-spectral matching. Specifically, our dataset consists of 40,644 cross-modal pairs with well supervision, covering multiple complex scenes. In addition, we also provided the camera metadata, 3D reference model, depth map of the visible images and 6-DoF pose of all images. We extensively evaluated the performance of state-of-the-art algorithms on our dataset and provided a comprehensive analysis of the results. We will publish our dataset and pre-processing code.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {cross-modality matching,low-light environment,thermal and visible dataset},
  file = {C:\Users\theun\Zotero\storage\PB96XWQ5\Liu et al. - 2023 - A Multi-View Thermal–Visible Image Dataset for Cross-Spectral Matching.pdf}
}

@article{liuRealtimeSemanticSegmentation2023,
  title = {A Real-Time Semantic Segmentation Method for End-to-End Autonomous Driving in Low-Light Environments},
  author = {Liu, Y. and Yi, F. and Ma, Y. and Wang, Y.},
  year = 2023,
  journal = {Journal of Intelligent and Fuzzy Systems},
  volume = {45},
  number = {6},
  pages = {9223--9237},
  publisher = {IOS Press BV},
  issn = {10641246 (ISSN)},
  doi = {10.3233/JIFS-230643},
  abstract = {To solve the problem that the existing semantic segmentation method is challenging to balance the two indicators of reasoning speed and accuracy under low-light conditions, a lightweight real-time semantic segmentation method that can run on embedded vehicle terminals under low-light conditions, namely, LCG-BiSeNetV2 is proposed. Firstly, the Zero-DCE++ method is introduced to preprocess the low-light video frames; Secondly, by improving the module of the BiSeNetV2 method and optimizing the downsampling convolution in the encoder structure, the semantic feature extraction ability of the BiSeNetV2 method has been significantly improved; Finally, the self-made road scene semantic segmentation dataset and the CAMVID dataset were used for training and testing, the mIoU coefficient reaches 72.37\% and 76.73\%, and the reasoning speed reaches 148.91 Frames Per Second (FPS) and 136.75 FPS, which shows that the LCG-BiSeNetV2 method has achieved fast reasoning and precise semantic segmentation.  \copyright{} 2023 - IOS Press. All rights reserved.},
  langid = {english},
  keywords = {/unread,Ability testing,Autonomous driving,autonomous driving system,Autonomous driving system,Autonomous vehicles,Driving systems,End to end,End-to-end,image enhancement,Image enhancement,Low light,Low light conditions,Real- time,real-time,Real-time semantics,Segmentation methods,semantic segmentation,Semantic segmentation,Semantic Segmentation,Semantics,Statistical tests}
}

@article{liuRedefiningAccuracyUnderwater2024,
  title = {Redefining {{Accuracy}}: {{Underwater Depth Estimation}} for {{Irregular Illumination Scenes}}},
  shorttitle = {Redefining {{Accuracy}}},
  author = {Liu, Tong and Zhang, Sainan and Yu, Zhibin},
  year = 2024,
  month = jul,
  journal = {Sensors (Basel, Switzerland)},
  volume = {24},
  number = {13},
  pages = {4353},
  issn = {1424-8220},
  doi = {10.3390/s24134353},
  urldate = {2025-06-30},
  abstract = {Acquiring underwater depth maps is essential as they provide indispensable three-dimensional spatial information for visualizing the underwater environment. These depth maps serve various purposes, including underwater navigation, environmental monitoring, and resource exploration. While most of the current depth estimation methods can work well in ideal underwater environments with homogeneous illumination, few consider the risk caused by irregular illumination, which is common in practical underwater environments. On the one hand, underwater environments with low-light conditions can reduce image contrast. The reduction brings challenges to depth estimation models in accurately differentiating among objects. On the other hand, overexposure caused by reflection or artificial illumination can degrade the textures of underwater objects, which is crucial to geometric constraints between frames. To address the above issues, we propose an underwater self-supervised monocular depth estimation network integrating image enhancement and auxiliary depth information. We use the Monte Carlo image enhancement module (MC-IEM) to tackle the inherent uncertainty in low-light underwater images through probabilistic estimation. When pixel values are enhanced, object recognition becomes more accessible, allowing for a more precise acquisition of distance information and thus resulting in more accurate depth estimation. Next, we extract additional geometric features through transfer learning, infusing prior knowledge from a supervised large-scale model into a self-supervised depth estimation network to refine loss functions and a depth network to address the overexposure issue. We conduct experiments with two public datasets, which exhibited superior performance compared to existing approaches in underwater depth estimation.},
  pmcid = {PMC11244248},
  pmid = {39001132},
  file = {C:\Users\theun\Zotero\storage\3YT6YV9B\Liu et al. - 2024 - Redefining Accuracy Underwater Depth Estimation for Irregular Illumination Scenes.pdf}
}

@inproceedings{liuRetinexinspiredUnrollingCooperative2021,
  title = {Retinex-Inspired {{Unrolling}} with {{Cooperative Prior Architecture Search}} for {{Low-light Image Enhancement}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Liu, Risheng and Ma, Long and Zhang, Jiaao and Fan, Xin and Luo, Zhongxuan},
  year = 2021,
  month = jun,
  pages = {10556--10565},
  publisher = {IEEE},
  address = {Nashville, TN, USA},
  doi = {10.1109/CVPR46437.2021.01042},
  urldate = {2025-02-27},
  abstract = {Low-light image enhancement plays very important roles in low-level vision areas. Recent works have built a great deal of deep learning models to address this task. However, these approaches mostly rely on significant architecture engineering and suffer from high computational burden. In this paper, we propose a new method, named Retinexinspired Unrolling with Architecture Search (RUAS), to construct lightweight yet effective enhancement network for low-light images in real-world scenario. Specifically, building upon Retinex rule, RUAS first establishes models to characterize the intrinsic underexposed structure of lowlight images and unroll their optimization processes to construct our holistic propagation structure. Then by designing a cooperative reference-free learning strategy to discover low-light prior architectures from a compact search space, RUAS is able to obtain a top-performing image enhancement network, which is with fast speed and requires few computational resources. Extensive experiments verify the superiority of our RUAS framework against recently proposed state-of-the-art methods. The project page is available at http://dutmedia.org/RUAS/.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-6654-4509-2},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\5KHWC9Y8\Liu et al. - 2021 - Retinex-inspired Unrolling with Cooperative Prior Architecture Search for Low-light Image Enhancemen.pdf}
}

@inproceedings{liuSelfSupervisedMonocularDepth2021,
  title = {Self-{{Supervised Monocular Depth Estimation}} for {{All Day Images Using Domain Separation}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Liu, Lina and Song, Xibin and Wang, Mengmeng and Liu, Yong and Zhang, Liangjun},
  year = 2021,
  pages = {12737--12746},
  urldate = {2025-06-30},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\YRSC8Z8B\Liu et al. - 2021 - Self-Supervised Monocular Depth Estimation for All Day Images Using Domain Separation.pdf}
}

@incollection{liuSSDSingleShot2016,
  title = {{{SSD}}: {{Single Shot MultiBox Detector}}},
  shorttitle = {{{SSD}}},
  author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  year = 2016,
  volume = {9905},
  eprint = {1512.02325},
  primaryclass = {cs},
  pages = {21--37},
  doi = {10.1007/978-3-319-46448-0_2},
  urldate = {2025-01-03},
  abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300 \texttimes{} 300 input, SSD achieves 74.3\% mAP1 on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512 \texttimes{} 512 input, SSD achieves 76.9\% mAP, outperforming a comparable state-of-the-art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at: https://github.com/weiliu89/caffe/tree/ssd .},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {/unread,Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\theun\Zotero\storage\LXX4BJJK\Liu et al. - 2016 - SSD Single Shot MultiBox Detector.pdf}
}

@inproceedings{liuSwinTransformerHierarchical2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer Using Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = 2021,
  pages = {10012--10022},
  urldate = {2025-06-20},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\EHCYP3CN\Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer Using Shifted Windows.pdf}
}

@inproceedings{liuUnsupervisedImagetoImageTranslation2017,
  title = {Unsupervised {{Image-to-Image Translation Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Liu, Ming-Yu and Breuel, Thomas and Kautz, Jan},
  year = 2017,
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-06-19},
  abstract = {Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in https://github.com/mingyuliutw/unit.},
  file = {C:\Users\theun\Zotero\storage\L495FX68\Liu et al. - 2017 - Unsupervised Image-to-Image Translation Networks.pdf}
}

@inproceedings{liuYOLOStereo3DStepBack2021,
  title = {{{YOLOStereo3D}}: {{A Step Back}} to {{2D}} for {{Efficient Stereo 3D Detection}}},
  shorttitle = {{{YOLOStereo3D}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Liu, Yuxuan and Wang, Lujia and Liu, Ming},
  year = 2021,
  month = may,
  pages = {13018--13024},
  issn = {2577-087X},
  doi = {10.1109/ICRA48506.2021.9561423},
  urldate = {2025-03-24},
  abstract = {Object detection in 3D with stereo cameras is an important problem in computer vision, and is particularly crucial in low-cost autonomous mobile robots without LiDARs. Nowadays, most of the best-performing frameworks for stereo 3D object detection are based on dense depth reconstruction from disparity estimation, making them extremely computationally expensive. To enable real-world deployments of vision detection with binocular images, we take a step back to gain insights from 2D image-based detection frameworks and enhance them with stereo features. We incorporate knowledge and the inference structure from real-time one-stage 2D/3D object detector and introduce a light-weight stereo matching module. Our proposed framework, YOLOStereo3D, is trained on one single GPU and runs at more than ten fps. It demonstrates performance comparable to state-of-the-art stereo 3D detection frameworks without usage of LiDAR data. The code will be published in https://github.com/Owen-Liuyuxuan/visualDet3D.},
  keywords = {3D object,Autonomous Mobile Robot,Computational modeling,Computer vision,Costs,Depth reconstruction,Detection framework,Disparity estimations,Gain insight,Graphics processing units,Image enhancement,Laser radar,Low-costs,Object detection,Object recognition,Objects detection,Optical radar,Real world deployment,Robot vision systems,Stereo cameras,Stereo image processing,Stereo vision,Three-dimensional displays,Training},
  file = {C\:\\Users\\theun\\Zotero\\storage\\XMYJ9QKI\\Liu et al. - 2021 - YOLOStereo3D A Step Back to 2D for Efficient Stereo 3D Detection.pdf;C\:\\Users\\theun\\Zotero\\storage\\IT3HW438\\9561423.html}
}

@article{lohGettingKnowLowlight2019,
  title = {Getting to Know Low-Light Images with the {{Exclusively Dark}} Dataset},
  author = {Loh, Yuen Peng and Chan, Chee Seng},
  year = 2019,
  month = jan,
  journal = {Computer Vision and Image Understanding},
  volume = {178},
  pages = {30--42},
  issn = {1077-3142},
  doi = {10.1016/j.cviu.2018.10.010},
  urldate = {2025-03-10},
  abstract = {Low-light is an inescapable element of our daily surroundings that greatly affects the efficiency of our vision. Research works on low-light imagery have seen a steady growth, particularly in the field of image enhancement, but there is still a lack of a go-to database as a benchmark. Besides, research fields that may assist us in low-light environments, such as object detection, has glossed over this aspect even though breakthroughs-after-breakthroughs had been achieved in recent years, most noticeably from the lack of low-light data (less than 2\% of the total images) in successful public benchmark datasets such as PASCAL VOC, ImageNet, and Microsoft COCO. Thus, we propose the Exclusively Dark dataset to elevate this data drought. It consists exclusively of low-light images captured in visible light only, with image and object level annotations. Moreover, we share insightful findings in regards to the effects of low-light on the object detection task by analyzing the visualizations of both hand-crafted and learned features. We found that the effects of low-light reach far deeper into the features than can be solved by simple ``illumination invariance''. It is our hope that this analysis and the Exclusively Dark dataset can encourage the growth in low-light domain researches on different fields. The dataset can be downloaded at https://github.com/cs-chan/Exclusively-Dark-Image-Dataset.},
  file = {C\:\\Users\\theun\\Zotero\\storage\\7CN2ZETW\\Loh and Chan - 2019 - Getting to know low-light images with the Exclusively Dark dataset.pdf;C\:\\Users\\theun\\Zotero\\storage\\SMHYV4RQ\\S1077314218304296.html}
}

@article{lohLowlightImageEnhancement2019,
  title = {Low-Light Image Enhancement Using {{Gaussian Process}} for Features Retrieval},
  author = {Loh, Yuen Peng and Liang, Xuefeng and Chan, Chee Seng},
  year = 2019,
  month = may,
  journal = {Signal Processing: Image Communication},
  volume = {74},
  pages = {175--190},
  issn = {0923-5965},
  doi = {10.1016/j.image.2019.02.001},
  urldate = {2025-03-10},
  abstract = {Low-light is a challenging environment for image processing and computer vision tasks, either in contrast enhancement for better visibility and quality, or application oriented tasks such as detection. We found that the current trend of low-light enhancement research is heavily on quality improvement. In this work, we aim to shift the focus towards a more functional direction, that is enhancement that prioritizes feature retrieval. For this reason, we first propose to model low-light enhancement as a set of localized functions using Gaussian Process (GP) that is trained at runtime using data from a simple Convolutional Neural Network (CNN) to provide the necessary feature information as reference. The CNN is in turn trained using large amount of synthetic data, based upon the luminance distribution of real world low-light images to learn the relationship between features and pixels. Secondly, we also proposed two new evaluation metrics to better assess enhancement algorithms to support high level computer vision tasks, namely, local features matching and intensity histogram similarity. In our experiments, our proposed low-light enhancement framework outperforms the state-of-the-arts with significant improvement in Recall, F1, and F2-score of SIFT features matching, and achieve comparable results for l1-norm distance of histograms as well as the PSNR. Moreover, our analysis of the performance showed that the PSNR quality metric is not only unable to assess the practicality of the results, but also inappropriately gives high assessment to low visual quality images.},
  keywords = {Convolutional neural network,Gaussian Process,Image enhancement,Low-light},
  file = {C\:\\Users\\theun\\Zotero\\storage\\UW37JYF9\\Loh et al. - 2019 - Low-light image enhancement using Gaussian Process for features retrieval.pdf;C\:\\Users\\theun\\Zotero\\storage\\QH5R8ELW\\S0923596518310452.html}
}

@inproceedings{longFullyConvolutionalNetworks2015,
  title = {Fully {{Convolutional Networks}} for {{Semantic Segmentation}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  year = 2015,
  pages = {3431--3440},
  urldate = {2025-06-03},
  file = {C:\Users\theun\Zotero\storage\A7BV7ISI\Long et al. - 2015 - Fully Convolutional Networks for Semantic Segmentation.pdf}
}

@article{loreLLNetDeepAutoencoder2017,
  title = {{{LLNet}}: {{A}} Deep Autoencoder Approach to Natural Low-Light Image Enhancement},
  shorttitle = {{{LLNet}}},
  author = {Lore, Kin Gwn and Akintayo, Adedotun and Sarkar, Soumik},
  year = 2017,
  month = jan,
  journal = {Pattern Recognition},
  volume = {61},
  pages = {650--662},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2016.06.008},
  urldate = {2025-02-27},
  abstract = {In surveillance, monitoring and tactical reconnaissance, gathering visual information from a dynamic environment and accurately processing such data are essential to making informed decisions and ensuring the success of a mission. Camera sensors are often cost-limited to capture clear images or videos taken in a poorly-lit environment. Many applications aim to enhance brightness, contrast and reduce noise content from the images in an on-board real-time manner. We propose a deep autoencoder-based approach to identify signal features from low-light images and adaptively brighten images without over-amplifying/saturating the lighter parts in images with a high dynamic range. We show that a variant of the stacked-sparse denoising autoencoder can learn from synthetically darkened and noise-added training examples to adaptively enhance images taken from natural low-light environment and/or are hardware-degraded. Results show significant credibility of the approach both visually and by quantitative comparison with various techniques.},
  keywords = {/unread,Deep autoencoders,Image enhancement,Natural low-light images},
  file = {C\:\\Users\\theun\\Zotero\\storage\\4Q4JPL86\\Lore et al. - 2017 - LLNet A deep autoencoder approach to natural low-light image enhancement.pdf;C\:\\Users\\theun\\Zotero\\storage\\QDFLW2FU\\S003132031630125X.html}
}

@article{lotfyImprovingPerformanceGNSS2022,
  title = {Improving the Performance of {{GNSS}} Precise Point Positioning by Developed Robust Adaptive {{Kalman}} Filter},
  author = {Lotfy, Ahmed and Abdelfatah, Mohamed and {El-Fiky}, Gamal},
  year = 2022,
  month = dec,
  journal = {The Egyptian Journal of Remote Sensing and Space Science},
  volume = {25},
  number = {4},
  pages = {919--928},
  issn = {11109823},
  doi = {10.1016/j.ejrs.2022.09.005},
  urldate = {2025-01-02},
  abstract = {Global Navigation Satellite Systems (GNSS) Precise Point Positioning (PPP) is a great precision positioning method based on GNSS. PPP based on multi-constellation GNSS uses the extended Kalman filter (EKF). Inappropriately, the measurement outliers and the system's dynamic model might produce mistakes in the positioning accuracy acquired using this method. An adaptive robust Kalman filter (RKF) was recently developed in order to alleviate these errors. In addition, the preceding variance is sometimes used to determine the weights of different categories of observations, which has an effect on PPP performance. A novel developed robust adaptive filtering (DRKF) has been developed to provide better placement results. For the equivalent weight matrix to be generated, an equivalent weight model that is statistically robust by the chi-square test for classification is constructed in this technique. According to prior Kalman filter models employing International GNSS Service (IGS) final clock and orbit products, the performance of GNSS PPP using the DRKF is confirmed in contrast to those produced using the current model. DRKF surpasses traditional robust adaptive filters in terms of positioning accuracy, convergence time, and PPP sturdiness according to the results of this study. In comparison to RKF, DRKF improves the positioning solution by 201 mm with 119.10 \%, 77 mm with 164.86 \%, and 160 mm with 211.74 \% in the east, north, and up (ENZ) directions, respectively. It is recommended to use the newly developed robust adaptive Kalman filter in kinematic mode, especially in urban areas, due to the noticeable improvement in position accuracy.},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\SPBLNZ88\Lotfy et al. - 2022 - Improving the performance of GNSS precise point positioning by developed robust adaptive Kalman filt.pdf}
}

@inproceedings{loweObjectRecognitionLocal1999,
  title = {Object Recognition from Local Scale-Invariant Features},
  booktitle = {Proceedings of the {{Seventh IEEE International Conference}} on {{Computer Vision}}},
  author = {Lowe, D.G.},
  year = 1999,
  pages = {1150-1157 vol.2},
  publisher = {IEEE},
  address = {Kerkyra, Greece},
  doi = {10.1109/ICCV.1999.790410},
  urldate = {2025-05-29},
  isbn = {978-0-7695-0164-2},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\2I684GDT\Lowe - 1999 - Object recognition from local scale-invariant features.pdf}
}

@misc{LowLightImageEnhancement,
  title = {Low-{{Light Image Enhancement}} via the {{Absorption Light Scattering Model}} \textbar{} {{IEEE Journals}} \& {{Magazine}} \textbar{} {{IEEE Xplore}}},
  urldate = {2025-02-19},
  howpublished = {https://ieeexplore-ieee-org.ezproxy-b.deakin.edu.au/abstract/document/8737871},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\ICPICM2G\8737871.html}
}

@article{lozaAutomaticContrastEnhancement2013,
  title = {Automatic Contrast Enhancement of Low-Light Images Based on Local Statistics of Wavelet Coefficients},
  author = {{\L}oza, Artur and Bull, David R. and Hill, Paul R. and Achim, Alin M.},
  year = 2013,
  month = dec,
  journal = {Digital Signal Processing},
  volume = {23},
  number = {6},
  pages = {1856--1866},
  issn = {1051-2004},
  doi = {10.1016/j.dsp.2013.06.002},
  urldate = {2025-05-29},
  abstract = {This paper describes a new method for contrast enhancement in images and image sequences of low-light or unevenly illuminated scenes based on statistical modelling of wavelet coefficients of the image. A non-linear enhancement function has been designed based on the local dispersion of the wavelet coefficients modelled as a bivariate Cauchy distribution. Within the same statistical framework, a simultaneous noise reduction in the image is performed by means of a shrinkage function, thus preventing noise amplification. The proposed enhancement method has been shown to perform very well with insufficiently illuminated and noisy imagery, outperforming other conventional methods, in terms of contrast enhancement and noise reduction in the output data.},
  keywords = {Denoising,Image and video contrast enhancement,Statistical modelling,Wavelets},
  file = {C\:\\Users\\theun\\Zotero\\storage\\75BIARY2\\Łoza et al. - 2013 - Automatic contrast enhancement of low-light images based on local statistics of wavelet coefficients.pdf;C\:\\Users\\theun\\Zotero\\storage\\5PQ9V8QX\\S1051200413001309.html}
}

@inproceedings{luoSimilarityMinMaxZeroShot2023,
  title = {Similarity {{Min-Max}}: {{Zero-Shot Day-Night Domain Adaptation}}},
  shorttitle = {Similarity {{Min-Max}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Luo, Rundong and Wang, Wenjing and Yang, Wenhan and Liu, Jiaying},
  year = 2023,
  pages = {8104--8114},
  urldate = {2025-06-16},
  langid = {english},
  keywords = {Computer vision,Domain adaptation,Domain knowledge,Domain Knowledge,Down-stream,HTTP,Human visual,Low light conditions,Min-max,Model Adaptation,Modeling performance,Semantic Segmentation,Semantics,Unified framework,Visual experiences,Zero-shot learning},
  file = {C:\Users\theun\Zotero\storage\32WBGLFN\Luo et al. - 2023 - Similarity Min-Max Zero-Shot Day-Night Domain Adaptation.pdf}
}

@misc{lvAttentionGuidedLowlight2020,
  title = {Attention {{Guided Low-light Image Enhancement}} with a {{Large Scale Low-light Simulation Dataset}}},
  author = {Lv, Feifan and Li, Yu and Lu, Feng},
  year = 2020,
  month = mar,
  number = {arXiv:1908.00682},
  eprint = {1908.00682},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1908.00682},
  urldate = {2025-01-18},
  abstract = {Low-light image enhancement is challenging in that it needs to consider not only brightness recovery but also complex issues like color distortion and noise, which usually hide in the dark. Simply adjusting the brightness of a low-light image will inevitably amplify those artifacts. To address this difficult problem, this paper proposes a novel end-to-end attention-guided method based on multi-branch convolutional neural network. To this end, we first construct a synthetic dataset with carefully designed low-light simulation strategies. The dataset is much larger and more diverse than existing ones. With the new dataset for training, our method learns two attention maps to guide the brightness enhancement and denoising tasks respectively. The first attention map distinguishes underexposed regions from well lit regions, and the second attention map distinguishes noises from real textures. With their guidance, the proposed multi-branch decomposition-and-fusion enhancement network works in an input adaptive way. Moreover, a reinforcement-net further enhances color and contrast of the output image. Extensive experiments on multiple datasets demonstrate that our method can produce high fidelity enhancement results for low-light images and outperforms the current state-of-the-art methods by a large margin both quantitatively and visually.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C\:\\Users\\theun\\Zotero\\storage\\DGEPR65V\\Lv et al. - 2020 - Attention Guided Low-light Image Enhancement with a Large Scale Low-light Simulation Dataset.pdf;C\:\\Users\\theun\\Zotero\\storage\\XBVHDB3M\\1908.html}
}

@article{lvMBLLENLowlightImage2018,
  title = {{{MBLLEN}}: {{Low-light Image}}/{{Video Enhancement Using CNNs}}},
  author = {Lv, Feifan and Lu, Feng and Wu, Jianhua and Lim, Chongsoon},
  year = 2018,
  journal = {The British Machine Vision Conference (BMVC)},
  abstract = {We present a deep learning based method for low-light image enhancement. This problem is challenging due to the difficulty in handling various factors simultaneously including brightness, contrast, artifacts and noise. To address this task, we propose the multi-branch low-light enhancement network (MBLLEN). The key idea is to extract rich features up to different levels, so that we can apply enhancement via multiple subnets and finally produce the output image via multi-branch fusion. In this manner, image quality is improved from different aspects. Through extensive experiments, our proposed MBLLEN is found to outperform the state-of-art techniques by a large margin. We additionally show that our method can be directly extended to handle low-light videos.},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\JIUGG5XL\Lv - MBLLEN Low-light ImageVideo Enhancement Using CNNs.pdf}
}

@article{lyuHRDepthHighResolution2021,
  title = {{{HR-Depth}}: {{High Resolution Self-Supervised Monocular Depth Estimation}}},
  shorttitle = {{{HR-Depth}}},
  author = {Lyu, Xiaoyang and Liu, Liang and Wang, Mengmeng and Kong, Xin and Liu, Lina and Liu, Yong and Chen, Xinxin and Yuan, Yi},
  year = 2021,
  month = may,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {3},
  pages = {2294--2301},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v35i3.16329},
  urldate = {2025-07-03},
  abstract = {Self-supervised learning shows great potential in monocular depth estimation, using image sequences as the only source of supervision. Although people try to use high-resolution image for depth estimation, the accuracy of prediction has not been significantly improved. In this work, we find the core reason comes from the inaccurate depth estimation in large gradient regions, making the bilinear interpolation error gradually disappear as the resolution increases. To obtain more accurate depth estimation in large gradient regions, it is necessary to obtain high-resolution features with spatial and semantic information. Therefore, we present an improved DepthNet, HR-Depth, with two effective strategies: (1) redesign the skip-connection in DepthNet to get better highresolution features and (2) propose feature fusion Squeezeand-Excitation(fSE) module to fuse feature more efficiently. Using Resnet-18 as the encoder, HR-Depth surpasses all previous state-of-the-art(SoTA) methods with the least parameters at both high and low resolution. Moreover, previous SoTA methods are based on fairly complex and deep networks with many parameters which limits their real applications. Thus we also construct a lightweight network which uses MobileNetV3 as encoder. Experiments show that the lightweight network can perform on par with many large models like Monodepth2 at high-resolution with only 20\% parameters. All codes and models will be available at https: //github.com/shawLyu/HR-Depth.},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\5NFFWSCT\Lyu et al. - 2021 - HR-Depth High Resolution Self-Supervised Monocular Depth Estimation.pdf}
}

@misc{ma3DObjectDetection2023,
  title = {{{3D Object Detection}} from {{Images}} for {{Autonomous Driving}}: {{A Survey}}},
  shorttitle = {{{3D Object Detection}} from {{Images}} for {{Autonomous Driving}}},
  author = {Ma, Xinzhu and Ouyang, Wanli and Simonelli, Andrea and Ricci, Elisa},
  year = 2023,
  month = dec,
  number = {arXiv:2202.02980},
  eprint = {2202.02980},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.02980},
  urldate = {2025-01-03},
  abstract = {3D object detection from images, one of the fundamental and challenging problems in autonomous driving, has received increasing attention from both industry and academia in recent years. Benefiting from the rapid development of deep learning technologies, image-based 3D detection has achieved remarkable progress. Particularly, more than 200 works have studied this problem from 2015 to 2021, encompassing a broad spectrum of theories, algorithms, and applications. However, to date no recent survey exists to collect and organize this knowledge. In this paper, we fill this gap in the literature and provide the first comprehensive survey of this novel and continuously growing research field, summarizing the most commonly used pipelines for image-based 3D detection and deeply analyzing each of their components. Additionally, we also propose two new taxonomies to organize the state-of-the-art methods into different categories, with the intent of providing a more systematic review of existing methods and facilitating fair comparisons with future works. In retrospect of what has been achieved so far, we also analyze the current challenges in the field and discuss future directions for image-based 3D detection research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\5LA75MV2\\Ma et al. - 2023 - 3D Object Detection from Images for Autonomous Driving A Survey.pdf;C\:\\Users\\theun\\Zotero\\storage\\ZKXDYQ95\\2202.html}
}

@misc{maadaadataMaadaadataLowlightingDashCamVideoDataset2024,
  title = {Maadaadata/{{Low-lighting-Dash-Cam-Video-Dataset}}},
  author = {Maadaadata},
  year = 2024,
  month = aug,
  urldate = {2025-01-18},
  abstract = {This dataset focuses on low lighting scenarios across various settings such as crossroads, avenues, and paths. It encompasses bounding boxes and tags for common urban objects like humans, cars, electric bicycles, vans, and trucks, providing a comprehensive view of the challenges faced by autonomous vehicles in reduced visibility.},
  keywords = {/unread,autonomous-driving,computer-vision,dashcam}
}

@article{maddern1Year10002017,
  title = {1 Year, 1000 Km: {{The Oxford RobotCar}} Dataset},
  shorttitle = {1 Year, 1000 Km},
  author = {Maddern, Will and Pascoe, Geoffrey and Linegar, Chris and Newman, Paul},
  year = 2017,
  month = jan,
  journal = {The International Journal of Robotics Research},
  volume = {36},
  number = {1},
  pages = {3--15},
  publisher = {SAGE Publications Ltd STM},
  issn = {0278-3649},
  doi = {10.1177/0278364916679498},
  urldate = {2025-05-15},
  abstract = {We present a challenging new dataset for autonomous driving: the Oxford RobotCar Dataset. Over the period of May 2014 to December 2015 we traversed a route through central Oxford twice a week on average using the Oxford RobotCar platform, an autonomous Nissan LEAF. This resulted in over 1000 km of recorded driving with almost 20 million images collected from 6 cameras mounted to the vehicle, along with LIDAR, GPS and INS ground truth. Data was collected in all weather conditions, including heavy rain, night, direct sunlight and snow. Road and building works over the period of a year significantly changed sections of the route from the beginning to the end of data collection. By frequently traversing the same route over the period of a year we enable research investigating long-term localization and mapping for autonomous vehicles in real-world, dynamic urban environments. The full dataset is available for download at: http://robotcar-dataset.robots.ox.ac.uk},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\BB42272P\Maddern et al. - 2017 - 1 year, 1000 km The Oxford RobotCar dataset.pdf}
}

@inproceedings{maFastFlexibleRobust2022,
  title = {Toward {{Fast}}, {{Flexible}}, and {{Robust Low-Light Image Enhancement}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Ma, Long and Ma, Tengyu and Liu, Risheng and Fan, Xin and Luo, Zhongxuan},
  year = 2022,
  month = jun,
  pages = {5627--5636},
  publisher = {IEEE},
  address = {New Orleans, LA, USA},
  doi = {10.1109/CVPR52688.2022.00555},
  urldate = {2025-03-10},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-6946-3},
  langid = {english},
  file = {C\:\\Users\\theun\\Zotero\\storage\\AHYA7WGF\\04550-supp.pdf;C\:\\Users\\theun\\Zotero\\storage\\CJHEW9WM\\Ma et al. - 2022 - Toward Fast, Flexible, and Robust Low-Light Image Enhancement.pdf}
}

@article{mahadevkarReviewMachineLearning2022,
  title = {A {{Review}} on {{Machine Learning Styles}} in {{Computer Vision}}---{{Techniques}} and {{Future Directions}}},
  author = {Mahadevkar, Supriya V. and Khemani, Bharti and Patil, Shruti and Kotecha, Ketan and Vora, Deepali R. and Abraham, Ajith and Gabralla, Lubna Abdelkareim},
  year = 2022,
  journal = {IEEE Access},
  volume = {10},
  pages = {107293--107329},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3209825},
  urldate = {2025-01-16},
  abstract = {Computer applications have considerably shifted from single data processing to machine learning in recent years due to the accessibility and availability of massive volumes of data obtained through the internet and various sources. Machine learning is automating human assistance by training an algorithm on relevant data. Supervised, Unsupervised, and Reinforcement Learning are the three fundamental categories of machine learning techniques. In this paper, we have discussed the different learning styles used in the field of Computer vision, Deep Learning, Neural networks, and machine learning. Some of the most recent applications of machine learning in computer vision include object identification, object classification, and extracting usable information from images, graphic documents, and videos. Some machine learning techniques frequently include zero-shot learning, active learning, contrastive learning, self-supervised learning, life-long learning, semi-supervised learning, ensemble learning, sequential learning, and multi-view learning used in computer vision until now. There is a lack of systematic reviews about all learning styles. This paper presents literature analysis of how different machine learning styles evolved in the field of Artificial Intelligence (AI) for computer vision. This research examines and evaluates machine learning applications in computer vision and future forecasting. This paper will be helpful for researchers working with learning styles as it gives a deep insight into future directions.},
  keywords = {/unread,artificial intelligence,Artificial intelligence,computer vision,Computer vision,Feature extraction,image categorization,Image segmentation,Machine learning,Machine learning algorithms,Machine learning techniques,multi-task learning,object detection,Object detection,supervised learning,zero-shot learning},
  file = {C\:\\Users\\theun\\Zotero\\storage\\RAHJC23N\\Mahadevkar et al. - 2022 - A Review on Machine Learning Styles in Computer Vision—Techniques and Future Directions.pdf;C\:\\Users\\theun\\Zotero\\storage\\ZM7AGXJ9\\9903420.html}
}

@article{maLowLightImageEnhancement2023,
  title = {Low-{{Light Image Enhancement}} via {{Self-Reinforced Retinex Projection Model}}},
  author = {Ma, Long and Liu, Risheng and Wang, Yiyang and Fan, Xin and Luo, Zhongxuan},
  year = 2023,
  journal = {IEEE Transactions on Multimedia},
  volume = {25},
  pages = {3573--3586},
  issn = {1941-0077},
  doi = {10.1109/TMM.2022.3162493},
  urldate = {2025-02-27},
  abstract = {Low-light image enhancement aims to improve the quality of images captured under low-lightening conditions, which is a fundamental problem in computer vision and multimedia areas. Although many efforts have been invested over the years, existing illumination-based models tend to generate unnatural-looking results (e.g., over-exposure). It is because that the widely-adopted illumination adjustment (e.g., Gamma Correction) breaks down the favorable smoothness property of the original illumination derived from the well-designed illumination estimation model. To settle this issue, a great-efficiency and high-quality Self-Reinforced Retinex Projection (SRRP) model is developed in this paper, which contains optimization modules of both illumination and reflectance layers. Specifically, we construct a new fidelity term with the self-reinforced function for the illumination optimization to eliminate the dependence of the illumination adjustment to obtain a desired illumination with the excellent smoothing property. By introducing a flexible feasible constraint, we obtain a reflectance optimization module with projection. Owing to its flexibility, we can extend our model to an enhanced version by integrating a data-driven denoising mechanism as the projection, which is able to effectively handle the generated noises/artifacts in the enhanced procedure. In the experimental part, on one side, we make ample comparative assessments on multiple benchmarks with considerable state-of-the-art methods. These evaluations fully verify the outstanding performance of our method, in terms of the qualitative and quantitative analyses and execution efficiency. On the other side, we also conduct extensive analytical experiments to indicate the effectiveness and advantages of our proposed model.},
  keywords = {Estimation,illumination estimation,Image color analysis,image denoising,Image edge detection,Image enhancement,Lighting,Low-light image enhancement,Optimization,retinex model,Smoothing methods},
  file = {C\:\\Users\\theun\\Zotero\\storage\\3I6D4N28\\Ma et al. - 2023 - Low-Light Image Enhancement via Self-Reinforced Retinex Projection Model.pdf;C\:\\Users\\theun\\Zotero\\storage\\BWYDW4A2\\9743313.html}
}

@misc{maoLeastSquaresGenerative2017,
  title = {Least {{Squares Generative Adversarial Networks}}},
  author = {Mao, Xudong and Li, Qing and Xie, Haoran and Lau, Raymond Y. K. and Wang, Zhen and Smolley, Stephen Paul},
  year = 2017,
  month = apr,
  number = {arXiv:1611.04076},
  eprint = {1611.04076},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1611.04076},
  urldate = {2025-06-03},
  abstract = {Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson \$\textbackslash chi\textasciicircum 2\$ divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\ZMK2P2I3\\Mao et al. - 2017 - Least Squares Generative Adversarial Networks.pdf;C\:\\Users\\theun\\Zotero\\storage\\UECHDQ9V\\1611.html}
}

@misc{maoOneMillionScenes2021,
  title = {One {{Million Scenes}} for {{Autonomous Driving}}: {{ONCE Dataset}}},
  shorttitle = {One {{Million Scenes}} for {{Autonomous Driving}}},
  author = {Mao, Jiageng and Niu, Minzhe and Jiang, Chenhan and Liang, Hanxue and Chen, Jingheng and Liang, Xiaodan and Li, Yamin and Ye, Chaoqiang and Zhang, Wei and Li, Zhenguo and Yu, Jie and Xu, Hang and Xu, Chunjing},
  year = 2021,
  month = oct,
  number = {arXiv:2106.11037},
  eprint = {2106.11037},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.11037},
  urldate = {2025-01-18},
  abstract = {Current perception models in autonomous driving have become notorious for greatly relying on a mass of annotated data to cover unseen cases and address the long-tail problem. On the other hand, learning from unlabeled large-scale collected data and incrementally self-training powerful recognition models have received increasing attention and may become the solutions of next-generation industry-level powerful and robust perception models in autonomous driving. However, the research community generally suffered from data inadequacy of those essential real-world scene data, which hampers the future exploration of fully/semi/self-supervised methods for 3D perception. In this paper, we introduce the ONCE (One millioN sCenEs) dataset for 3D object detection in the autonomous driving scenario. The ONCE dataset consists of 1 million LiDAR scenes and 7 million corresponding camera images. The data is selected from 144 driving hours, which is 20x longer than the largest 3D autonomous driving dataset available (e.g. nuScenes and Waymo), and it is collected across a range of different areas, periods and weather conditions. To facilitate future research on exploiting unlabeled data for 3D detection, we additionally provide a benchmark in which we reproduce and evaluate a variety of self-supervised and semi-supervised methods on the ONCE dataset. We conduct extensive analyses on those methods and provide valuable observations on their performance related to the scale of used data. Data, code, and more information are available at https://once-for-auto-driving.github.io/index.html.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\AHGXDNJ3\\Mao et al. - 2021 - One Million Scenes for Autonomous Driving ONCE Dataset.pdf;C\:\\Users\\theun\\Zotero\\storage\\RNY8L5IY\\2106.html}
}

@article{maPerceptualQualityAssessment2015,
  title = {Perceptual {{Quality Assessment}} for {{Multi-Exposure Image Fusion}}},
  author = {Ma, Kede and Zeng, Kai and Wang, Zhou},
  year = 2015,
  month = nov,
  journal = {IEEE Transactions on Image Processing},
  volume = {24},
  number = {11},
  pages = {3345--3356},
  issn = {1941-0042},
  doi = {10.1109/TIP.2015.2442920},
  urldate = {2025-03-11},
  abstract = {Multi-exposure image fusion (MEF) is considered an effective quality enhancement technique widely adopted in consumer electronics, but little work has been dedicated to the perceptual quality assessment of multi-exposure fused images. In this paper, we first build an MEF database and carry out a subjective user study to evaluate the quality of images generated by different MEF algorithms. There are several useful findings. First, considerable agreement has been observed among human subjects on the quality of MEF images. Second, no single state-of-the-art MEF algorithm produces the best quality for all test images. Third, the existing objective quality models for general image fusion are very limited in predicting perceived quality of MEF images. Motivated by the lack of appropriate objective models, we propose a novel objective image quality assessment (IQA) algorithm for MEF images based on the principle of the structural similarity approach and a novel measure of patch structural consistency. Our experimental results on the subjective database show that the proposed model well correlates with subjective judgments and significantly outperforms the existing IQA models for general image fusion. Finally, we demonstrate the potential application of the proposed model by automatically tuning the parameters of MEF algorithms.1},
  keywords = {/unread,Computational modeling,Databases,Image edge detection,Image fusion,image quality assessment,Image sequences,luminance consistency,Multi-exposure image fusion (MEF),perceptual image processing,Prediction algorithms,Quality assessment,structural similarity,subjective evaluations},
  file = {C\:\\Users\\theun\\Zotero\\storage\\JTLQPU54\\Ma et al. - 2015 - Perceptual Quality Assessment for Multi-Exposure Image Fusion.pdf;C\:\\Users\\theun\\Zotero\\storage\\LHT5NAXD\\7120119.html}
}

@inproceedings{maPIAParallelArchitecture2022,
  title = {{{PIA}}: {{Parallel Architecture}} with {{Illumination Allocator}} for {{Joint Enhancement}} and {{Detection}} in {{Low-Light}}},
  shorttitle = {{{PIA}}},
  booktitle = {Proceedings of the 30th {{ACM International Conference}} on {{Multimedia}}},
  author = {Ma, Tengyu and Ma, Long and Fan, Xin and Luo, Zhongxuan and Liu, Risheng},
  year = 2022,
  month = oct,
  series = {{{MM}} '22},
  pages = {2070--2078},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3503161.3548041},
  urldate = {2025-02-26},
  abstract = {Visual perception in low-light conditions (e.g., nighttime) plays an important role in various multimedia-related applications (e.g., autonomous driving). The enhancement (provides a visual-friendly appearance) and detection (detects the instances of objects) in low-light are two fundamental and crucial visual perception tasks. In this paper, we make efforts on how to simultaneously realize low-light enhancement and detection from two aspects. First, we define a parallel architecture to satisfy the task demand for both two tasks. In which, a decomposition-type warm-start acting on the entrance of parallel architecture is developed to narrow down the adverse effects brought by low-light scenes to some extent. Second, a novel illumination allocator is designed by encoding the key illumination component (the inherent difference between normal-light and low-light) to extract hierarchical features for assisting in enhancement and detection. Further, we make a substantive discussion for our proposed method. That is, we solve enhancement in a coarse-to-fine manner and handle detection in a decomposed-to-integrated fashion. Finally, multidimensional analytical and evaluated experiments are performed to indicate our effectiveness and superiority. The code is available at \textbackslash urlhttps://github.com/tengyu1998/PIA},
  isbn = {978-1-4503-9203-7},
  file = {C:\Users\theun\Zotero\storage\5NF3TG9I\Ma et al. - 2022 - PIA Parallel Architecture with Illumination Allocator for Joint Enhancement and Detection in Low-Li.pdf}
}

@article{mariniComputationalApproachColor2000,
  title = {A Computational Approach to Color Adaptation Effects},
  author = {Marini, D. and Rizzi, A.},
  year = 2000,
  month = oct,
  journal = {Image and Vision Computing},
  volume = {18},
  number = {13},
  pages = {1005--1014},
  issn = {0262-8856},
  doi = {10.1016/S0262-8856(00)00037-8},
  urldate = {2025-05-29},
  abstract = {The human vision system has adaptation mechanisms that cannot be managed with the classic tri-stimulus color theory. The effects of these mechanisms are clearly visible in some well-known perception phenomena as color illusions, but they are always present in human observation. The discrepancy between the observation of a real scene and the observation of a picture taken from the same scene, derives from the fact that the camera does not have such mechanisms. In this paper, we propose a biologically inspired implementation of the Retinex algorithm, introduced by Land and McCann, that simulates these adaptation mechanisms, in order to reproduce some effects like dynamic adjustment, color constancy, etc. typical of the human vision system. The algorithm has been tested not only on a Mondrian-like patchwork to measure its effect, but also on different pictures, photographs and typical color illusions to test its adaptation effects. The examples demonstrate the ability of the model to emulate some characteristics of human color perception and to obtain better equalized and color-corrected images.},
  keywords = {Color appearance,Color constancy,Color illusions,Color perception,Retinex},
  file = {C\:\\Users\\theun\\Zotero\\storage\\YR6SFHG6\\Marini and Rizzi - 2000 - A computational approach to color adaptation effects.pdf;C\:\\Users\\theun\\Zotero\\storage\\8TP8RT95\\S0262885600000378.html}
}

@misc{masoumianGCNDepthSelfsupervisedMonocular2021,
  title = {{{GCNDepth}}: {{Self-supervised Monocular Depth Estimation}} Based on {{Graph Convolutional Network}}},
  shorttitle = {{{GCNDepth}}},
  author = {Masoumian, Armin and Rashwan, Hatem A. and Abdulwahab, Saddam and Cristiano, Julian and Puig, Domenec},
  year = 2021,
  month = dec,
  number = {arXiv:2112.06782},
  eprint = {2112.06782},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.06782},
  urldate = {2025-07-02},
  abstract = {Depth estimation is a challenging task of 3D reconstruction to enhance the accuracy sensing of environment awareness. This work brings a new solution with a set of improvements, which increase the quantitative and qualitative understanding of depth maps compared to existing methods. Recently, a convolutional neural network (CNN) has demonstrated its extraordinary ability in estimating depth maps from monocular videos. However, traditional CNN does not support topological structure and they can work only on regular image regions with determined size and weights. On the other hand, graph convolutional networks (GCN) can handle the convolution on non-Euclidean data and it can be applied to irregular image regions within a topological structure. Therefore, in this work in order to preserve object geometric appearances and distributions, we aim at exploiting GCN for a self-supervised depth estimation model. Our model consists of two parallel auto-encoder networks: the first is an auto-encoder that will depend on ResNet-50 and extract the feature from the input image and on multi-scale GCN to estimate the depth map. In turn, the second network will be used to estimate the ego-motion vector (i.e., 3D pose) between two consecutive frames based on ResNet-18. Both the estimated 3D pose and depth map will be used for constructing a target image. A combination of loss functions related to photometric, projection, and smoothness is used to cope with bad depth prediction and preserve the discontinuities of the objects. In particular, our method provided comparable and promising results with a high prediction accuracy of 89\% on the publicly KITTI and Make3D datasets along with a reduction of 40\% in the number of trainable parameters compared to the state of the art solutions. The source code is publicly available at https://github.com/ArminMasoumian/GCNDepth.git},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\3Z8XQA63\\Masoumian et al. - 2021 - GCNDepth Self-supervised Monocular Depth Estimation based on Graph Convolutional Network.pdf;C\:\\Users\\theun\\Zotero\\storage\\6TY7PSL9\\2112.html}
}

@article{masoumianMonocularDepthEstimation2022,
  title = {Monocular {{Depth Estimation Using Deep Learning}}: {{A Review}}},
  shorttitle = {Monocular {{Depth Estimation Using Deep Learning}}},
  author = {Masoumian, Armin and Rashwan, Hatem A. and Cristiano, Juli{\'a}n and Asif, M. Salman and Puig, Domenec},
  year = 2022,
  month = jan,
  journal = {Sensors},
  volume = {22},
  number = {14},
  pages = {5353},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s22145353},
  urldate = {2025-06-29},
  abstract = {In current decades, significant advancements in robotics engineering and autonomous vehicles have improved the requirement for precise depth measurements. Depth estimation (DE) is a traditional task in computer vision that can be appropriately predicted by applying numerous procedures. This task is vital in disparate applications such as augmented reality and target tracking. Conventional monocular DE (MDE) procedures are based on depth cues for depth prediction. Various deep learning techniques have demonstrated their potential applications in managing and supporting the traditional ill-posed problem. The principal purpose of this paper is to represent a state-of-the-art review of the current developments in MDE based on deep learning techniques. For this goal, this paper tries to highlight the critical points of the state-of-the-art works on MDE from disparate aspects. These aspects include input data shapes and training manners such as supervised, semi-supervised, and unsupervised learning approaches in combination with applying different datasets and evaluation indicators. At last, limitations regarding the accuracy of the DL-based MDE models, computational time requirements, real-time inference, transferability, input images shape and domain adaptation, and generalization are discussed to open new directions for future research.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {and unsupervised learning,deep learning,monocular depth estimation,multi-task learning,semi-supervised,single image depth estimation,supervised},
  file = {C:\Users\theun\Zotero\storage\YHGIXYE2\Masoumian et al. - 2022 - Monocular Depth Estimation Using Deep Learning A Review.pdf}
}

@incollection{matosAppleCountingSystem2025,
  title = {An {{Apple Counting System Robust}} to {{Multiple Intermittent Occlusions}}},
  booktitle = {Progress in {{Artificial Intelligence}}},
  author = {Matos, Gon{\c c}alo P. and Oliveira, Tiago G. and Silva, Filipe and Martinho, Francisco and Le{\~a}o, Miguel and Fonseca, Filipe and Silvestre, Jos{\'e} and Costeira, Jo{\~a}o P. and Saldanha, Ricardo L. and Santiago, Carlos and Morgado, Ernesto M.},
  editor = {Santos, Manuel Filipe and Machado, Jos{\'e} and Novais, Paulo and Cortez, Paulo and Moreira, Pedro Miguel},
  year = 2025,
  volume = {14967},
  pages = {181--192},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-73497-7_15},
  urldate = {2024-12-20},
  abstract = {Automated fruit counting, through the use of computer vision methods, is a very important step towards the development of precision agriculture, and, as a consequence, has been object of study by multiple authors. In this work, we present a novel method to detect, track and count fruits that is scalable and robust towards problems existing in other fruit counting methods. Namely, our approach consists in a method that uses (1) a stereo camera that can be used reliably under direct sunlight, (2) a fast detection algorithm, and (3) an algorithmic approach to track fruits which is robust to fruit occlusions. We applied our proposed solution in an apple orchard and were able to provide apple counts with an error ranging from 15\% to 57\%. The characteristics of this novel approach and the preliminary results achieved seem promising in order to tackle the problem of fruit counting with occlusions at the large scale of a whole orchard, since it addresses the problem of intermittent occlusions that is overlooked by other approaches. Even with errors, the overview of the entire orchard, resulting from the scalability of the process, which can be implemented with a camera in a tractor, makes real-time mapping possible, supporting technical decisions with major economical impact, such as supplementary fruit thinning, water or nutrient adjustments or harvest management.},
  isbn = {978-3-031-73496-0 978-3-031-73497-7},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\SXI826J5\Matos et al. - 2025 - An Apple Counting System Robust to Multiple Intermittent Occlusions.pdf}
}

@inproceedings{mayerLargeDatasetTrain2016,
  title = {A {{Large Dataset}} to {{Train Convolutional Networks}} for {{Disparity}}, {{Optical Flow}}, and {{Scene Flow Estimation}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Mayer, Nikolaus and Ilg, Eddy and Hausser, Philip and Fischer, Philipp and Cremers, Daniel and Dosovitskiy, Alexey and Brox, Thomas},
  year = 2016,
  month = jun,
  pages = {4040--4048},
  publisher = {IEEE},
  address = {Las Vegas, NV, USA},
  doi = {10.1109/CVPR.2016.438},
  urldate = {2025-05-06},
  abstract = {Recent work has shown that optical flow estimation can be formulated as a supervised learning task and can be successfully solved with convolutional networks. Training of the so-called FlowNet was enabled by a large synthetically generated dataset. The present paper extends the concept of optical flow estimation via convolutional networks to disparity and scene flow estimation. To this end, we propose three synthetic stereo video datasets with sufficient realism, variation, and size to successfully train large networks. Our datasets are the first large-scale datasets to enable training and evaluation of scene flow methods. Besides the datasets, we present a convolutional network for real-time disparity estimation that provides state-of-the-art results. By combining a flow and disparity estimation network and training it jointly, we demonstrate the first scene flow estimation with a convolutional network.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\8YD8B32R\Mayer et al. - 2016 - A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimati.pdf}
}

@inproceedings{mengDepthEstimationNighttime2024,
  title = {Depth {{Estimation}} of {{Nighttime Images Based}} on {{Feature Separation}}},
  booktitle = {Proceedings of the 2023 6th {{International Conference}} on {{Artificial Intelligence}} and {{Pattern Recognition}}},
  author = {Meng, Caixia and Hou, Qin and Gao, Yuxue},
  year = 2024,
  month = jun,
  series = {{{AIPR}} '23},
  pages = {15--20},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3641584.3641587},
  urldate = {2025-06-29},
  abstract = {In recent years, unsupervised monocular depth estimation frameworks based on the photometric consistency assumption have shown good performance in daytime scenes. However, the above unsupervised framework cannot be directly used for nighttime scenes due to the low visibility and complex illumination conditions that violate the luminance consistency assumption. In this paper, we propose a feature separation network for self-supervised depth estimation of nighttime images. Specifically, in order to mitigate the negative impact of interference terms such as illumination, shared features (texture, etc.) that are causally associated with depth information are separated from the circadian image pairs for depth estimation in this paper. Meanwhile, to ensure that the day and night images contain the same information, CycleGAN is used to generate night images corresponding to daytime scenes and input to the network as day and night image pairs to obtain better depth estimation by cross mutual information estimation and maximizing the separation of shared features. Experimental results show that the method in this paper achieves significant improvements on the Oxford RobotCar dataset and the challenging nuScenes dataset.},
  isbn = {979-8-4007-0767-4},
  file = {C:\Users\theun\Zotero\storage\LX7ZKAFK\Meng et al. - 2024 - Depth Estimation of Nighttime Images Based on Feature Separation.pdf}
}

@inproceedings{menzeObjectSceneFlow2015,
  title = {Object {{Scene Flow}} for {{Autonomous Vehicles}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Menze, Moritz and Geiger, Andreas},
  year = 2015,
  pages = {3061--3070},
  urldate = {2025-07-03},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\FESDC6AR\Menze and Geiger - 2015 - Object Scene Flow for Autonomous Vehicles.pdf}
}

@article{meyerMorphologicalSegmentation1990,
  title = {Morphological Segmentation},
  author = {Meyer, F. and Beucher, S.},
  year = 1990,
  month = sep,
  journal = {Journal of Visual Communication and Image Representation},
  volume = {1},
  number = {1},
  pages = {21--46},
  issn = {1047-3203},
  doi = {10.1016/1047-3203(90)90014-M},
  urldate = {2025-06-06},
  abstract = {Methods for image segmentation using mathematical morphology are presented. These methods are based on two main tools: the watershed transform and the homotopy modification which solve the problem of the oversegmentation and introduce the notion of markers of the objects to be segmented in the image. Some examples in various domains (biology, medicine, scene analysis, 3D images, detection of moving objects, color images) are given. We tried in these examples to emphasize the problems encountered and to explain shortly the proposed solutions. The algorithms are given in the Appendix. The detection of the markers requiring a large amount of morphological tools, many of them, are presented, though not directly related to segmentation},
  file = {C\:\\Users\\theun\\Zotero\\storage\\EZPVMB5G\\Meyer and Beucher - 1990 - Morphological segmentation.pdf;C\:\\Users\\theun\\Zotero\\storage\\FDWQBVFH\\104732039090014M.html}
}

@incollection{miaoBEVTempEnhancingVisionBased2025,
  title = {{{BEVTemp}}: {{Enhancing Vision-Based Roadside 3D Object Detection}} with {{Temporal Information}}},
  shorttitle = {{{BEVTemp}}},
  booktitle = {{{PRICAI}} 2024: {{Trends}} in {{Artificial Intelligence}}},
  author = {Miao, Gaoyuan and Quan, Rong and Pan, Cong and Hu, Zhiheng and Qin, Jie},
  editor = {Hadfi, Rafik and Anthony, Patricia and Sharma, Alok and Ito, Takayuki and Bai, Quan},
  year = 2025,
  volume = {15283},
  pages = {233--245},
  publisher = {Springer Nature Singapore},
  address = {Singapore},
  doi = {10.1007/978-981-96-0122-6_21},
  urldate = {2024-12-20},
  abstract = {Autonomous driving systems relying solely on ego-vehicle sensors face critical safety challenges due to limited perceptual scope and restricted long-range sensing capabilities. Roadside cameras offer a complementary paradigm capable of mitigating blindspots and extending the perceptual horizon. However, existing roadside detection methods struggle to accurately detect small, distant objects, which are crucial for timely hazard anticipation. In this work, we introduce BEVTemp, a 3D object detector that leverages multi-frame information to construct temporal stereo pairs, enabling enhanced long-range perception. Our key insight is to exploit information about nearby objects from past frames to facilitate detecting small, distant targets in the present view. Furthermore, we introduce a dedicated small object enhancement module to encode multi-scale features and strengthen localization signals, explicitly enhancing the detection of smaller classes like bicycles and pedestrians. Through these innovations, our framework delivers robust and accurate 3D object detection across diverse scales and distances prevalent in roadside environments. On the DAIR-V2X-I and Rope3D datasets, BEVTemp achieves state-of-the-art performance, surpassing all previous methods by a significant 4\% margin. Our work pioneers the use of multiframe cues for roadside perception and presents a comprehensive solution for long-range and small object detection in this domain.},
  isbn = {978-981-96-0121-9 978-981-96-0122-6},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\V6AP7ZL5\Miao et al. - 2025 - BEVTemp Enhancing Vision-Based Roadside 3D Object Detection with Temporal Information.pdf}
}

@inproceedings{milfordSeqSLAMVisualRoutebased2012,
  title = {{{SeqSLAM}}: {{Visual}} Route-Based Navigation for Sunny Summer Days and Stormy Winter Nights},
  shorttitle = {{{SeqSLAM}}},
  booktitle = {2012 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Milford, Michael J. and Wyeth, {\relax Gordon}. F.},
  year = 2012,
  month = may,
  pages = {1643--1649},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2012.6224623},
  urldate = {2025-06-19},
  abstract = {Learning and then recognizing a route, whether travelled during the day or at night, in clear or inclement weather, and in summer or winter is a challenging task for state of the art algorithms in computer vision and robotics. In this paper, we present a new approach to visual navigation under changing conditions dubbed SeqSLAM. Instead of calculating the single location most likely given a current image, our approach calculates the best candidate matching location within every local navigation sequence. Localization is then achieved by recognizing coherent sequences of these ``local best matches''. This approach removes the need for global matching performance by the vision front-end - instead it must only pick the best match within any short sequence of images. The approach is applicable over environment changes that render traditional feature-based techniques ineffective. Using two car-mounted camera datasets we demonstrate the effectiveness of the algorithm and compare it to one of the most successful feature-based SLAM algorithms, FAB-MAP. The perceptual change in the datasets is extreme; repeated traverses through environments during the day and then in the middle of the night, at times separated by months or years and in opposite seasons, and in clear weather and extremely heavy rain. While the feature-based method fails, the sequence-based algorithm is able to match trajectory segments at 100\% precision with recall rates of up to 60\%.},
  keywords = {/unread,Cameras,Navigation,Robot sensing systems,Trajectory,Vectors,Videos,Visualization},
  file = {C:\Users\theun\Zotero\storage\5G7WM3NA\Milford and Wyeth - 2012 - SeqSLAM Visual route-based navigation for sunny summer days and stormy winter nights.pdf}
}

@inproceedings{milfordSequenceSearchingDeeplearnt2015,
  title = {Sequence Searching with Deep-Learnt Depth for Condition- and Viewpoint-Invariant Route-Based Place Recognition},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Milford, Michael and Lowry, Stephanie and Sunderhauf, Niko and Shirazi, Sareh and Pepperell, Edward and Upcroft, Ben and Shen, Chunhua and Lin, Guosheng and Liu, Fayao and Cadena, Cesar and Reid, Ian},
  year = 2015,
  month = jun,
  pages = {18--25},
  publisher = {IEEE},
  address = {Boston, MA, USA},
  doi = {10.1109/CVPRW.2015.7301395},
  urldate = {2025-06-19},
  isbn = {978-1-4673-6759-2},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\EHSL7Z9B\Milford et al. - 2015 - Sequence searching with deep-learnt depth for condition- and viewpoint-invariant route-based place r.pdf}
}

@article{minaeeImageSegmentationUsing2022,
  title = {Image {{Segmentation Using Deep Learning}}: {{A Survey}}},
  shorttitle = {Image {{Segmentation Using Deep Learning}}},
  author = {Minaee, Shervin and Boykov, Yuri and Porikli, Fatih and Plaza, Antonio and Kehtarnavaz, Nasser and Terzopoulos, Demetri},
  year = 2022,
  month = jul,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {7},
  pages = {3523--3542},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2021.3059968},
  urldate = {2025-06-05},
  abstract = {Image segmentation is a key task in computer vision and image processing with important applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among others, and numerous segmentation algorithms are found in the literature. Against this backdrop, the broad success of deep learning (DL) has prompted the development of new image segmentation approaches leveraging DL models. We provide a comprehensive review of this recent literature, covering the spectrum of pioneering efforts in semantic and instance segmentation, including convolutional pixel-labeling networks, encoder-decoder architectures, multiscale and pyramid-based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the relationships, strengths, and challenges of these DL-based segmentation models, examine the widely used datasets, compare performances, and discuss promising research directions.},
  keywords = {Computational modeling,Computer architecture,convolutional neural networks,deep learning,Deep learning,encoder-decoder models,Generative adversarial networks,generative models,Image segmentation,instance segmentation,Logic gates,medical image segmentation,panoptic segmentation,recurrent models,semantic segmentation,Semantics},
  file = {C:\Users\theun\Zotero\storage\UHNZHBY7\Minaee et al. - 2022 - Image Segmentation Using Deep Learning A Survey.pdf}
}

@article{mingDeepLearningMonocular2021,
  title = {Deep Learning for Monocular Depth Estimation: {{A}} Review},
  shorttitle = {Deep Learning for Monocular Depth Estimation},
  author = {Ming, Yue and Meng, Xuyang and Fan, Chunxiao and Yu, Hui},
  year = 2021,
  month = may,
  journal = {Neurocomputing},
  volume = {438},
  pages = {14--33},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2020.12.089},
  urldate = {2025-06-29},
  abstract = {Depth estimation is a classic task in computer vision, which is of great significance for many applications such as augmented reality, target tracking and autonomous driving. Traditional monocular depth estimation methods are based on depth cues for depth prediction with strict requirements, e.g. shape-from-focus/ defocus methods require low depth of field on the scenes and images. Recently, a large body of deep learning methods have been proposed and has shown great promise in handling the traditional ill-posed problem. This paper aims to review the state-of-the-art development in deep learning-based monocular depth estimation. We give an overview of published papers between 2014 and 2020 in terms of training manners and task types. We firstly summarize the deep learning models for monocular depth estimation. Secondly, we categorize various deep learning-based methods in monocular depth estimation. Thirdly, we introduce the publicly available dataset and the evaluation metrics. And we also analysis the properties of these methods and compare their performance. Finally, we highlight the challenges in order to inform the future research directions.},
  keywords = {Deep learning,Monocular depth estimation,Multi-task learning,Supervised learning,Unsupervised learning},
  file = {C\:\\Users\\theun\\Zotero\\storage\\8LSKE5HT\\Ming et al. - 2021 - Deep learning for monocular depth estimation A review.pdf;C\:\\Users\\theun\\Zotero\\storage\\8KL3YHYT\\S0925231220320014.html}
}

@article{miralievRealTimeMemoryEfficient2024,
  title = {Real-{{Time Memory Efficient Multitask Learning Model}} for {{Autonomous Driving}}},
  author = {Miraliev, Shokhrukh and Abdigapporov, Shakhboz and Kakani, Vijay and Kim, Hakil},
  year = 2024,
  month = jan,
  journal = {IEEE Transactions on Intelligent Vehicles},
  volume = {9},
  number = {1},
  pages = {247--258},
  issn = {2379-8904},
  doi = {10.1109/TIV.2023.3270878},
  urldate = {2025-01-03},
  abstract = {Developing a self-driving system is a challenging task that requires a high level of scene comprehension with real-time inference, and it is safety-critical. This study proposes a real-time memory efficient multitask learning-based model for joint object detection, drivable area segmentation, and lane detection tasks. To accomplish this research objective, the encoder-decoder architecture efficiently utilized to handle input frames through shared representation. Comprehensive experiments conducted on a challenging public Berkeley Deep Drive (BDD100 K) dataset. For further performance comparisons, a private dataset consisting of 30 K frames was collected and annotated for the three aforementioned tasks. Experimental results demonstrated the superiority of the proposed method's over existing baseline approaches in terms of computational efficiency, model power consumption and accuracy performance. The performance results for object detection, drivable area segmentation and lane detection tasks showed the highest 77.5 mAP50, 91.9 mIoU and 33.8 mIoU results on BDD100K dataset respectively. In addition, the model achieved 112.29 fps processing speed improving both performance and inference speed results of existing multi-tasking models.},
  keywords = {autonomous driving,convolutional neural networks,Decoding,drivable area segmentation,edge device,Feature extraction,lane detection,Lane detection,Multitask learning,object detection,Object detection,Performance evaluation,Roads,Task analysis},
  file = {C\:\\Users\\theun\\Zotero\\storage\\XNLGQNF7\\Miraliev et al. - 2024 - Real-Time Memory Efficient Multitask Learning Model for Autonomous Driving.pdf;C\:\\Users\\theun\\Zotero\\storage\\TGBABBNR\\10109860.html}
}

@article{moReviewStateoftheartTechnologies2022,
  title = {Review the State-of-the-Art Technologies of Semantic Segmentation Based on Deep Learning},
  author = {Mo, Yujian and Wu, Yan and Yang, Xinneng and Liu, Feilin and Liao, Yujun},
  year = 2022,
  month = jul,
  journal = {Neurocomputing},
  volume = {493},
  pages = {626--646},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2022.01.005},
  urldate = {2025-01-03},
  abstract = {The goal of semantic segmentation is to segment the input image according to semantic information and predict the semantic category of each pixel from a given label set. With the gradual intellectualization of modern life, more and more applications need to infer relevant semantic information from images for subsequent processing, such as augmented reality, autonomous driving, video surveillance, etc. This paper reviews the state-of-the-art technologies of semantic segmentation based on deep learning. Because semantic segmentation requires a large number of pixel-level annotations, in order to reduce the fine-grained requirements of annotation and reduce the economic and time cost of manual annotation, this paper studies the works on weakly-supervised semantic segmentation. In order to enhance the generalization ability and robustness of the segmentation model, this paper investigates the works on domain adaptation in semantic segmentation. Many types of sensors are usually equipped in some practical applications, such as autonomous driving and medical image analysis. In order to mine the association between multi-modal data and improve the accuracy of the segmentation model, this paper investigates the works based on multi-modal data fusion semantic segmentation. The real-time performance of the model needs to be considered in practical application. This paper analyzes the key factors affecting the real-time performance of the segmentation model and investigates the works on real-time semantic segmentation. Finally, this paper summarizes the challenges and promising research directions of semantic segmentation tasks based on deep learning.},
  keywords = {/unread,Convolutional neural networks,Deep learning,Domain adaptation,Multi-modal fusion,Real-time,Semantic segmentation,Weakly-supervised},
  file = {C\:\\Users\\theun\\Zotero\\storage\\7DUZ3A2I\\Mo et al. - 2022 - Review the state-of-the-art technologies of semantic segmentation based on deep learning.pdf;C\:\\Users\\theun\\Zotero\\storage\\E2VL5EMJ\\S0925231222000054.html}
}

@inproceedings{mottaghiRoleContextObject2014,
  title = {The {{Role}} of {{Context}} for {{Object Detection}} and {{Semantic Segmentation}} in the {{Wild}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Mottaghi, Roozbeh and Chen, Xianjie and Liu, Xiaobai and Cho, Nam-Gyu and Lee, Seong-Whan and Fidler, Sanja and Urtasun, Raquel and Yuille, Alan},
  year = 2014,
  pages = {891--898},
  urldate = {2025-06-05},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\GXRVUD7P\Mottaghi et al. - 2014 - The Role of Context for Object Detection and Semantic Segmentation in the Wild.pdf}
}

@inproceedings{muEndtoendSemanticSegmentation2024,
  title = {End-to-End {{Semantic Segmentation Network}} for {{Low-Light Scenes}}},
  booktitle = {2024 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Mu, Hongmin and Zhang, Gang and Zhou, MengChu and Cao, Zhengcai},
  year = 2024,
  month = may,
  pages = {7725--7731},
  doi = {10.1109/ICRA57147.2024.10611148},
  urldate = {2025-06-18},
  abstract = {In the fields of robotic perception and computer vision, achieving accurate semantic segmentation of low-light or nighttime scenes is challenging. This is primarily due to the limited visibility of objects and the reduced texture and color contrasts among them. To address the issue of limited visibility, we propose a hierarchical gated convolution unit, which simultaneously expands the receptive field and restores edge texture. To address the issue of reduced texture among objects, we propose a dual closed-loop bipartite matching algorithm to establish a total loss function consisting of the unsupervised illumination enhancement loss and supervised intersection-over-union loss, thus enabling the joint minimization of both losses via the Hungarian algorithm. We thus achieve end-to-end training for a semantic segmentation network especially suitable for handling low-light scenes. Experimental results demonstrate that the proposed network surpasses existing methods on the Cityscapes dataset and notably outperforms state-of-the-art methods on both Dark Zurich and Nighttime Driving datasets.},
  keywords = {Accuracy,Bipartite matching algorithm,Colour contrast,Computer vision,Convolution,Dual closed loops,End to end,Lighting,Limited visibility,Logic gates,Loss functions,Low light,Receptive fields,Semantic segmentation,Semantic Segmentation,Total loss,Training},
  file = {C:\Users\theun\Zotero\storage\F6R8UATJ\Mu et al. - 2024 - End-to-end Semantic Segmentation Network for Low-Light Scenes.pdf}
}

@article{muhammadVisionBasedSemanticSegmentation2022,
  title = {Vision-{{Based Semantic Segmentation}} in {{Scene Understanding}} for {{Autonomous Driving}}: {{Recent Achievements}}, {{Challenges}}, and {{Outlooks}}},
  shorttitle = {Vision-{{Based Semantic Segmentation}} in {{Scene Understanding}} for {{Autonomous Driving}}},
  author = {Muhammad, Khan and Hussain, Tanveer and Ullah, Hayat and Ser, Javier Del and Rezaei, Mahdi and Kumar, Neeraj and Hijji, Mohammad and Bellavista, Paolo and {de Albuquerque}, Victor Hugo C.},
  year = 2022,
  month = dec,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {23},
  number = {12},
  pages = {22694--22715},
  issn = {1558-0016},
  doi = {10.1109/TITS.2022.3207665},
  urldate = {2025-01-03},
  abstract = {Scene understanding plays a crucial role in autonomous driving by utilizing sensory data for contextual information extraction and decision making. Beyond modeling advances, the enabler for vehicles to become aware of their surroundings is the availability of visual sensory data, which expand the vehicular perception and realizes vehicular contextual awareness in real-world environments. Research directions for scene understanding pursued by related studies include person/vehicle detection and segmentation, their transition analysis, lane change, and turns detection, among many others. Unfortunately, these tasks seem insufficient to completely develop fully-autonomous vehicles i.e., achieving level-5 autonomy, travelling just like human-controlled cars. This latter statement is among the conclusions drawn from this review paper: scene understanding for autonomous driving cars using vision sensors still requires significant improvements. With this motivation, this survey defines, analyzes, and reviews the current achievements of the scene understanding research area that mostly rely on computationally complex deep learning models. Furthermore, it covers the generic scene understanding pipeline, investigates the performance reported by the state-of-the-art, informs about the time complexity analysis of avant garde modeling choices, and highlights major triumphs and noted limitations encountered by current research efforts. The survey also includes a comprehensive discussion on the available datasets, and the challenges that, even if lately confronted by researchers, still remain open to date. Finally, our work outlines future research directions to welcome researchers and practitioners to this exciting domain.},
  keywords = {/unread,Automobiles,Autonomous driving,autonomous vehicles,Computer architecture,context prediction,deep learning,Feature extraction,Image color analysis,Roads,scene understanding,semantic segmentation,Semantics,Visualization},
  file = {C\:\\Users\\theun\\Zotero\\storage\\ZIBY4RA9\\Muhammad et al. - 2022 - Vision-Based Semantic Segmentation in Scene Understanding for Autonomous Driving Recent Achievement.pdf;C\:\\Users\\theun\\Zotero\\storage\\GHZ5PHGB\\9913352.html}
}

@article{mukhtarVehicleDetectionTechniques2015,
  title = {Vehicle {{Detection Techniques}} for {{Collision Avoidance Systems}}: {{A Review}}},
  shorttitle = {Vehicle {{Detection Techniques}} for {{Collision Avoidance Systems}}},
  author = {Mukhtar, Amir and Xia, Likun and Tang, Tong Boon},
  year = 2015,
  month = oct,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {16},
  number = {5},
  pages = {2318--2338},
  issn = {1524-9050, 1558-0016},
  doi = {10.1109/TITS.2015.2409109},
  urldate = {2025-01-02},
  abstract = {Over the past decade, vision-based vehicle detection techniques for road safety improvement have gained an increasing amount of attention. Unfortunately, the techniques suffer from robustness due to huge variability in vehicle shape (particularly for motorcycles), cluttered environment, various illumination conditions, and driving behavior. In this paper, we provide a comprehensive survey in a systematic approach about the state-of-the-art on-road vision-based vehicle detection and tracking systems for collision avoidance systems (CASs). This paper is structured based on a vehicle detection processes starting from sensor selection to vehicle detection and tracking. Techniques in each process/step are reviewed and analyzed individually. Two main contributions in this paper are the following: survey on motorcycle detection techniques and the sensor comparison in terms of cost and range parameters. Finally, the survey provides an optimal choice with a low cost and reliable CAS design in vehicle industries.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\B98GENL8\Mukhtar et al. - 2015 - Vehicle Detection Techniques for Collision Avoidance Systems A Review.pdf}
}

@inproceedings{munirAutonomousVehicleArchitecture2018,
  title = {Autonomous {{Vehicle}}: {{The Architecture Aspect}} of {{Self Driving Car}}},
  shorttitle = {Autonomous {{Vehicle}}},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Sensors}}, {{Signal}} and {{Image Processing}}},
  author = {Munir, Farzeen and Azam, Shoaib and Hussain, Muhammad Ishfaq and Sheri, Ahmed Muqeem and Jeon, Moongu},
  year = 2018,
  month = oct,
  series = {{{SSIP}} '18},
  pages = {1--5},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3290589.3290599},
  urldate = {2025-01-09},
  abstract = {Self-driving cars have received a lot of attention in recent years and many stakeholders like Google, Uber, Tesla, and so forth have invested a lot in this area and developed their own autonomous driving car platforms. The challenge to make an autonomous car is not only the stringent performance but also the safety of the passengers and pedestrians. Even with the development of technologies, autonomous driving is still an active research area and still requires a lot of experimentations and making architecture entirely autonomous.The intriguing area of self-driving car motivates us to build an autonomous driving platform. In this paper, we discuss the architecture of the self-driving car and its software components that include localization, detection, motion planning and mission planning. We also highlight the hardware modules that are responsible for controlling the car. The autonomous driving is running state-of-the-art algorithms used in localization, detection, mission and motion planning.},
  isbn = {978-1-4503-6620-5},
  file = {C:\Users\theun\Zotero\storage\FNXMMS5A\Munir et al. - 2018 - Autonomous Vehicle The Architecture Aspect of Self Driving Car.pdf}
}

@article{mustafaReviewHistogramEqualization2018,
  title = {A {{Review}} of {{Histogram Equalization Techniques}} in {{Image Enhancement Application}}},
  author = {Mustafa, Wan Azani and Abdul Kader, Mohamed Mydin M.},
  year = 2018,
  month = jun,
  journal = {Journal of Physics: Conference Series},
  volume = {1019},
  number = {1},
  pages = {012026},
  publisher = {IOP Publishing},
  issn = {1742-6596},
  doi = {10.1088/1742-6596/1019/1/012026},
  urldate = {2025-05-26},
  abstract = {Image enhancement can be considered as one of the fundamental processes in image analysis. The goal of contrast enhancement is to improve the quality of an image to become more suitable for a particular application. Till today, numerous image enhancement methods have been proposed for various applications and efforts have been directed to further increase the quality of the enhancement results and minimize the computational complexity and memory usage. In this paper, an image enhancement methods based on Histogram Equalization (HE) was studied. This paper presents an exhaustive review of these studies and suggests a direction for future developments of image enhancement methods. Each method shows the owned advantages and drawbacks. In future, this work will give the direction to other researchers in order to propose new advanced enhancement techniques.},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\EQL5BRXW\Mustafa and Abdul Kader - 2018 - A Review of Histogram Equalization Techniques in Image Enhancement Application.pdf}
}

@article{najmanWatershedContinuousFunction1994,
  title = {Watershed of a Continuous Function},
  author = {Najman, Laurent and Schmitt, Michel},
  year = 1994,
  month = jul,
  journal = {Signal Processing},
  series = {Mathematical {{Morphology}} and Its {{Applications}} to {{Signal Processing}}},
  volume = {38},
  number = {1},
  pages = {99--112},
  issn = {0165-1684},
  doi = {10.1016/0165-1684(94)90059-0},
  urldate = {2025-06-06},
  abstract = {The notion of watershed, used in morphological segmentation, has only a digital definition. In this paper, we propose to extend this definition to the continuous plane. Using this continuous definition, we present the watershed differences with classical edge detectors. We then exhibit a metric in the plane for which the watershed is a skeleton by influence zones and show the lower semicontinuous behaviour of the associated skeleton. This theoretical approach suggests an algorithm for solving the eikonal equation: \textbardbl ∇\textflorin\textbardbl{} = g. Finally, we end with some new watershed algorithms, which present the advantage of allowing the use of markers and/or anchor points, thus opening the way towards grey-tone skeletons. Zusammenfassung Der Begriff Wassersheide, der in der morphologischen Segmentation verwendet wird, hat nur eine diskrete Definition. In diesem Artikel schlagen wir vor, diese Definition auf die kontinuierlich Ebene auszudehnen. Indem wir die kontinuierliche Definition verwenden, stellen wird die Unterschiede zwischen Wasserscheide und klassischen Kantendetektoren vor. Wir zeigen dann eine Metrik in der Ebene, f\"ur die die Wasserscheide ein Skelett von Einflu{$\beta$}zonen ist und zeigen das untergeordnete halbkontinuierliche Verhalten von damit verbunden Skeletten. Dieser theoretische Ansatz schl\"agt einen Algorithmus f\"ur die L\"osung der Eikonal-Gleichung \textbardbl ∇\textflorin\textbardbl{} = g vor. Schlie{$\beta$}lich gelangen wir zu einem neuen Wasserscheiden-Algorithmus, der den Vorteil hat, die Benutzung von Markierungen und/oder Ankerpunkten zu erlauben und daher den Weg zu Graustufenskeletten \"offnet. R\'esum\'e La notion de ligne de partage des eaux, utilis\'ee en segmentation morphologique dispose uniquement d'une d\'efinition digitale. Dans cet article, nous proposons d'\'etendre la d\'efinition de la ligne de partage des eaux au plan continu. En utilisant cette d\'efinition continue, nous comparons la ligne de partage des eaux avec les extracteurs de contours classiques, et montrons leurs diff\'erences. Nous introduisons ensuite une m\'etrique pour laquelle la ligne de partage des eaux est un squelette par zones d'influence, ce qui nous permet de montrer son comportement semi-continu. Cette approche th\'eorique nous sugg\`ere un nouvel algorithme pour r\'esoudre l'\'equation eikonal: trouver \textflorin{} telle que \textbardbl ∇\textflorin\textbardbl{} = g.},
  file = {C\:\\Users\\theun\\Zotero\\storage\\PWWLQTU2\\Najman and Schmitt - 1994 - Watershed of a continuous function.pdf;C\:\\Users\\theun\\Zotero\\storage\\UM8DQKIE\\0165168494900590.html}
}

@article{nakaguchiDevelopmentMachineStereo2024,
  title = {Development of a {{Machine}} Stereo Vision-Based Autonomous Navigation System for Orchard Speed Sprayers},
  author = {Nakaguchi, Victor Massaki and Rasika D. Abeyrathna, R.M. and Liu, Zifu and Noguchi, Ryozo and Ahamed, Tofael},
  year = 2024,
  month = dec,
  journal = {Computers and Electronics in Agriculture},
  volume = {227},
  pages = {109669},
  issn = {01681699},
  doi = {10.1016/j.compag.2024.109669},
  urldate = {2024-12-06},
  abstract = {In orchards, radio frequency scintillation caused by high vegetation density can hinder the effectiveness of machinery auto guidance based on the Global Navigation Satellite System (GNSS). Signal interference not only leads to poor quality of machine operation but can also pose operational risks. In this work, we propose an alternative trajectory positioning system to supplement traditional GNSS-based navigation for machinery used in orchards. The aim of this research was to develop a deep learning machine stereo vision guidance system onboard an orchard speed sprayer. The developed system combines a collision avoidance methodology along with deep learning-driven machine vision for interrow positioning and a dead reckoning set of rules for alternating Uturns. The developed methodology was tested in 4 rows of an artificial orchard. The results show that it is possible for the embedded EfficientDet target detection algorithm to guide the equipment at 1, 1.5 and 2 km \texttimes{} h 1 with minimum average root-mean-square errors (RMSEs) of 0.24 m, 0.20 m, and 0.31 m, respectively. When the system navigation was performed using YOLOv7, the minimum average RMSEs for each row were 0.40 m, 0.48 m, and 0.43 m, respectively, for the abovementioned speeds. The U-turn by dead reckoning showed minimum average RMSE values of 0.56 m, 0.22 m, and 0.35 m for row navigation based on EfficientDet at 1, 1.5 and 2 km \texttimes{} h 1, respectively. For YOLOv7-based navigation in rows, the minimum average RMSE values at these speeds were 0.35 m, 0.81 m, and 0.44 m, respectively. This study contributes to the field by proposing an alternative navigation system for orchard machines that operates without the limitations associated with GNSS. In addition, our proposed guidance methodology introduces an RGB-D collision avoidance system with a demonstrated safety capacity for navigation under real scenario conditions.},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\NQKZQDRC\Nakaguchi et al. - 2024 - Development of a Machine stereo vision-based autonomous navigation system for orchard speed sprayers.pdf}
}

@article{nampoothiriRecentDevelopmentsTerrain2021,
  title = {Recent Developments in Terrain Identification, Classification, Parameter Estimation for the Navigation of Autonomous Robots},
  author = {Nampoothiri, M. G. Harinarayanan and Vinayakumar, B and Sunny, Youhan and Antony, Rahul},
  year = 2021,
  month = apr,
  journal = {SN Applied Sciences},
  volume = {3},
  number = {4},
  pages = {480},
  issn = {2523-3963, 2523-3971},
  doi = {10.1007/s42452-021-04453-3},
  urldate = {2025-01-03},
  abstract = {The work presents a review on ongoing researches in terrain-related challenges influencing the navigation of Autonomous Robots, specifically Unmanned Ground ones. The paper aims to highlight the recent developments in robot design and advanced computing techniques in terrain identification, classification, parameter estimation, and developing modern control strategies. The objective of our research is to familiarize the gaps and opportunities of the aforementioned areas to the researchers who are passionate to take up research in the field of autonomous robots. The paper brings recent works related to terrain strategies under a single platform focusing on the advancements in planetary rovers, rescue robots, military robots, agricultural robots, etc. Finally, this paper provides a comprehensive analysis of the related works which can bridge the AI techniques and advanced control strategies to improve navigation. The study focuses on various Deep Learning techniques and Fuzzy Logic Systems in detail. The work can be extended to develop new control schemes to improve multiple terrain navigation performance.},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\XVXZ4N6V\Nampoothiri et al. - 2021 - Recent developments in terrain identification, classification, parameter estimation for the navigati.pdf}
}

@article{nandaInternetAutonomousVehicles2019,
  title = {Internet of {{Autonomous Vehicles Communications Security}}: {{Overview}}, {{Issues}}, and {{Directions}}},
  shorttitle = {Internet of {{Autonomous Vehicles Communications Security}}},
  author = {Nanda, Ashish and Puthal, Deepak and Rodrigues, Joel J. P. C. and Kozlov, Sergei A.},
  year = 2019,
  month = aug,
  journal = {IEEE Wireless Communications},
  volume = {26},
  number = {4},
  pages = {60--65},
  issn = {1558-0687},
  doi = {10.1109/MWC.2019.1800503},
  urldate = {2025-01-02},
  abstract = {The Internet of Things (IoT) is an emerging technology that has gained a huge user base by facilitating Internet-connected devices being used in numerous applications including smart vehicular infrastructure. In this context, we focus on the traditional vehicular ad hoc network that has evolved into a new perception called the Internet of Vehicles (IoV), and is expected to soon transform into the Internet of Autonomous Vehicles (IoAV). IoAV hopes to facilitate smart vehicular infrastructure and autonomous driving without the need for human involvement. However, as the number of connected vehicles keeps increasing, so does the need for autonomous decision making. Hence, the IoAV must provide robust, secure, seamless, and scalable communication among the vehicles as well as the roadside units. This article provides an overview of autonomous vehicle communication layers, its associated properties, and security threats. Further, this article also briefly discusses the current research trends and future research issues.},
  keywords = {/unread,Accidents,Autonomous vehicles,Internet,Jamming,Security,Sensors,Vehicular ad hoc networks},
  file = {C\:\\Users\\theun\\Zotero\\storage\\W7YBP5TB\\Nanda et al. - 2019 - Internet of Autonomous Vehicles Communications Security Overview, Issues, and Directions.pdf;C\:\\Users\\theun\\Zotero\\storage\\5YYLEAGK\\8809661.html}
}

@article{nidamanuriProgressiveReviewEmerging2022,
  title = {A {{Progressive Review}}: {{Emerging Technologies}} for {{ADAS Driven Solutions}}},
  shorttitle = {A {{Progressive Review}}},
  author = {Nidamanuri, Jaswanth and Nibhanupudi, Chinmayi and Assfalg, Rolf and Venkataraman, Hrishikesh},
  year = 2022,
  month = jun,
  journal = {IEEE Transactions on Intelligent Vehicles},
  volume = {7},
  number = {2},
  pages = {326--341},
  issn = {2379-8904},
  doi = {10.1109/TIV.2021.3122898},
  urldate = {2025-01-02},
  abstract = {Over the last decade, the Advanced Driver Assistance System (ADAS) concept has evolved significantly. ADAS involves several technologies such as automotive electronics, vehicle-to-vehicle (V2V) vehicle-to-infrastructure (V2I) communication, RADAR, LIDAR, computer vision, and machine learning. Of these, computer vision and machine learning based solutions have mainly been effective that have allowed real-time vehicle control, driver aided systems, etc. However, most of the existing works deal with the deployment of ADAS and autonomous driving functionality in countries with well-disciplined lane traffic. Nevertheless, these solutions and frameworks do not work in countries and cities with less-disciplined/chaotic traffic. This paper identifies the research gaps, reviews the state-of-the-art looking at the different functionalities of ADAS and its levels of autonomy. Importantly, it provides a detailed description of vision intelligence and computational intelligence for ADAS. The eye-gaze and head pose estimation in vision intelligence is detailed. Notably, the learning algorithms such as supervised, unsupervised, reinforcement learning and deep learning solutions for ADAS are considered and discussed. Significantly, this would enable developing a real-time recommendation system for system-assisted/autonomous vehicular environments with less-disciplined road traffic.},
  keywords = {/unread,Autonomous driving,Cameras,computer vision,intelligent transportation,Laser radar,machine learning,multi-sensor,Radar,Roads,Safety,Sensors,Vehicles},
  file = {C\:\\Users\\theun\\Zotero\\storage\\QJECL42D\\Nidamanuri et al. - 2022 - A Progressive Review Emerging Technologies for ADAS Driven Solutions.pdf;C\:\\Users\\theun\\Zotero\\storage\\PYY4GDZX\\9591277.html}
}

@misc{nieReason2DriveInterpretableChainbased2024,
  title = {{{Reason2Drive}}: {{Towards Interpretable}} and {{Chain-based Reasoning}} for {{Autonomous Driving}}},
  shorttitle = {{{Reason2Drive}}},
  author = {Nie, Ming and Peng, Renyuan and Wang, Chunwei and Cai, Xinyue and Han, Jianhua and Xu, Hang and Zhang, Li},
  year = 2024,
  month = jul,
  number = {arXiv:2312.03661},
  eprint = {2312.03661},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.03661},
  urldate = {2024-12-06},
  abstract = {Large vision-language models (VLMs) have garnered increasing interest in autonomous driving areas, due to their advanced capabilities in complex reasoning tasks essential for highly autonomous vehicle behavior. Despite their potential, research in autonomous systems is hindered by the lack of datasets with annotated reasoning chains that explain the decision-making processes in driving. To bridge this gap, we present Reason2Drive, a benchmark dataset with over 600K video-text pairs, aimed at facilitating the study of interpretable reasoning in complex driving environments. We distinctly characterize the autonomous driving process as a sequential combination of perception, prediction, and reasoning steps, and the question-answer pairs are automatically collected from a diverse range of open-source outdoor driving datasets, including nuScenes, Waymo and ONCE. Moreover, we introduce a novel aggregated evaluation metric to assess chain-based reasoning performance in autonomous systems, addressing the semantic ambiguities of existing metrics such as BLEU and CIDEr. Based on the proposed benchmark, we conduct experiments to assess various existing VLMs, revealing insights into their reasoning capabilities. Additionally, we develop an efficient approach to empower VLMs to leverage object-level perceptual elements in both feature extraction and prediction, further enhancing their reasoning accuracy. The code and dataset will be released.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\Z32IQV7S\\Nie et al. - 2024 - Reason2Drive Towards Interpretable and Chain-based Reasoning for Autonomous Driving.pdf;C\:\\Users\\theun\\Zotero\\storage\\JF68L23A\\2312.html}
}

@article{nockStatisticalRegionMerging2004,
  title = {Statistical Region Merging},
  author = {Nock, R. and Nielsen, F.},
  year = 2004,
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {26},
  number = {11},
  pages = {1452--1458},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2004.110},
  urldate = {2025-06-06},
  abstract = {This paper explores a statistical basis for a process often described in computer vision: image segmentation by region merging following a particular order in the choice of regions. We exhibit a particular blend of algorithmics and statistics whose segmentation error is, as we show, limited from both the qualitative and quantitative standpoints. This approach can be efficiently approximated in linear time/space, leading to a fast segmentation algorithm tailored to processing images described using most common numerical pixel attribute spaces. The conceptual simplicity of the approach makes it simple to modify and cope with hard noise corruption, handle occlusion, authorize the control of the segmentation scale, and process unconventional data such as spherical images. Experiments on gray-level and color images, obtained with a short readily available C-code, display the quality of the segmentations obtained.},
  keywords = {Color,Computer errors,Computer vision,Displays,Error analysis,Image segmentation,image segmentation.,Index Terms- Grouping,Linear approximation,Merging,Pixel,Testing},
  file = {C:\Users\theun\Zotero\storage\5EZNA2UB\Nock and Nielsen - 2004 - Statistical region merging.pdf}
}

@inproceedings{nohLearningDeconvolutionNetwork2015,
  title = {Learning {{Deconvolution Network}} for {{Semantic Segmentation}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
  year = 2015,
  pages = {1520--1528},
  urldate = {2025-06-05},
  file = {C:\Users\theun\Zotero\storage\A8VVH6XA\Noh et al. - 2015 - Learning Deconvolution Network for Semantic Segmentation.pdf}
}

@article{nuradiliSemanticSegmentationUAV2024,
  title = {Semantic Segmentation for {{UAV}} Low-Light Scenes Based on Deep Learning and Thermal Infrared Image Features},
  author = {Nuradili, P. and Zhou, G. and Zhou, J. and Wang, Z. and Meng, Y. and Tang, W. and Melgani, F.},
  year = 2024,
  journal = {International Journal of Remote Sensing},
  volume = {45},
  number = {12},
  pages = {4160--4177},
  publisher = {{Taylor and Francis Ltd.}},
  doi = {10.1080/01431161.2024.2357842},
  abstract = {With advancements in unmanned aerial vehicle (UAV) remote sensing technology, remote sensing images have emerged as a critical source of research data across various domains, including agriculture, forestry, and environmental research. UAVs fitted with diverse spectral sensors are capable of capturing diverse image modalities, presenting both challenges and opportunities for image semantic segmentation technology. Most existing semantic segmentation networks excel in processing images captured by visible light cameras and often fail to segment images captured by unmanned aerial vehicles under low-light conditions due to insufficient lighting, reduced visual clarity, high noise levels, and uneven illumination. Thermal infrared imaging sensors can capture thermal radiation information, which has the potential to improve segmentation accuracy when integrated with visible images. In this study, we introduce a novel semantic segmentation processing framework, which evaluates different fusing methods and fuses visible and thermal infrared images. The framework employs a lightweight deep learning model and is designed for accurate semantic segmentation on the fused images. Experiments are conducted on images collected in our unmanned aerial vehicle flight experiments and a public night-time dataset to assess the performance of our proposed approach. Experimental results show that our proposed framework achieves state-of-the-art performance in semantic segmentation tasks in low-light conditions. \copyright{} 2024 Informa UK Limited, trading as Taylor \& Francis Group.},
  keywords = {/unread}
}

@inproceedings{o.pinheiroLearningSegmentObject2015,
  title = {Learning to {{Segment Object Candidates}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {O. Pinheiro, Pedro O and Collobert, Ronan and Dollar, Piotr},
  year = 2015,
  volume = {28},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-06-05},
  abstract = {Recent object detection systems rely on two critical steps: (1) a set of object proposals is predicted as efficiently as possible, and (2) this set of candidate proposals is then passed to an object classifier. Such approaches have been shown they can be fast, while achieving the state of the art in detection performance. In this paper, we propose a new way to generate object proposals, introducing an approach based on a discriminative convolutional network. Our model is trained jointly with two objectives: given an image patch, the first part of the system outputs a class-agnostic segmentation mask, while the second part of the system outputs the likelihood of the patch being centered on a full object. At test time, the model is efficiently applied on the whole test image and generates a set of segmentation masks, each of them being assigned with a corresponding object likelihood score. We show that our model yields significant improvements over state-of-the-art object proposal algorithms. In particular, compared to previous approaches, our model obtains substantially higher object recall using fewer proposals. We also show that our model is able to generalize to unseen categories it has not seen during training. Unlike all previous approaches for generating object masks, we do not rely on edges, superpixels, or any other form of low-level segmentation.},
  file = {C:\Users\theun\Zotero\storage\4AY8X58E\O. Pinheiro et al. - 2015 - Learning to Segment Object Candidates.pdf}
}

@misc{osheaIntroductionConvolutionalNeural2015,
  title = {An {{Introduction}} to {{Convolutional Neural Networks}}},
  author = {O'Shea, Keiron and Nash, Ryan},
  year = 2015,
  month = dec,
  number = {arXiv:1511.08458},
  eprint = {1511.08458},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1511.08458},
  urldate = {2024-12-09},
  abstract = {The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs. This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\theun\\Zotero\\storage\\VANBTDTJ\\O'Shea and Nash - 2015 - An Introduction to Convolutional Neural Networks.pdf;C\:\\Users\\theun\\Zotero\\storage\\N593MHB7\\1511.html}
}

@article{otsuThresholdSelectionMethod1979,
  title = {A {{Threshold Selection Method}} from {{Gray-Level Histograms}}},
  author = {Otsu, Nobuyuki},
  year = 1979,
  month = jan,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  abstract = {A nonparametric and unsupervised method of automatic threshold selection for picture segmentation is presented. An optimal threshold is selected by the discriminant criterion, namely, so as to maximize the separability of the resultant classes in gray levels. The procedure is very simple, utilizing only the zeroth- and the first-order cumulative moments of the gray-level histogram. It is straightforward to extend the method to multithreshold problems. Several experimental results are also presented to support the validity of the method.},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\RIKTTQNH\Otsu - A Tlreshold Selection Method from Gray-Level Histograms.pdf}
}

@article{panAutomaticPavementCrack2023,
  title = {Automatic Pavement Crack Segmentation Using a Generative Adversarial Network ({{GAN}})-Based Convolutional Neural Network},
  author = {Pan, Zhihao and Lau, Stephen L. H. and Yang, Xu and Guo, Ningqun and Wang, Xin},
  year = 2023,
  month = sep,
  journal = {Results in Engineering},
  volume = {19},
  pages = {101267},
  issn = {2590-1230},
  doi = {10.1016/j.rineng.2023.101267},
  urldate = {2025-01-03},
  abstract = {Due to the increasing demand on road maintenance around the whole world, advanced techniques have been developed to automatically detect and segment pavement cracks. However, most of methods suffer from background noise or fail in fine crack segmentation. This paper proposes a generative adversarial network (GAN)-based neural network named CrackSegAN to segment pavement cracks automatically. The generator of CrackSegAN generates segmentation results, while the discriminator trains the generator adversarially. A joint loss function is proposed to optimize the generator with sufficient gradients and mitigate the high class imbalance in pavement crack images. Elastic deformation data augmentation method is applied to force CrackSegAN to learn the transformation invariance. The proposed CrackSegAN reaches an average F1 score of 0.9780 on CrackForest dataset and 0.8412 on Crack500 dataset. Ablation study shows that the most prominent difference is made by the proposed joint loss function which increases the average F1 score by 8.98\% on CrackForest dataset. Besides, the comparison between using different data augmentation strategies validates the effectiveness of elastic deformation. Overall, the proposed CrackSegAN increases the F1 score by 1.91\% on CrackForest dataset and 1.01\% on Crack500 compared with state-of-the-art methods. Qualitatively, CrackSegAN is more robust to background noises and segments cracks with more details. Moreover, the test on field data proves a better generalizability of CrackSegAN on unseen background noises.},
  keywords = {/unread,Deep learning,Fully convolutional network,Generative adversarial network (GAN),Pavement crack segmentation,Transportation safety},
  file = {C\:\\Users\\theun\\Zotero\\storage\\R96DV8BV\\Pan et al. - 2023 - Automatic pavement crack segmentation using a generative adversarial network (GAN)-based convolution.pdf;C\:\\Users\\theun\\Zotero\\storage\\63S9U4DZ\\S2590123023003948.html}
}

@article{panDeepDualResolutionNetworks2023,
  title = {Deep {{Dual-Resolution Networks}} for {{Real-Time}} and {{Accurate Semantic Segmentation}} of {{Traffic Scenes}}},
  author = {Pan, Huihui and Hong, Yuanduo and Sun, Weichao and Jia, Yisong},
  year = 2023,
  month = mar,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {24},
  number = {3},
  pages = {3448--3460},
  issn = {1558-0016},
  doi = {10.1109/TITS.2022.3228042},
  urldate = {2025-05-05},
  abstract = {Using light-weight architectures or reasoning on low-resolution images, recent methods realize very fast scene parsing, even running at more than 100 FPS on a single GPU. However, there is still a significant gap in performance between these real-time methods and the models based on dilation backbones. To this end, we proposed a family of deep dual-resolution networks (DDRNets) for real-time and accurate semantic segmentation, which consist of deep dual-resolution backbones and enhanced low-resolution contextual information extractors. The two deep branches and multiple bilateral fusions of backbones generate higher quality details compared to existing two-pathway methods. The enhanced contextual information extractor named Deep Aggregation Pyramid Pooling Module (DAPPM) enlarges effective receptive fields and fuses multi-scale context based on low-resolution feature maps with little time cost. Our method achieves a new state-of-the-art trade-off between accuracy and speed on both Cityscapes and CamVid dataset. For the input of full resolution, on a single 2080Ti GPU without hardware acceleration, DDRNet-23-slim yields 77.4\% mIoU at 102 FPS on Cityscapes test set and 74.7\% mIoU at 230 FPS on CamVid test set. With widely used test augmentation, our method is superior to most state-of-the-art models and requires much less computation. Codes and trained models are available at https://github.com/ydhongHIT/DDRNet.},
  keywords = {/unread,autonomous driving,Computer architecture,Data mining,deep convolutional neural networks,Feature extraction,real-time,Real-time systems,Semantic segmentation,Semantics,Task analysis},
  file = {C:\Users\theun\Zotero\storage\DABDV9BB\Pan et al. - 2023 - Deep Dual-Resolution Networks for Real-Time and Accurate Semantic Segmentation of Traffic Scenes.pdf}
}

@article{pandharipandeSensingMachineLearning2023,
  title = {Sensing and {{Machine Learning}} for {{Automotive Perception}}: {{A Review}}},
  shorttitle = {Sensing and {{Machine Learning}} for {{Automotive Perception}}},
  author = {Pandharipande, Ashish and Cheng, Chih-Hong and Dauwels, Justin and Gurbuz, Sevgi Z. and {Ibanez-Guzman}, Javier and Li, Guofa and Piazzoni, Andrea and Wang, Pu and Santra, Avik},
  year = 2023,
  month = jun,
  journal = {IEEE Sensors Journal},
  volume = {23},
  number = {11},
  pages = {11097--11115},
  issn = {1558-1748},
  doi = {10.1109/JSEN.2023.3262134},
  urldate = {2025-01-02},
  abstract = {Automotive perception involves understanding the external driving environment and the internal state of the vehicle cabin and occupants using sensor data. It is critical to achieving high levels of safety and autonomy in driving. This article provides an overview of different sensor modalities, such as cameras, radars, and light detection and ranging (LiDAR) used commonly for perception, along with the associated data processing techniques. Critical aspects of perception are considered, such as architectures for processing data from single or multiple sensor modalities, sensor data processing algorithms and the role of machine learning techniques, methodologies for validating the performance of perception systems, and safety. The technical challenges for each aspect are analyzed, emphasizing machine learning approaches, given their potential impact on improving perception. Finally, future research opportunities in automotive perception for their wider deployment are outlined.},
  keywords = {/unread,Advanced driver assistance system (ADAS),Automotive engineering,automotive perception,autonomous driving,cameras,Cameras,light detection and ranging (LiDAR),Monitoring,Planning,radars,safety,Safety,sensor data processing,Sensor systems,Sensors},
  file = {C\:\\Users\\theun\\Zotero\\storage\\MKSJXSX7\\Pandharipande et al. - 2023 - Sensing and Machine Learning for Automotive Perception A Review.pdf;C\:\\Users\\theun\\Zotero\\storage\\CABVU9L5\\10089400.html}
}

@article{panettaParameterizedLogarithmicFramework2011,
  title = {Parameterized {{Logarithmic Framework}} for {{Image Enhancement}}},
  author = {Panetta, Karen and Agaian, Sos and Zhou, Yicong and Wharton, Eric J.},
  year = 2011,
  month = apr,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  volume = {41},
  number = {2},
  pages = {460--473},
  issn = {1941-0492},
  doi = {10.1109/TSMCB.2010.2058847},
  urldate = {2025-05-27},
  abstract = {Image processing technologies such as image enhancement generally utilize linear arithmetic operations to manipulate images. Recently, Jourlin and Pinoli successfully used the logarithmic image processing (LIP) model for several applications of image processing such as image enhancement and segmentation. In this paper, we introduce a parameterized LIP (PLIP) model that spans both the linear arithmetic and LIP operations and all scenarios in between within a single unified model. We also introduce both frequency- and spatial-domain PLIP-based image enhancement methods, including the PLIP Lee's algorithm, PLIP bihistogram equalization, and the PLIP alpha rooting. Computer simulations and comparisons demonstrate that the new PLIP model allows the user to obtain improved enhancement performance by changing only the PLIP parameters, to yield better image fusion results by utilizing the PLIP addition or image multiplication, to represent a larger span of cases than the LIP and linear arithmetic cases by changing parameters, and to utilize and illustrate the logarithmic exponential operation for image fusion and enhancement.},
  keywords = {Alpha rooting (AR),Computational modeling,histogram equalization (HE),Humans,image enhancement,Image enhancement,Mathematical model,parameterized logarithmic image processing (PLIP),Pixel,Visual system},
  file = {C:\Users\theun\Zotero\storage\HJ6XTTSP\Panetta et al. - 2011 - Parameterized Logarithmic Framework for Image Enhancement.pdf}
}

@incollection{panExploringReliableMatching2025,
  title = {Exploring {{Reliable Matching}} with {{Phase Enhancement}} for {{Night-Time Semantic Segmentation}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2024},
  author = {Pan, Yuwen and Sun, Rui and Luo, Naisong and Zhang, Tianzhu and Zhang, Yongdong},
  editor = {Leonardis, Ale{\v s} and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, G{\"u}l},
  year = 2025,
  volume = {15111},
  pages = {408--424},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-73668-1_24},
  urldate = {2025-06-18},
  abstract = {Semantic segmentation of night-time images holds significant importance in computer vision, particularly for applications like night environment perception in autonomous driving systems. However, existing methods tend to parse night-time images from a day-time perspective, leaving the inherent challenges in low-light conditions (such as compromised texture and deceiving matching errors) unexplored. To address these issues, we propose a novel end-to-end optimized approach, named NightFormer, tailored for night-time semantic segmentation, avoiding the conventional practice of forcibly fitting night-time images into daytime distributions. Specifically, we design a pixel-level texture enhancement module to acquire texture-aware features hierarchically with phase enhancement and amplified attention, and an object-level reliable matching module to realize accurate association matching via reliable attention in low-light environments. Extensive experimental results on various challenging benchmarks including NightCity, BDD and Cityscapes demonstrate that our proposed method performs favorably against stateof-the-art night-time semantic segmentation methods.},
  isbn = {978-3-031-73667-4 978-3-031-73668-1},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\Z7AJAAKG\Pan et al. - 2025 - Exploring Reliable Matching with Phase Enhancement for Night-Time Semantic Segmentation.pdf}
}

@inproceedings{panickerAnalysisImageProcessing2021,
  title = {Analysis of {{Image Processing Techniques}} to {{Segment}} the {{Target Animal}} in {{Non-uniformly Illuminated}} and {{Occluded Images}}},
  booktitle = {Inventive {{Communication}} and {{Computational Technologies}}},
  author = {Panicker, Shruti Ajithkumar and Kumar, Rahul Vinod and Ramachandran, Aishwarya and Padmavathi, S.},
  editor = {Ranganathan, G. and Chen, Joy and Rocha, {\'A}lvaro},
  year = 2021,
  pages = {15--26},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-15-7345-3_2},
  abstract = {Non-uniformly illuminated images are a class of images that, from a subjective perspective, are difficult to analyze. The excess noise and the lack of properly defined boundaries all contribute to making these images a difficult dataset for any form of analysis or segmentation. This calls for proper feature extraction and specific enhancement to make these images ready for efficient information gathering. This paper aims to visualize the features that can be enhanced using image enhancement techniques to identify the target animal in a non-uniformly illuminated and occluded image, thereby enhancing the recognition power of the proposed system. This paper uses a method to approximately detect and locate the position of the animal in an image. Segmentation Using Region Adjacency Graphs, Interactive Foreground Extraction using GrabCut Algorithm and DeepLab model for semantic image segmentation have also been analyzed.},
  isbn = {978-981-15-7345-3},
  langid = {english},
  keywords = {Feature extraction,Image enhancement,Image segmentation,Low-light images,Non-uniform illumination},
  file = {C:\Users\theun\Zotero\storage\QXLLCSVG\Panicker et al. - 2021 - Analysis of Image Processing Techniques to Segment the Target Animal in Non-uniformly Illuminated an.pdf}
}

@misc{papandreouUntanglingLocalGlobal2014,
  title = {Untangling {{Local}} and {{Global Deformations}} in {{Deep Convolutional Networks}} for {{Image Classification}} and {{Sliding Window Detection}}},
  author = {Papandreou, George and Kokkinos, Iasonas and Savalle, Pierre-Andr{\'e}},
  year = 2014,
  month = nov,
  number = {arXiv:1412.0296},
  eprint = {1412.0296},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1412.0296},
  urldate = {2025-06-09},
  abstract = {Deep Convolutional Neural Networks (DCNNs) commonly use generic `max-pooling' (MP) layers to extract deformation-invariant features, but we argue in favor of a more refined treatment. First, we introduce epitomic convolution as a building block alternative to the common convolution-MP cascade of DCNNs; while having identical complexity to MP, Epitomic Convolution allows for parameter sharing across different filters, resulting in faster convergence and better generalization. Second, we introduce a Multiple Instance Learning approach to explicitly accommodate global translation and scaling when training a DCNN exclusively with class labels. For this we rely on a `patchwork' data structure that efficiently lays out all image scales and positions as candidates to a DCNN. Factoring global and local deformations allows a DCNN to `focus its resources' on the treatment of non-rigid deformations and yields a substantial classification accuracy improvement. Third, further pursuing this idea, we develop an efficient DCNN sliding window object detector that employs explicit search over position, scale, and aspect ratio. We provide competitive image classification and localization results on the ImageNet dataset and object detection results on the Pascal VOC 2007 benchmark.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\B7HL5Y6G\\Papandreou et al. - 2014 - Untangling Local and Global Deformations in Deep Convolutional Networks for Image Classification and.pdf;C\:\\Users\\theun\\Zotero\\storage\\FUEXPV36\\1412.html}
}

@article{parkContrastEnhancementLowlight2018,
  title = {{Contrast Enhancement for Low-light Image Enhancement: A Survey}},
  shorttitle = {{Contrast Enhancement for Low-light Image Enhancement}},
  author = {Park, Seonhee and Kim, Kiyeon and Yu, Soohwan and Paik, Joonki},
  year = 2018,
  month = feb,
  journal = {IEIE Transactions on Smart Processing \& Computing},
  volume = {7},
  number = {1},
  pages = {36--48},
  doi = {10.5573/IEIESPC.2018.7.1.036},
  urldate = {2025-05-26},
  abstract = {In this paper, various contrast and low-light image enhancement methods are described and classified into three categories: i) histogram-based, ii) transmission map--based, and iii) retinexbased. The performance of the image enhancement algorithms is evaluated by comparing the resulting images using low-contrast and low-light images acquired under different illumination conditions. Various image enhancement algorithms are analyzed for both performance and efficiency. The described image enhancement algorithms can be applied to various visual surveillance or video analytics applications with low-light, low-contrast video input.},
  langid = {korean},
  keywords = {Contrast Enhancement,Histogram Equalization,Low-Light Image,Retinex,Transmission Map},
  file = {C:\Users\theun\Zotero\storage\LF9393IH\articleDetail.html}
}

@misc{paulPerformanceAnalysisKeypoint2020,
  title = {Performance {{Analysis}} of {{Keypoint Detectors}} and {{Binary Descriptors Under Varying Degrees}} of {{Photometric}} and {{Geometric Transformations}}},
  author = {Paul, Shuvo Kumar and Hoseini, Pourya and Nicolescu, Mircea and Nicolescu, Monica},
  year = 2020,
  month = dec,
  number = {arXiv:2012.04135},
  eprint = {2012.04135},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.04135},
  urldate = {2025-01-03},
  abstract = {Detecting image correspondences by feature matching forms the basis of numerous computer vision applications. Several detectors and descriptors have been presented in the past, addressing the efficient generation of features from interest points (keypoints) in an image. In this paper, we investigate eight binary descriptors (AKAZE, BoostDesc, BRIEF, BRISK, FREAK, LATCH, LUCID, and ORB) and eight interest point detector (AGAST, AKAZE, BRISK, FAST, HarrisLapalce, KAZE, ORB, and StarDetector). We have decoupled the detection and description phase to analyze the interest point detectors and then evaluate the performance of the pairwise combination of different detectors and descriptors. We conducted experiments on a standard dataset and analyzed the comparative performance of each method under different image transformations. We observed that: (1) the FAST, AGAST, ORB detectors were faster and detected more keypoints, (2) the AKAZE and KAZE detectors performed better under photometric changes while ORB was more robust against geometric changes, (3) in general, descriptors performed better when paired with the KAZE and AKAZE detectors, (4) the BRIEF, LUCID, ORB descriptors were relatively faster, and (5) none of the descriptors did particularly well under geometric transformations, only BRISK, FREAK, and AKAZE showed reasonable resiliency.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\6Q5QL2UW\\Paul et al. - 2020 - Performance Analysis of Keypoint Detectors and Binary Descriptors Under Varying Degrees of Photometr.pdf;C\:\\Users\\theun\\Zotero\\storage\\4QGTNKV2\\2012.html}
}

@article{phongIlluminationComputerGenerated1975,
  title = {Illumination for Computer Generated Pictures},
  author = {Phong, Bui Tuong},
  year = 1975,
  month = jun,
  journal = {Commun. ACM},
  volume = {18},
  number = {6},
  pages = {311--317},
  issn = {0001-0782},
  doi = {10.1145/360825.360839},
  urldate = {2025-07-07},
  abstract = {The quality of computer generated images of three-dimensional scenes depends on the shading technique used to paint the objects on the cathode-ray tube screen. The shading algorithm itself depends in part on the method for modeling the object, which also determines the hidden surface algorithm. The various methods of object modeling, shading, and hidden surface removal are thus strongly interconnected. Several shading techniques corresponding to different methods of object modeling and the related hidden surface algorithms are presented here. Human visual perception and the fundamental laws of optics are considered in the development of a shading rule that provides better quality and increased realism in generated images.},
  file = {C:\Users\theun\Zotero\storage\495KKE9V\Phong - 1975 - Illumination for computer generated pictures.pdf}
}

@article{pizerAdaptiveHistogramEqualization1987,
  title = {Adaptive Histogram Equalization and Its Variations},
  author = {Pizer, Stephen M. and Amburn, E. Philip and Austin, John D. and Cromartie, Robert and Geselowitz, Ari and Greer, Trey and {ter Haar Romeny}, Bart and Zimmerman, John B. and Zuiderveld, Karel},
  year = 1987,
  month = sep,
  journal = {Computer Vision, Graphics, and Image Processing},
  volume = {39},
  number = {3},
  pages = {355--368},
  issn = {0734-189X},
  doi = {10.1016/S0734-189X(87)80186-X},
  urldate = {2025-02-27},
  abstract = {Adaptive histogram equalization (ahe) is a contrast enhancement method designed to be broadly applicable and having demonstrated effectiveness. However, slow speed and the overenhancement of noise it produces in relatively homogeneous regions are two problems. We report algorithms designed to overcome these and other concerns. These algorithms include interpolated ahe, to speed up the method on general purpose computers; a version of interpolated ahe designed to run in a few seconds on feedback processors; a version of full ahe designed to run in under one second on custom VLSI hardware; weighted ahe, designed to improve the quality of the result by emphasizing pixels' contribution to the histogram in relation to their nearness to the result pixel; and clipped ahe, designed to overcome the problem of overenhancement of noise contrast. We conclude that clipped ahe should become a method of choice in medical imaging and probably also in other areas of digital imaging, and that clipped ahe can be made adequately fast to be routinely applied in the normal display sequence.},
  file = {C\:\\Users\\theun\\Zotero\\storage\\LXT8MVHE\\Pizer et al. - 1987 - Adaptive histogram equalization and its variations.pdf;C\:\\Users\\theun\\Zotero\\storage\\D9ZILG7N\\S0734189X8780186X.html}
}

@inproceedings{plathMulticlassImageSegmentation2009,
  title = {Multi-Class Image Segmentation Using Conditional Random Fields and Global Classification},
  booktitle = {Proceedings of the 26th {{Annual International Conference}} on {{Machine Learning}}},
  author = {Plath, Nils and Toussaint, Marc and Nakajima, Shinichi},
  year = 2009,
  month = jun,
  series = {{{ICML}} '09},
  pages = {817--824},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1553374.1553479},
  urldate = {2025-06-06},
  abstract = {A key aspect of semantic image segmentation is to integrate local and global features for the prediction of local segment labels. We present an approach to multi-class segmentation which combines two methods for this integration: a Conditional Random Field (CRF) which couples to local image features and an image classification method which considers global features. The CRF follows the approach of Reynolds \&amp; Murphy (2007) and is based on an unsupervised multi scale pre-segmentation of the image into patches, where patch labels correspond to the random variables of the CRF. The output of the classifier is used to constraint this CRF. We demonstrate and compare the approach on a standard semantic segmentation data set.},
  isbn = {978-1-60558-516-1},
  file = {C:\Users\theun\Zotero\storage\834B5X6L\Plath et al. - 2009 - Multi-class image segmentation using conditional random fields and global classification.pdf}
}

@inproceedings{poggiUncertaintySelfSupervisedMonocular2020,
  title = {On the {{Uncertainty}} of {{Self-Supervised Monocular Depth Estimation}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Poggi, Matteo and Aleotti, Filippo and Tosi, Fabio and Mattoccia, Stefano},
  year = 2020,
  month = jun,
  pages = {3224--3234},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR42600.2020.00329},
  urldate = {2025-07-04},
  abstract = {Self-supervised paradigms for monocular depth estimation are very appealing since they do not require ground truth annotations at all. Despite the astonishing results yielded by such methodologies, learning to reason about the uncertainty of the estimated depth maps is of paramount importance for practical applications, yet uncharted in the literature. Purposely, we explore for the first time how to estimate the uncertainty for this task and how this affects depth accuracy, proposing a novel peculiar technique specifically designed for self-supervised approaches. On the standard KITTI dataset, we exhaustively assess the performance of each method with different self-supervised paradigms. Such evaluation highlights that our proposal i) always improves depth accuracy significantly and ii) yields state-of-the-art results concerning uncertainty estimation when training on sequences and competitive results uniquely deploying stereo pairs.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-7281-7168-5},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\3LBUBPUE\Poggi et al. - 2020 - On the Uncertainty of Self-Supervised Monocular Depth Estimation.pdf}
}

@inproceedings{ponomarevImageEnhancementHomomorphic1995,
  title = {Image Enhancement by Homomorphic Filters},
  booktitle = {Applications of {{Digital Image Processing XVIII}}},
  author = {Ponomarev, Vladimir Illich and Pogrebnyak, Oleksiy B.},
  year = 1995,
  month = aug,
  volume = {2564},
  pages = {153--159},
  publisher = {SPIE},
  doi = {10.1117/12.217396},
  urldate = {2025-05-29},
  abstract = {Homomorphic filter approach for image processing is very well known as a way for image dynamic range and increasing contrast. According to this approach, input signal is assumed to consist of two multiplicative components: background and details. The standard problem in processing such signals involves logarithm operation, division on two components by implementing low frequency and high-pass filters, addition of evaluations multiplied by different gain coefficients, and exponent calculation. In this paper we propose to use median filter for deriving multiplicative component evaluations. It was found that the proposed homomorphic filter has several useful properties in remote sensing image enhancement applications. Experimental results for simulated and real image processing are presented in the paper.},
  file = {C:\Users\theun\Zotero\storage\6AFAXGDK\Ponomarev and Pogrebnyak - 1995 - Image enhancement by homomorphic filters.pdf}
}

@article{provenziMathematicalDefinitionAnalysis2005,
  title = {Mathematical Definition and Analysis of the {{Retinex}} Algorithm},
  author = {Provenzi, Edoardo and Marini, Daniele and De Carli, Luca and Rizzi, Alessandro},
  year = 2005,
  month = dec,
  journal = {Journal of the Optical Society of America A},
  volume = {22},
  number = {12},
  pages = {2613},
  issn = {1084-7529, 1520-8532},
  doi = {10.1364/JOSAA.22.002613},
  urldate = {2025-05-29},
  copyright = {https://doi.org/10.1364/OA\_License\_v1\#VOR},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\ZKWBGEXR\Provenzi et al. - 2005 - Mathematical definition and analysis of the Retinex algorithm.pdf}
}

@article{provenziRandomSprayRetinex2007,
  title = {Random {{Spray Retinex}}: {{A New Retinex Implementation}} to {{Investigate}} the {{Local Properties}} of the {{Model}}},
  shorttitle = {Random {{Spray Retinex}}},
  author = {Provenzi, Edoardo and Fierro, Massimo and Rizzi, Alessandro and De Carli, Luca and Gadia, Davide and Marini, Daniele},
  year = 2007,
  month = jan,
  journal = {IEEE Transactions on Image Processing},
  volume = {16},
  number = {1},
  pages = {162--171},
  issn = {1941-0042},
  doi = {10.1109/TIP.2006.884946},
  urldate = {2025-05-29},
  abstract = {In order to investigate the local filtering behavior of the Retinex model, we propose a new implementation in which paths are replaced by 2-D pixel sprays, hence the name ``random spray Retinex.'' A peculiar feature of this implementation is the way its parameters can be controlled to perform spatial investigation. The parameters' tuning is accomplished by an unsupervised method based on quantitative measures. This procedure has been validated via user panel tests. Furthermore, the spray approach has faster performances than the path-wise one. Tests and results are presented and discussed.},
  keywords = {Filtering,Humans,Image sampling,Layout,Locality of color perception,Pixel,pixel sprays,Psychology,Retinex,Spraying,Testing,Visual system},
  file = {C:\Users\theun\Zotero\storage\F6ZN3X8Q\Provenzi et al. - 2007 - Random Spray Retinex A New Retinex Implementation to Investigate the Local Properties of the Model.pdf}
}

@article{qiComprehensiveOverviewImage2022,
  title = {A {{Comprehensive Overview}} of {{Image Enhancement Techniques}}},
  author = {Qi, Yunliang and Yang, Zhen and Sun, Wenhao and Lou, Meng and Lian, Jing and Zhao, Wenwei and Deng, Xiangyu and Ma, Yide},
  year = 2022,
  month = jan,
  journal = {Archives of Computational Methods in Engineering},
  volume = {29},
  number = {1},
  pages = {583--607},
  publisher = {Springer Netherlands},
  issn = {1886-1784},
  doi = {10.1007/s11831-021-09587-6},
  urldate = {2025-01-16},
  abstract = {Image enhancement plays an important role in improving image quality in the field of image processing, which is achieved by highlighting useful information and suppressing redundant information in the image. In this paper, the development of image enhancement algorithms is surveyed. The purpose of our review is to provide relevant researchers with a comprehensive and systematic analysis on image enhancement techniques and give them a valuable reference. Various image enhancement algorithms were mentioned and underlying difficulties, limitations, merits and disadvantages were discussed in applying these techniques in the past two decades with three aspects: supervised algorithm, unsupervised algorithm and quality evaluation, respectively. Further, we summarize some existing problems and analyze the future development trend of existing enhanced algorithms.},
  copyright = {2021 CIMNE, Barcelona, Spain},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\2C2N97SZ\Qi et al. - 2022 - A Comprehensive Overview of Image Enhancement Techniques.pdf}
}

@misc{qinGeometricTransformerFast2022,
  title = {Geometric {{Transformer}} for {{Fast}} and {{Robust Point Cloud Registration}}},
  author = {Qin, Zheng and Yu, Hao and Wang, Changjian and Guo, Yulan and Peng, Yuxing and Xu, Kai},
  year = 2022,
  month = mar,
  number = {arXiv:2202.06688},
  eprint = {2202.06688},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.06688},
  urldate = {2025-01-03},
  abstract = {We study the problem of extracting accurate correspondences for point cloud registration. Recent keypoint-free methods bypass the detection of repeatable keypoints which is difficult in low-overlap scenarios, showing great potential in registration. They seek correspondences over downsampled superpoints, which are then propagated to dense points. Superpoints are matched based on whether their neighboring patches overlap. Such sparse and loose matching requires contextual features capturing the geometric structure of the point clouds. We propose Geometric Transformer to learn geometric feature for robust superpoint matching. It encodes pair-wise distances and triplet-wise angles, making it robust in low-overlap cases and invariant to rigid transformation. The simplistic design attains surprisingly high matching accuracy such that no RANSAC is required in the estimation of alignment transformation, leading to \$100\$ times acceleration. Our method improves the inlier ratio by \$17\textbraceleft\textbackslash sim\textbraceright 30\$ percentage points and the registration recall by over \$7\$ points on the challenging 3DLoMatch benchmark. Our code and models are available at https://github.com/qinzheng93/GeoTransformer.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\JBZJQ3R6\\Qin et al. - 2022 - Geometric Transformer for Fast and Robust Point Cloud Registration.pdf;C\:\\Users\\theun\\Zotero\\storage\\V8ICN2E9\\2202.html}
}

@article{qinRectifiedSelfsupervisedMonocular2025,
  title = {Rectified Self-Supervised Monocular Depth Estimation Loss for Nighttime and Dynamic Scenes},
  author = {Qin, Xiaofei and Wang, Lin and Zhu, Yongchao and Mao, Fan and Zhang, Xuedian and He, Changxiang and Dong, Qiulei},
  year = 2025,
  month = mar,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {144},
  pages = {110026},
  issn = {09521976},
  doi = {10.1016/j.engappai.2025.110026},
  urldate = {2025-06-30},
  abstract = {Self-supervised monocular depth estimation has attracted much attention in computer vision recently. However, most existing methods assume that the scenes are static and the photometric is consistent, so that their performances tend to degrade significantly in nighttime and dynamic scenes. To address this issue, this paper proposes a self-supervised monocular depth estimation model to tackle two challenges. One is the drastic photometric changes problem due to underexposure of distant areas in nighttime scenes, the other is the moving objects problem in dynamic scenes. In the proposed model, an Effective Area Photometric Loss function (EAPL) is designed which is gated by the Effective Area Mask (EAM) and Potentially Moving Objects Mask (PMOM). Then, a motion flow network is introduced to estimate the motion of moving objects, and a Motion Flow Loss function (MFL) is proposed based on three facts, i.e., the motion flow of static objects should be zero, most moving objects in autonomous driving scenarios are approximately rigid objects, and the relative motion flows between consecutive frames should be mutually inverse. Finally, a decoupled training approach is provided to facilitate the optimization process of the model. Experimental results show that our model achieves state-of-the-art or second best performance on the nuTonomy Scenes (nuScenes) and Dense Depth for Autonomous Driving (DDAD) dataset which contains many nighttime or dynamic scenes, and also achieves competitive performance on the Karlsruhe Institute of Technology and Toyota Technological Institute at Chicago (KITTI) dataset which is dominated by daytime and static scenes. Codes are available at https://github.com/pandaswfas/effdepth.},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\8C4BM23E\Qin et al. - 2025 - Rectified self-supervised monocular depth estimation loss for nighttime and dynamic scenes.pdf}
}

@inproceedings{qiPointNetDeepHierarchical2017,
  title = {{{PointNet}}++: {{Deep Hierarchical Feature Learning}} on {{Point Sets}} in a {{Metric Space}}},
  shorttitle = {{{PointNet}}++},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Qi, Charles Ruizhongtai and Yi, Li and Su, Hao and Guibas, Leonidas J},
  year = 2017,
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-01-03},
  abstract = {Few prior works study deep learning on point sets. PointNet is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\6BEST9FF\Qi et al. - 2017 - PointNet++ Deep Hierarchical Feature Learning on Point Sets in a Metric Space.pdf}
}

@article{quNoctuDroneNetRealTimeSemantic2025,
  title = {{{NoctuDroneNet}}: {{Real-Time Semantic Segmentation}} of {{Nighttime UAV Imagery}} in {{Complex Environments}}},
  shorttitle = {{{NoctuDroneNet}}},
  author = {Qu, Ruokun and Tan, Jintao and Liu, Yelu and Li, Chenglong and Jiang, Hui},
  year = 2025,
  month = feb,
  journal = {Drones},
  volume = {9},
  number = {2},
  pages = {97},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2504-446X},
  doi = {10.3390/drones9020097},
  urldate = {2025-06-18},
  abstract = {Nighttime semantic segmentation represents a challenging frontier in computer vision, made particularly difficult by severe low-light conditions, pronounced noise, and complex illumination patterns. These challenges intensify when dealing with Unmanned Aerial Vehicle (UAV) imagery, where varying camera angles and altitudes compound the difficulty. In this paper, we introduce NoctuDroneNet (Nocturnal UAV Drone Network, hereinafter referred to as NoctuDroneNet), a real-time segmentation model tailored specifically for nighttime UAV scenarios. Our approach integrates convolution-based global reasoning with training-only semantic alignment modules to effectively handle diverse and extreme nighttime conditions. We construct a new dataset, NUI-Night, focusing on low-illumination UAV scenes to rigorously evaluate performance under conditions rarely represented in standard benchmarks. Beyond NUI-Night, we assess NoctuDroneNet on the Varied Drone Dataset (VDD), a normal-illumination UAV dataset, demonstrating the model's robustness and adaptability to varying flight domains despite the lack of large-scale low-light UAV benchmarks. Furthermore, evaluations on the Night-City dataset confirm its scalability and applicability to complex nighttime urban environments. NoctuDroneNet achieves state-of-the-art performance on NUI-Night, surpassing strong real-time baselines in both segmentation accuracy and speed. Qualitative analyses highlight its resilience to under-/over-exposure and small-object detection, underscoring its potential for real-world applications like UAV emergency landings under minimal illumination.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Aerial photography,Aerial vehicle,Benchmarking,Compound helicopters,Drones,Illumination conditions,Low illuminations,Low-illumination condition,low-illumination conditions,nighttime semantic segmentation,Nighttime semantic segmentation,Noctudronenet,NoctuDroneNet,NUI-night dataset,NUI-Night dataset,Photomapping,real-time segmentation,Real-time segmentation,Semantic segmentation,Semantic Segmentation,UAV imagery,Unmanned aerial vehicle imagery},
  file = {C:\Users\theun\Zotero\storage\4MYB3ZIV\Qu et al. - 2025 - NoctuDroneNet Real-Time Semantic Segmentation of Nighttime UAV Imagery in Complex Environments.pdf}
}

@inproceedings{rahmanMultiscaleRetinexColor1996,
  title = {Multi-Scale Retinex for Color Image Enhancement},
  booktitle = {Proceedings of 3rd {{IEEE International Conference}} on {{Image Processing}}},
  author = {Rahman, Z. and Jobson, D.J. and Woodell, G.A.},
  year = 1996,
  month = sep,
  volume = {3},
  pages = {1003-1006 vol.3},
  doi = {10.1109/ICIP.1996.560995},
  urldate = {2025-03-06},
  abstract = {The retinex is a human perception-based image processing algorithm which provides color constancy and dynamic range compression. We have previously reported on a single-scale retinex (SSR) and shown that it can either achieve color/lightness rendition or dynamic range compression, but not both simultaneously. We now present a multi-scale retinex (MSR) which overcomes this limitation for most scenes. Both color rendition and dynamic range compression are successfully accomplished except for some "pathological" scenes that have very strong spectral characteristics in a single band.},
  keywords = {Color,Contracts,Costs,Dynamic range,Humans,Image coding,Image enhancement,Layout,NASA,Very large scale integration},
  file = {C\:\\Users\\theun\\Zotero\\storage\\YRUNJP8Z\\Rahman et al. - 1996 - Multi-scale retinex for color image enhancement.pdf;C\:\\Users\\theun\\Zotero\\storage\\NAJ5KRTT\\560995.html}
}

@article{rahmanRetinexProcessingAutomatic2004,
  title = {Retinex Processing for Automatic Image Enhancement},
  author = {Rahman, Zia-ur and Jobson, Daniel J. and Woodell, Glenn A.},
  year = 2004,
  month = jan,
  journal = {Journal of Electronic Imaging},
  volume = {13},
  number = {1},
  pages = {100--110},
  publisher = {SPIE},
  issn = {1017-9909, 1560-229X},
  doi = {10.1117/1.1636183},
  urldate = {2025-03-06},
  abstract = {The Journal of Electronic Imaging publishes papers that are normally considered in the design, engineering, and applications of electronic imaging technologies.},
  file = {C:\Users\theun\Zotero\storage\RDA2BXSM\Rahman et al. - 2004 - Retinex processing for automatic image enhancement.pdf}
}

@article{rajapakshaDeepLearningbasedDepth2024,
  title = {Deep {{Learning-based Depth Estimation Methods}} from {{Monocular Image}} and {{Videos}}: {{A Comprehensive Survey}}},
  shorttitle = {Deep {{Learning-based Depth Estimation Methods}} from {{Monocular Image}} and {{Videos}}},
  author = {Rajapaksha, Uchitha and Sohel, Ferdous and Laga, Hamid and Diepeveen, Dean and Bennamoun, Mohammed},
  year = 2024,
  month = oct,
  journal = {ACM Comput. Surv.},
  volume = {56},
  number = {12},
  pages = {315:1--315:51},
  issn = {0360-0300},
  doi = {10.1145/3677327},
  urldate = {2025-05-29},
  abstract = {Estimating depth from single RGB images and videos is of widespread interest due to its applications in many areas, including autonomous driving, 3D reconstruction, digital entertainment, and robotics. More than 500 deep learning-based papers have been published in the past 10 years, which indicates the growing interest in the task. This paper presents a comprehensive survey of the existing deep learning-based methods, the challenges they address, and how they have evolved in their architecture and supervision methods. It provides a taxonomy for classifying the current work based on their input and output modalities, network architectures, and learning methods. It also discusses the major milestones in the history of monocular depth estimation, and different pipelines, datasets, and evaluation metrics used in existing methods.},
  file = {C:\Users\theun\Zotero\storage\6LHMJH32\Rajapaksha et al. - 2024 - Deep Learning-based Depth Estimation Methods from Monocular Image and Videos A Comprehensive Survey.pdf}
}

@article{ranftlRobustMonocularDepth2022,
  title = {Towards {{Robust Monocular Depth Estimation}}: {{Mixing Datasets}} for {{Zero-Shot Cross-Dataset Transfer}}},
  shorttitle = {Towards {{Robust Monocular Depth Estimation}}},
  author = {Ranftl, Ren{\'e} and Lasinger, Katrin and Hafner, David and Schindler, Konrad and Koltun, Vladlen},
  year = 2022,
  month = mar,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {3},
  pages = {1623--1637},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2020.3019967},
  urldate = {2025-06-29},
  abstract = {The success of monocular depth estimation relies on large and diverse training sets. Due to the challenges associated with acquiring dense ground-truth depth across different environments at scale, a number of datasets with distinct characteristics and biases have emerged. We develop tools that enable mixing multiple datasets during training, even if their annotations are incompatible. In particular, we propose a robust training objective that is invariant to changes in depth range and scale, advocate the use of principled multi-objective learning to combine data from different sources, and highlight the importance of pretraining encoders on auxiliary tasks. Armed with these tools, we experiment with five diverse training datasets, including a new, massive data source: 3D films. To demonstrate the generalization power of our approach we use zero-shot cross-dataset transfer, i.e. we evaluate on datasets that were not seen during training. The experiments confirm that mixing data from complementary sources greatly improves monocular depth estimation. Our approach clearly outperforms competing methods across diverse datasets, setting a new state of the art for monocular depth estimation.},
  keywords = {Cameras,Estimation,Measurement,Monocular depth estimation,Motion pictures,multi-dataset training,single-image depth prediction,Three-dimensional displays,Training,Videos,zero-shot cross-dataset transfer},
  file = {C:\Users\theun\Zotero\storage\N8IFEY3S\Ranftl et al. - 2022 - Towards Robust Monocular Depth Estimation Mixing Datasets for Zero-Shot Cross-Dataset Transfer.pdf}
}

@article{rasheedEmpiricalStudyRetinex2022,
  title = {An {{Empirical Study}} on {{Retinex Methods}} for {{Low-Light Image Enhancement}}},
  author = {Rasheed, Muhammad Tahir and Guo, Guiyu and Shi, Daming and Khan, Hufsa and Cheng, Xiaochun},
  year = 2022,
  month = jan,
  journal = {Remote Sensing},
  volume = {14},
  number = {18},
  pages = {4608},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2072-4292},
  doi = {10.3390/rs14184608},
  urldate = {2025-01-03},
  abstract = {A key part of interpreting, visualizing, and monitoring the surface conditions of remote-sensing images is enhancing the quality of low-light images. It aims to produce higher contrast, noise-suppressed, and better quality images from the low-light version. Recently, Retinex theory-based enhancement methods have gained a lot of attention because of their robustness. In this study, Retinex-based low-light enhancement methods are compared to other state-of-the-art low-light enhancement methods to determine their generalization ability and computational costs. Different commonly used test datasets covering different content and lighting conditions are used to compare the robustness of Retinex-based methods and other low-light enhancement techniques. Different evaluation metrics are used to compare the results, and an average ranking system is suggested to rank the enhancement methods.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {deep learning,low-light image enhancement,remote-sensing,retinex theory},
  file = {C:\Users\theun\Zotero\storage\HIPNZIYH\Rasheed et al. - 2022 - An Empirical Study on Retinex Methods for Low-Light Image Enhancement.pdf}
}

@inproceedings{readingCategoricalDepthDistribution2021,
  title = {Categorical {{Depth Distribution Network}} for {{Monocular 3D Object Detection}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Reading, Cody and Harakeh, Ali and Chae, Julia and Waslander, Steven L.},
  year = 2021,
  month = jun,
  pages = {8551--8560},
  publisher = {IEEE},
  address = {Nashville, TN, USA},
  doi = {10.1109/CVPR46437.2021.00845},
  urldate = {2025-01-13},
  abstract = {Monocular 3D object detection is a key problem for autonomous vehicles, as it provides a solution with simple configuration compared to typical multi-sensor systems. The main challenge in monocular 3D detection lies in accurately predicting object depth, which must be inferred from object and scene cues due to the lack of direct range measurement. Many methods attempt to directly estimate depth to assist in 3D detection, but show limited performance as a result of depth inaccuracy. Our proposed solution, Categorical Depth Distribution Network (CaDDN), uses a predicted categorical depth distribution for each pixel to project rich contextual feature information to the appropriate depth interval in 3D space. We then use the computationally efficient bird's-eye-view projection and single-stage detector to produce the final output detections. We design CaDDN as a fully differentiable end-to-end approach for joint depth estimation and object detection. We validate our approach on the KITTI 3D object detection benchmark, where we rank 1st among published monocular methods. We also provide the first monocular 3D detection results on the newly released Waymo Open Dataset. We provide a code release for CaDDN which is made available.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-6654-4509-2},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\ARA4P452\Reading et al. - 2021 - Categorical Depth Distribution Network for Monocular 3D Object Detection.pdf}
}

@inproceedings{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year = 2016,
  month = jun,
  pages = {779--788},
  publisher = {IEEE},
  address = {Las Vegas, NV, USA},
  doi = {10.1109/CVPR.2016.91},
  urldate = {2024-12-06},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\I7VVFNP6\Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Detection.pdf}
}

@article{renLightweightObjectDetection2023,
  title = {A Lightweight Object Detection Network in Low-Light Conditions Based on Depthwise Separable Pyramid Network and Attention Mechanism on Embedded Platforms},
  author = {Ren, Kun and Tao, Qingyang and Han, Honggui},
  year = 2023,
  month = apr,
  journal = {Journal of the Franklin Institute},
  volume = {360},
  number = {6},
  pages = {4427--4455},
  issn = {0016-0032},
  doi = {10.1016/j.jfranklin.2023.02.023},
  urldate = {2025-02-17},
  abstract = {Deep learning-based object detection algorithms have been widely used and are effective in autonomous driving. But their performance degrades dramatically under low-light conditions. The existing mitigation approach for low-light object detection uses image enhancement preprocessing to improve detection performance with limited success. In addition, this preprocessing approach incurs extra computation costs, consumes more resources, and makes it more challenging to implement the algorithm on embedded platforms. This paper proposes a novel object detection network in low-light conditions for embedded platforms. The new system consists of two innovative modules, including a lightweight low-light enhancement network DS-PyLENet and an anchor-free lightweight object detector CFEDet. DS-PyLENet is a three-level pyramid network configured with depthwise separable convolution residual blocks (DSCRBs) and multiscale DSCRBs. The CFEDet is configured with an improved EfficientNet backbone network with Coordinate Attention (CA) and vanilla MBconv, i.e., CA-fused EfficientNet, an improved Path Aggregation Network (PAN) fusion module, and an anchor-free Global Focal Loss (GFL) detection head. Two evaluation metrics, relative recall ratios, are proposed to depict the effectiveness of image enhancement on object detection more intuitively. The new model is demonstrated to be superior to the assembled model of EnlightenGAN and YOLOv5n in terms of detection accuracy and speed on GPU and embedded platforms. The testing results of the new model reveal that 83.5\% mAP is achieved on the Exdark dataset and 67.4\% mAP on the DarkFace dataset. The running rates of GTX 1080Ti and NVIDIA Jetson Xavier NX are 22 FPS and 2.6 FPS, respectively. The recall of CFEDet with DS-PyLENet increased to 22.1\% and significantly improved over 18\% with Zero-DCE and 17.2\% with EnlightenGAN.},
  file = {C\:\\Users\\theun\\Zotero\\storage\\8Z8E6CJR\\Ren et al. - 2023 - A lightweight object detection network in low-light conditions based on depthwise separable pyramid.pdf;C\:\\Users\\theun\\Zotero\\storage\\9LCNBXH5\\S0016003223001047.html}
}

@article{renLR3MRobustLowLight2020,
  title = {{{LR3M}}: {{Robust Low-Light Enhancement}} via {{Low-Rank Regularized Retinex Model}}},
  shorttitle = {{{LR3M}}},
  author = {Ren, Xutong and Yang, Wenhan and Cheng, Wen-Huang and Liu, Jiaying},
  year = 2020,
  journal = {IEEE Transactions on Image Processing},
  volume = {29},
  pages = {5862--5876},
  issn = {1941-0042},
  doi = {10.1109/TIP.2020.2984098},
  urldate = {2025-01-18},
  abstract = {Noise causes unpleasant visual effects in low-light image/video enhancement. In this paper, we aim to make the enhancement model and method aware of noise in the whole process. To deal with heavy noise which is not handled in previous methods, we introduce a robust low-light enhancement approach, aiming at well enhancing low-light images/videos and suppressing intensive noise jointly. Our method is based on the proposed Low-Rank Regularized Retinex Model (LR3M), which is the first to inject low-rank prior into a Retinex decomposition process to suppress noise in the reflectance map. Our method estimates a piece-wise smoothed illumination and a noise-suppressed reflectance sequentially, avoiding remaining noise in the illumination and reflectance maps which are usually presented in alternative decomposition methods. After getting the estimated illumination and reflectance, we adjust the illumination layer and generate our enhancement result. Furthermore, we apply our LR3M to video low-light enhancement. We consider inter-frame coherence of illumination maps and find similar patches through reflectance maps of successive frames to form the low-rank prior to make use of temporal correspondence. Our method performs well for a wide variety of images and videos, and achieves better quality both in enhancing and denoising, compared with the state-of-the-art methods.},
  keywords = {denoising,Estimation,Histograms,Lighting,Low-light enhancement,low-rank decomposition,Minimization,Noise reduction,retinex model,Robustness,Visualization},
  file = {C\:\\Users\\theun\\Zotero\\storage\\B9N8B48Z\\Ren et al. - 2020 - LR3M Robust Low-Light Enhancement via Low-Rank Regularized Retinex Model.pdf;C\:\\Users\\theun\\Zotero\\storage\\B97N7FVX\\9056796.html}
}

@inproceedings{romeraBridgingDayNight2019,
  title = {Bridging the {{Day}} and {{Night Domain Gap}} for {{Semantic Segmentation}}},
  booktitle = {2019 {{IEEE Intelligent Vehicles Symposium}} ({{IV}})},
  author = {Romera, Eduardo and Bergasa, Luis M. and Yang, Kailun and Alvarez, Jose M. and Barea, Rafael},
  year = 2019,
  month = jun,
  pages = {1312--1318},
  issn = {2642-7214},
  doi = {10.1109/IVS.2019.8813888},
  urldate = {2025-06-16},
  abstract = {Perception in autonomous vehicles has progressed exponentially in the last years thanks to the advances of vision-based methods such as Convolutional Neural Networks (CNNs). Current deep networks are both efficient and reliable, at least in standard conditions, standing as a suitable solution for the perception tasks of autonomous vehicles. However, there is a large accuracy downgrade when these methods are taken to adverse conditions such as nighttime. In this paper, we study methods to alleviate this accuracy gap by using recent techniques such as Generative Adversarial Networks (GANs). We explore diverse options such as enlarging the dataset to cover these domains in unsupervised training or adapting the images on-the-fly during inference to a comfortable domain such as sunny daylight in a pre-processing step. The results show some interesting insights and demonstrate that both proposed approaches considerably reduce the domain gap, allowing IV perception systems to work reliably also at night.},
  file = {C:\Users\theun\Zotero\storage\4UJICNU8\Romera et al. - 2019 - Bridging the Day and Night Domain Gap for Semantic Segmentation.pdf}
}

@article{romeraERFNetEfficientResidual2018,
  title = {{{ERFNet}}: {{Efficient Residual Factorized ConvNet}} for {{Real-Time Semantic Segmentation}}},
  shorttitle = {{{ERFNet}}},
  author = {Romera, Eduardo and {\'A}lvarez, Jos{\'e} M. and Bergasa, Luis M. and Arroyo, Roberto},
  year = 2018,
  month = jan,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {19},
  number = {1},
  pages = {263--272},
  issn = {1558-0016},
  doi = {10.1109/TITS.2017.2750080},
  urldate = {2025-06-19},
  abstract = {Semantic segmentation is a challenging task that addresses most of the perception needs of intelligent vehicles (IVs) in an unified way. Deep neural networks excel at this task, as they can be trained end-to-end to accurately classify multiple object categories in an image at pixel level. However, a good tradeoff between high quality and computational resources is yet not present in the state-of-the-art semantic segmentation approaches, limiting their application in real vehicles. In this paper, we propose a deep architecture that is able to run in real time while providing accurate semantic segmentation. The core of our architecture is a novel layer that uses residual connections and factorized convolutions in order to remain efficient while retaining remarkable accuracy. Our approach is able to run at over 83 FPS in a single Titan X, and 7 FPS in a Jetson TX1 (embedded device). A comprehensive set of experiments on the publicly available Cityscapes data set demonstrates that our system achieves an accuracy that is similar to the state of the art, while being orders of magnitude faster to compute than other architectures that achieve top precision. The resulting tradeoff makes our model an ideal approach for scene understanding in IV applications. The code is publicly available at: https://github.com/Eromera/erfnet.},
  keywords = {Computer architecture,deep learning,Image segmentation,Intelligent vehicles,Kernel,real-time,Real-time systems,residual layers,scene understanding,semantic segmentation,Semantics,Two dimensional displays},
  file = {C:\Users\theun\Zotero\storage\XK6JBRV2\Romera et al. - 2018 - ERFNet Efficient Residual Factorized ConvNet for Real-Time Semantic Segmentation.pdf}
}

@inproceedings{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  booktitle = {Medical {{Image Computing}} and {{Computer-Assisted Intervention}} -- {{MICCAI}} 2015},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = 2015,
  pages = {234--241},
  publisher = {Springer, Cham},
  issn = {1611-3349},
  doi = {10.1007/978-3-319-24574-4_28},
  urldate = {2025-06-05},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples...},
  isbn = {978-3-319-24574-4},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\YNFUQ6FQ\Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf}
}

@inproceedings{rosSYNTHIADatasetLarge2016,
  title = {The {{SYNTHIA Dataset}}: {{A Large Collection}} of {{Synthetic Images}} for {{Semantic Segmentation}} of {{Urban Scenes}}},
  shorttitle = {The {{SYNTHIA Dataset}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Ros, German and Sellart, Laura and Materzynska, Joanna and Vazquez, David and Lopez, Antonio M.},
  year = 2016,
  month = jun,
  pages = {3234--3243},
  publisher = {IEEE},
  address = {Las Vegas, NV, USA},
  doi = {10.1109/CVPR.2016.352},
  urldate = {2025-05-15},
  abstract = {Vision-based semantic segmentation in urban scenarios is a key functionality for autonomous driving. Recent revolutionary results of deep convolutional neural networks (DCNNs) foreshadow the advent of reliable classifiers to perform such visual tasks. However, DCNNs require learning of many parameters from raw images; thus, having a sufficient amount of diverse images with class annotations is needed. These annotations are obtained via cumbersome, human labour which is particularly challenging for semantic segmentation since pixel-level annotations are required. In this paper, we propose to use a virtual world to automatically generate realistic synthetic images with pixel-level annotations. Then, we address the question of how useful such data can be for semantic segmentation -- in particular, when using a DCNN paradigm. In order to answer this question we have generated a synthetic collection of diverse urban images, named SYNTHIA, with automatically generated class annotations. We use SYNTHIA in combination with publicly available real-world urban images with manually provided annotations. Then, we conduct experiments with DCNNs that show how the inclusion of SYNTHIA in the training stage significantly improves performance on the semantic segmentation task.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\SIFR7KPA\Ros et al. - 2016 - The SYNTHIA Dataset A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scene.pdf}
}

@inproceedings{sakaridisACDCAdverseConditions2021,
  title = {{{ACDC}}: {{The Adverse Conditions Dataset With Correspondences}} for {{Semantic Driving Scene Understanding}}},
  shorttitle = {{{ACDC}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Sakaridis, Christos and Dai, Dengxin and Van Gool, Luc},
  year = 2021,
  pages = {10765--10775},
  urldate = {2025-06-15},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\W2NN6XED\Sakaridis et al. - 2021 - ACDC The Adverse Conditions Dataset With Correspondences for Semantic Driving Scene Understanding.pdf}
}

@misc{sakaridisGuidedCurriculumModel2019,
  title = {Guided {{Curriculum Model Adaptation}} and {{Uncertainty-Aware Evaluation}} for {{Semantic Nighttime Image Segmentation}}},
  author = {Sakaridis, Christos and Dai, Dengxin and Gool, Luc Van},
  year = 2019,
  month = jul,
  number = {arXiv:1901.05946},
  eprint = {1901.05946},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1901.05946},
  urldate = {2025-02-27},
  abstract = {Most progress in semantic segmentation reports on daytime images taken under favorable illumination conditions. We instead address the problem of semantic segmentation of nighttime images and improve the state-of-the-art, by adapting daytime models to nighttime without using nighttime annotations. Moreover, we design a new evaluation framework to address the substantial uncertainty of semantics in nighttime images. Our central contributions are: 1) a curriculum framework to gradually adapt semantic segmentation models from day to night via labeled synthetic images and unlabeled real images, both for progressively darker times of day, which exploits cross-time-of-day correspondences for the real images to guide the inference of their labels; 2) a novel uncertainty-aware annotation and evaluation framework and metric for semantic segmentation, designed for adverse conditions and including image regions beyond human recognition capability in the evaluation in a principled fashion; 3) the Dark Zurich dataset, which comprises 2416 unlabeled nighttime and 2920 unlabeled twilight images with correspondences to their daytime counterparts plus a set of 151 nighttime images with fine pixel-level annotations created with our protocol, which serves as a first benchmark to perform our novel evaluation. Experiments show that our guided curriculum adaptation significantly outperforms state-of-the-art methods on real nighttime sets both for standard metrics and our uncertainty-aware metric. Furthermore, our uncertainty-aware evaluation reveals that selective invalidation of predictions can lead to better results on data with ambiguous content such as our nighttime benchmark and profit safety-oriented applications which involve invalid inputs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\V5FZMGC8\\Sakaridis et al. - 2019 - Guided Curriculum Model Adaptation and Uncertainty-Aware Evaluation for Semantic Nighttime Image Seg.pdf;C\:\\Users\\theun\\Zotero\\storage\\3DD82V87\\1901.html}
}

@article{sakaridisMapGuidedCurriculumDomain2022,
  title = {Map-{{Guided Curriculum Domain Adaptation}} and {{Uncertainty-Aware Evaluation}} for {{Semantic Nighttime Image Segmentation}}},
  author = {Sakaridis, Christos and Dai, Dengxin and Van Gool, Luc},
  year = 2022,
  month = jun,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {6},
  pages = {3139--3153},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2020.3045882},
  urldate = {2025-06-15},
  abstract = {We address the problem of semantic nighttime image segmentation and improve the state-of-the-art, by adapting daytime models to nighttime without using nighttime annotations. Moreover, we design a new evaluation framework to address the substantial uncertainty of semantics in nighttime images. Our central contributions are: 1) a curriculum framework to gradually adapt semantic segmentation models from day to night through progressively darker times of day, exploiting cross-time-of-day correspondences between daytime images from a reference map and dark images to guide the label inference in the dark domains; 2) a novel uncertainty-aware annotation and evaluation framework and metric for semantic segmentation, including image regions beyond human recognition capability in the evaluation in a principled fashion; 3) the Dark Zurich dataset, comprising 2416 unlabeled nighttime and 2920 unlabeled twilight images with correspondences to their daytime counterparts plus a set of 201 nighttime images with fine pixel-level annotations created with our protocol, which serves as a first benchmark for our novel evaluation. Experiments show that our map-guided curriculum adaptation significantly outperforms state-of-the-art methods on nighttime sets both for standard metrics and our uncertainty-aware metric. Furthermore, our uncertainty-aware evaluation reveals that selective invalidation of predictions can improve results on data with ambiguous content such as our benchmark and profit safety-oriented applications involving invalid inputs.},
  keywords = {Adaptation models,Annotations,curriculum learning,Domain adaptation,evaluation,Image segmentation,Lighting,Measurement,nighttime,semantic segmentation,Semantics,Task analysis},
  file = {C:\Users\theun\Zotero\storage\BFPRPQMH\Sakaridis et al. - 2022 - Map-Guided Curriculum Domain Adaptation and Uncertainty-Aware Evaluation for Semantic Nighttime Imag.pdf}
}

@inproceedings{scharsteinHighResolutionStereoDatasets2014,
  title = {High-{{Resolution Stereo Datasets}} with {{Subpixel-Accurate Ground Truth}}},
  booktitle = {Pattern {{Recognition}}},
  author = {Scharstein, Daniel and Hirschm{\"u}ller, Heiko and Kitajima, York and Krathwohl, Greg and Ne{\v s}i{\'c}, Nera and Wang, Xi and Westling, Porter},
  year = 2014,
  pages = {31--42},
  publisher = {Springer, Cham},
  issn = {1611-3349},
  doi = {10.1007/978-3-319-11752-2_3},
  urldate = {2025-05-06},
  abstract = {We present a structured lighting system for creating high-resolution stereo datasets of static indoor scenes with highly accurate ground-truth disparities. The system includes novel techniques for efficient 2D subpixel correspondence search and self-calibration of...},
  isbn = {978-3-319-11752-2},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\KMVYM5I3\Scharstein et al. - 2014 - High-Resolution Stereo Datasets with Subpixel-Accurate Ground Truth.pdf}
}

@inproceedings{schopsMultiViewStereoBenchmark2017,
  title = {A {{Multi-View Stereo Benchmark With High-Resolution Images}} and {{Multi-Camera Videos}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Schops, Thomas and Schonberger, Johannes L. and Galliani, Silvano and Sattler, Torsten and Schindler, Konrad and Pollefeys, Marc and Geiger, Andreas},
  year = 2017,
  pages = {3260--3269},
  urldate = {2025-07-03},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\3FTVGAYR\Schops et al. - 2017 - A Multi-View Stereo Benchmark With High-Resolution Images and Multi-Camera Videos.pdf}
}

@misc{schuelerOverlapawareSegmentationTopological2025,
  title = {Overlap-Aware Segmentation for Topological Reconstruction of Obscured Objects},
  author = {Schueler, J. and Ara{\'u}jo, H. M. and Balashov, S. N. and Borg, J. E. and Brew, C. and Brunbauer, F. M. and Cazzaniga, C. and Cottle, A. and Edgeman, D. and Frost, C. D. and Garcia, F. and Hunt, D. and Kastriotou, M. and Knights, P. and Kraus, H. and Lindote, A. and Lisowska, M. and Loomba, D. and Asamar, E. Lopez and Majewski, P. A. and Marley, T. and McCabe, C. and Millins, L. and Nandakumar, R. and Neep, T. and Neves, F. and Nikolopoulos, K. and Oliveri, E. and Roy, A. and Sumner, T. J. and Tilly, E. and Thompson, W. and Vogiatzi, M. A.},
  year = 2025,
  month = oct,
  number = {arXiv:2510.06194},
  eprint = {2510.06194},
  primaryclass = {hep-ex},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2510.06194},
  urldate = {2025-10-17},
  abstract = {The separation of overlapping objects presents a significant challenge in scientific imaging. While deep learning segmentation-regression algorithms can predict pixel-wise intensities, they typically treat all regions equally rather than prioritizing overlap regions where attribution is most ambiguous. Recent advances in instance segmentation show that weighting regions of pixel overlap in training can improve segmentation boundary predictions in regions of overlap, but this idea has not yet been extended to segmentation regression. We address this with Overlap-Aware Segmentation of ImageS (OASIS): a new segmentation-regression framework with a weighted loss function designed to prioritize regions of object-overlap during training, enabling extraction of pixel intensities and topological features from heavily obscured objects. We demonstrate OASIS in the context of the MIGDAL experiment, which aims to directly image the Migdal effect--a rare process where electron emission is induced by nuclear scattering--in a low-pressure optical time projection chamber. This setting poses an extreme test case, as the target for reconstruction is a faint electron recoil track which is often heavily-buried within the orders-of-magnitude brighter nuclear recoil track. Compared to unweighted training, OASIS improves median intensity reconstruction errors from -32\% to -14\% for low-energy electron tracks (4-5 keV) and improves topological intersection-over-union scores from 0.828 to 0.855. These performance gains demonstrate OASIS's ability to recover obscured signals in overlap-dominated regions. The framework provides a generalizable methodology for scientific imaging where pixels represent physical quantities and overlap obscures features of interest. All code is openly available to facilitate cross-domain adoption.},
  archiveprefix = {arXiv},
  keywords = {/unread,Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Computer Vision and Pattern Recognition,High Energy Physics - Experiment},
  file = {C\:\\Users\\theun\\Zotero\\storage\\3TYJGGUB\\Schueler et al. - 2025 - Overlap-aware segmentation for topological reconstruction of obscured objects.pdf;C\:\\Users\\theun\\Zotero\\storage\\32APT337\\2510.html}
}

@misc{schwingFullyConnectedDeep2015,
  title = {Fully {{Connected Deep Structured Networks}}},
  author = {Schwing, Alexander G. and Urtasun, Raquel},
  year = 2015,
  month = mar,
  journal = {arXiv.org},
  urldate = {2025-06-11},
  abstract = {Convolutional neural networks with many layers have recently been shown to achieve excellent results on many high-level tasks such as image classification, object detection and more recently also semantic segmentation. Particularly for semantic segmentation, a two-stage procedure is often employed. Hereby, convolutional networks are trained to provide good local pixel-wise features for the second step being traditionally a more global graphical model. In this work we unify this two-stage process into a single joint training algorithm. We demonstrate our method on the semantic image segmentation task and show encouraging results on the challenging PASCAL VOC 2012 dataset.},
  howpublished = {https://arxiv.org/abs/1503.02351v1},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\RRXB7VTM\Schwing and Urtasun - 2015 - Fully Connected Deep Structured Networks.pdf}
}

@inproceedings{seifEdgeBasedLossFunction2018,
  title = {Edge-{{Based Loss Function}} for {{Single Image Super-Resolution}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Seif, George and Androutsos, Dimitrios},
  year = 2018,
  month = apr,
  pages = {1468--1472},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2018.8461664},
  urldate = {2025-06-03},
  abstract = {In recent years, convolutional neural networks have shown state-of-the-art performance on the task of single-image super-resolution. Although these proposed networks have shown high-quality reconstruction results, the use of the mean-squared error (MSE) loss function for training tends to produce images that are overly smooth and blurry. The MSE does not consider image structures that are often important for achieving high human-perceived image quality. We propose a novel edge-based loss function to improve super-resolution resconstruction of images. Our loss function directly optimizes the edge pixels of the reconstructed image, thus driving the trained network to produce high-quality salient edges and thus sharper images. Extensive quantitative and qualitative results show that our proposed loss function significantly outperforms the MSE.},
  keywords = {convolutional neural network,Deep neural network,edge detection,Image edge detection,Image reconstruction,Image restoration,Image super-resolution,Spatial resolution,Task analysis,Training},
  file = {C:\Users\theun\Zotero\storage\BHGFB9CS\Seif and Androutsos - 2018 - Edge-Based Loss Function for Single Image Super-Resolution.pdf}
}

@misc{shahAdversaryMLResilience2023,
  title = {Adversary {{ML Resilience}} in {{Autonomous Driving Through Human Centered Perception Mechanisms}}},
  author = {Shah, Aakriti},
  year = 2023,
  month = nov,
  number = {arXiv:2311.01478},
  eprint = {2311.01478},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.01478},
  urldate = {2024-12-04},
  abstract = {Physical adversarial attacks on road signs are continuously exploiting vulnerabilities in modern day autonomous vehicles (AVs) and impeding their ability to correctly classify what type of road sign they encounter. Current models cannot generalize input data well, resulting in overfitting or underfitting. In overfitting, the model memorizes the input data but cannot generalize to new scenarios. In underfitting, the model does not learn enough of the input data to accurately classify these road signs. This paper explores the resilience of autonomous driving systems against three main physical adversarial attacks (tape, graffiti, illumination), specifically targeting object classifiers. Several machine learning models were developed and evaluated on two distinct datasets: road signs (stop signs, speed limit signs, traffic lights, and pedestrian crosswalk signs) and geometric shapes (octagons, circles, squares, and triangles). The study compared algorithm performance under different conditions, including clean and adversarial training and testing on these datasets. To build robustness against attacks, defense techniques like adversarial training and transfer learning were implemented. Results demonstrated transfer learning models played a crucial role in performance by allowing knowledge gained from shape training to improve generalizability of road sign classification, despite the datasets being completely different. The paper suggests future research directions, including human-in-the-loop validation, security analysis, real-world testing, and explainable AI for transparency. This study aims to contribute to improving security and robustness of object classifiers in autonomous vehicles and mitigating adversarial example impacts on driving systems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {/unread,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\theun\Zotero\storage\VV9YJ93R\Shah - 2023 - Adversary ML Resilience in Autonomous Driving Through Human Centered Perception Mechanisms.pdf}
}

@misc{shankarCLEARIRClarityEnhancedActive2025,
  title = {{{CLEAR-IR}}: {{Clarity-Enhanced Active Reconstruction}} of {{Infrared Imagery}}},
  shorttitle = {{{CLEAR-IR}}},
  author = {Shankar, Nathan and Ladosz, Pawel and Yin, Hujun},
  year = 2025,
  month = oct,
  number = {arXiv:2510.04883},
  eprint = {2510.04883},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2510.04883},
  urldate = {2025-10-17},
  abstract = {This paper presents a novel approach for enabling robust robotic perception in dark environments using infrared (IR) stream. IR stream is less susceptible to noise than RGB in low-light conditions. However, it is dominated by active emitter patterns that hinder high-level tasks such as object detection, tracking and localisation. To address this, a U-Net-based architecture is proposed that reconstructs clean IR images from emitter-populated input, improving both image quality and downstream robotic performance. This approach outperforms existing enhancement techniques and enables reliable operation of vision-driven robotic systems across illumination conditions from well-lit to extreme low-light scenes.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\theun\\Zotero\\storage\\ZZZNLHJU\\Shankar et al. - 2025 - CLEAR-IR Clarity-Enhanced Active Reconstruction of Infrared Imagery.pdf;C\:\\Users\\theun\\Zotero\\storage\\T9PLJM4B\\2510.html}
}

@inproceedings{sharmaNighttimeStereoDepth2020,
  title = {Nighttime {{Stereo Depth Estimation}} Using {{Joint Translation-Stereo Learning}}: {{Light Effects}} and {{Uninformative Regions}}},
  shorttitle = {Nighttime {{Stereo Depth Estimation}} Using {{Joint Translation-Stereo Learning}}},
  booktitle = {2020 {{International Conference}} on {{3D Vision}} ({{3DV}})},
  author = {Sharma, Aashish and Cheong, Loong-Fah and Heng, Lionel and Tan, Robby T.},
  year = 2020,
  month = nov,
  pages = {23--31},
  issn = {2475-7888},
  doi = {10.1109/3DV50981.2020.00012},
  urldate = {2025-05-10},
  abstract = {Nighttime stereo depth estimation is still challenging, as assumptions associated with daytime lighting conditions do not hold any longer. Nighttime is not only about lowlight and dense noise, but also about glow/glare, flares, non-uniform distribution of light, etc. One of the possible solutions is to train a network on night stereo images in a fully supervised manner. However, to obtain proper disparity ground-truths that are dense, independent from glare/glow, and have sufficiently far depth ranges is extremely intractable. To address the problem, we introduce a network joining day/night translation and stereo. In training the network, our method does not require ground-truth disparities of the night images, or paired day/night images. We utilize a translation network that can render realistic night stereo images from day stereo images. We then train a stereo network on the rendered night stereo images using the available disparity supervision from the corresponding day stereo images, and simultaneously also train the day/night translation network. We handle the fake depth problem, which occurs due to the unsupervised/unpaired translation, for light effects (e.g., glow/glare) and uninformative regions (e.g., low-light and saturated regions), by adding structure-preservation and weighted-smoothness constraints. Our experiments show that our method outperforms the baseline methods on night images.},
  keywords = {Estimation,Optimization,Stereo vision,Testing,Three-dimensional displays,Training,Transforms},
  file = {C:\Users\theun\Zotero\storage\A8E6PW4U\Sharma et al. - 2020 - Nighttime Stereo Depth Estimation using Joint Translation-Stereo Learning Light Effects and Uninfor.pdf}
}

@article{shenDNADepthFrequencyBasedDayNight2023,
  title = {{{DNA-Depth}}: {{A Frequency-Based Day-Night Adaptation}} for {{Monocular Depth Estimation}}},
  shorttitle = {{{DNA-Depth}}},
  author = {Shen, Mengjiao and Wang, Zhongyi and Su, Shuai and Liu, Chengju and Chen, Qijun},
  year = 2023,
  journal = {IEEE Transactions on Instrumentation and Measurement},
  volume = {72},
  pages = {1--12},
  issn = {1557-9662},
  doi = {10.1109/TIM.2023.3322498},
  urldate = {2025-06-30},
  abstract = {Autonomous driving necessitates ensuring safety across diverse environments, particularly in challenging conditions like low-light or nighttime scenarios. As a fundamental task in autonomous driving, monocular depth estimation has garnered significant attention and discussion. However, current monocular depth estimation methods primarily rely on daytime images, which limits their applicability to nighttime scenarios due to the substantial domain shift between daytime and nighttime styles. In this article, we propose a novel Day-Night Adaptation method (DNA-Depth) to realize monocular depth estimation in a night environment. Specifically, we simply use Fourier Transform to address the domain alignment problem. Our method does not require extra adversarial optimization but is quite effective. The simplicity of our method makes it easy to guide day-to-night domains. To the best of our knowledge, we are the first to utilize fast Fourier transformation for nighttime monocular depth estimation. Furthermore, to alleviate the problem of mobile light sources, we utilize an unsupervised joint learning framework for depth, optical flow, and ego-motion estimation in an end-to-end manner, which is coupled by 3-D geometry cues. Our model can simultaneously reason about the camera motion, the depth of a static background, and the optical flow of moving objects. Extensive experiments on the Oxford RobotCar, nuScenes, and Synthia datasets demonstrate the accuracy and precision of our method by comparing it with those state-of-the-art algorithms in depth estimation, both qualitatively and quantitatively.},
  keywords = {Cameras,Depth estimation,domain adaptation,dynamic environment,Estimation,Fourier transform,Frequency estimation,Frequency-domain analysis,Lighting,monocular vision,Optical flow,Training},
  file = {C:\Users\theun\Zotero\storage\F2X7L4JY\Shen et al. - 2023 - DNA-Depth A Frequency-Based Day-Night Adaptation for Monocular Depth Estimation.pdf}
}

@inproceedings{shengMonocularDepthDistribution2022,
  title = {Monocular {{Depth Distribution Alignment}} with {{Low Computation}}},
  booktitle = {2022 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Sheng, Fei and Xue, Feng and Chang, Yicong and Liang, Wenteng and Ming, Anlong},
  year = 2022,
  month = may,
  pages = {6548--6555},
  doi = {10.1109/ICRA46639.2022.9811937},
  urldate = {2025-06-30},
  abstract = {The performance of monocular depth estimation generally depends on the amount of parameters and computational cost. It leads to a large accuracy contrast between light-weight networks and heavy-weight networks, which limits their application in the real world. In this paper, we model the majority of accuracy contrast between them as the difference of depth distribution, which we call 'Distribution drift'. To this end, a distribution alignment network (DANet) is proposed. We firstly design a pyramid scene transformer (PST) module to capture inter-region interaction in multiple scales. By perceiving the difference of depth features between every two regions, DANet tends to predict a reasonable scene structure, which fits the shape of distribution to ground truth. Then, we propose a local-global optimization (LGO) scheme to realize the supervision of global range of scene depth. Thanks to the alignment of depth distribution shape and scene depth range, DANet sharply alleviates the distribution drift, and achieves a comparable performance with prior heavy-weight methods, but uses only 1\% floating-point operations per second (FLOPs) of them. The experiments on two datasets, namely the widely used NYUDv2 dataset and the more challenging iBims-1 dataset, demonstrate the effectiveness of our method. The source code is available at https://github.com/YiLiM1/DANet.},
  keywords = {Computational efficiency,Computational modeling,Estimation,Real-time systems,Reliability,Shape,Transformers},
  file = {C:\Users\theun\Zotero\storage\4SUXALBC\Sheng et al. - 2022 - Monocular Depth Distribution Alignment with Low Computation.pdf}
}

@article{shenLDWLESelfsupervisedDriven2025,
  title = {{{LDWLE}}: Self-Supervised Driven Low-Light Object Detection Framework},
  shorttitle = {{{LDWLE}}},
  author = {Shen, Xiaoyang and Li, Haibin and Li, Yaqian and Zhang, Wenming},
  year = 2025,
  month = jan,
  journal = {Complex \& Intelligent Systems},
  volume = {11},
  number = {1},
  pages = {82},
  issn = {2199-4536, 2198-6053},
  doi = {10.1007/s40747-024-01681-z},
  urldate = {2025-02-17},
  abstract = {Low-light object detection involves identifying and locating objects in images captured under poor lighting conditions. It plays a significant role in surveillance and security, night pedestrian recognition, and autonomous driving, showcasing broad application prospects. Most existing object detection algorithms and datasets are designed for normal lighting conditions, leading to a significant drop in detection performance when applied to low-light environments. To address this issue, we propose a Low-Light Detection with Low-Light Enhancement (LDWLE) framework. LDWLE is an encoder-decoder architecture where the encoder transforms the raw input data into a compact, abstract representation (encoding), and the decoder gradually generates the target output format from the representation produced by the encoder. Specifically, during training, low-light images are input into the encoder, which produces feature representations that are decoded by two separate decoders: an object detection decoder and a low-light image enhancement decoder. Both decoders share the same encoder and are trained jointly. Throughout the training process, the two decoders optimize each other, guiding the low-light image enhancement towards improvements that benefit object detection. If the input image is normally lit, it first passes through a low-light image conversion module to be transformed into a low-light image before being fed into the encoder. If the input image is already a low-light image, it is directly input into the encoder. During the testing phase, the model can be evaluated in the same way as a standard object detection algorithm. Compared to existing object detection algorithms, LDWLE can train a low-light robust object detection model using standard, normally lit object detection datasets. Additionally, LDWLE is a versatile training framework that can be implemented on most one-stage object detection algorithms. These algorithms typically consist of three components: the backbone, neck, and head. In this framework, the backbone functions as the encoder, while the neck and head form the object detection decoder. Extensive experiments on the COCO, VOC, and ExDark datasets have demonstrated the effectiveness of LDWLE in low-light object detection. In quantitative measurements, it achieves an AP of 25.5 and 38.4 on the synthetic datasets COCO-d and VOC-d, respectively, and achieves the best AP of 30.5 on the real-world dataset ExDark. In qualitative measurements, LDWLE can accurately detect most objects on both public real-world low-light datasets and self-collected ones, demonstrating strong adaptability to varying lighting conditions and multi-scale objects.},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\SESDEGPC\Shen et al. - 2025 - LDWLE self-supervised driven low-light object detection framework.pdf}
}

@article{shenSelfsupervisedMonocularDepth2024,
  title = {Self-Supervised Monocular Depth Estimation on Construction Sites in Low-Light Conditions and Dynamic Scenes},
  author = {Shen, Jie and Huang, Ziyi and Jiao, Lang},
  year = 2024,
  month = dec,
  journal = {Automation in Construction},
  volume = {168},
  pages = {105848},
  issn = {09265805},
  doi = {10.1016/j.autcon.2024.105848},
  urldate = {2025-06-30},
  abstract = {Estimating construction scene depth from a single image is crucial for various downstream tasks. Selfsupervised monocular depth estimation methods have recently achieved impressive results and demonstrated state-of-the-art performance. However, the low-light conditions and dynamic scenes on construction sites pose significant challenges to these methods, hindering their practical deployment. Therefore, an architecture called LLD-Depth is presented to address these challenges, including an improved ForkGAN model to generate paired low-light images from clear-day images, a new unifying learning method for accurately estimating monocular depth, motion flow, camera ego-motion, and its intrinsic parameters, as well as a training framework to estimate monocular depth under both low-light and clear-day conditions effectively. Finally, the effectiveness of monocular depth estimation in construction scenes is verified. LLD-Depth brings 16.67\% and 20.17\% gain in relative mean error for clear-day and low-light scenes and 2.60\% and 1.80\% gain in average order accuracy, achieving state-of-the-art performance.},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\X7VNN2SR\Shen et al. - 2024 - Self-supervised monocular depth estimation on construction sites in low-light conditions and dynamic.pdf}
}

@inproceedings{silbermanIndoorSegmentationSupport2012,
  title = {Indoor {{Segmentation}} and {{Support Inference}} from {{RGBD Images}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2012},
  author = {Silberman, Nathan and Hoiem, Derek and Kohli, Pushmeet and Fergus, Rob},
  year = 2012,
  pages = {746--760},
  publisher = {Springer, Berlin, Heidelberg},
  issn = {1611-3349},
  doi = {10.1007/978-3-642-33715-4_54},
  urldate = {2025-07-03},
  abstract = {We present an approach to interpret the major surfaces, objects, and support relations of an indoor scene from an RGBD image. Most existing work ignores physical interactions or is applied only to tidy rooms and hallways. Our goal is to parse typical, often messy,...},
  isbn = {978-3-642-33715-4},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\SNYDN2FA\Silberman et al. - 2012 - Indoor Segmentation and Support Inference from RGBD Images.pdf}
}

@misc{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = 2015,
  month = apr,
  number = {arXiv:1409.1556},
  eprint = {1409.1556},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1409.1556},
  urldate = {2025-06-10},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\ICNJHYVB\\Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf;C\:\\Users\\theun\\Zotero\\storage\\KQCYXVSZ\\1409.html}
}

@article{sivaramanLookingVehiclesRoad2013,
  title = {Looking at {{Vehicles}} on the {{Road}}: {{A Survey}} of {{Vision-Based Vehicle Detection}}, {{Tracking}}, and {{Behavior Analysis}}},
  shorttitle = {Looking at {{Vehicles}} on the {{Road}}},
  author = {Sivaraman, Sayanan and Trivedi, Mohan Manubhai},
  year = 2013,
  month = dec,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {14},
  number = {4},
  pages = {1773--1795},
  issn = {1524-9050, 1558-0016},
  doi = {10.1109/TITS.2013.2266661},
  urldate = {2025-01-02},
  abstract = {This paper provides a review of the literature in on-road vision-based vehicle detection, tracking, and behavior understanding. Over the past decade, vision-based surround perception has progressed from its infancy into maturity. We provide a survey of recent works in the literature, placing vision-based vehicle detection in the context of sensor-based on-road surround analysis. We detail advances in vehicle detection, discussing monocular, stereo vision, and active sensor--vision fusion for on-road vehicle detection. We discuss vision-based vehicle tracking in the monocular and stereo-vision domains, analyzing filtering, estimation, and dynamical models. We discuss the nascent branch of intelligent vehicles research concerned with utilizing spatiotemporal measurements, trajectories, and various features to characterize on-road behavior. We provide a discussion on the state of the art, detail common performance metrics and benchmarks, and provide perspective on future research directions in the field.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\JAGZ5CAI\Sivaraman and Trivedi - 2013 - Looking at Vehicles on the Road A Survey of Vision-Based Vehicle Detection, Tracking, and Behavior.pdf}
}

@article{songNighttimeRoadScene2022,
  title = {Nighttime {{Road Scene Parsing}} by {{Unsupervised Domain Adaptation}}},
  author = {Song, Can and Wu, Jin and Zhu, Lei and Zhang, Mei and Ling, Haibin},
  year = 2022,
  month = apr,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {23},
  number = {4},
  pages = {3244--3255},
  issn = {1558-0016},
  doi = {10.1109/TITS.2020.3033569},
  urldate = {2025-06-16},
  abstract = {Due to recent advances in learning-based semantic segmentation, road scene parsing can usually achieve satisfactory results under normal illumination conditions. However, training a robust model for parsing nighttime road scenes is still very challenging, especially when semantic labels of training samples are absent. In this paper, we propose a convolutional neural network (CNN)-based method for parsing nighttime road scenes in an unsupervised manner. The proposed system includes an appearance transferring module and a segmentation module, which are coupled together and learned in an end-to-end fashion. The appearance transferring module aims to transfer unlabeled images acquired during both daytime and nighttime into a shared latent feature space that encodes the image content of both scenes at the semantic level. Then, the segmentation module is used to map the feature to its corresponding semantic labels. To better evaluate the proposed model, we also construct a new semantic segmentation dataset including 1,566 nighttime images. The extensive experimental results on the proposed benchmark illustrate that the proposed model achieves significant improvement compared with the baselines as well as a recently released system.},
  keywords = {Annotations,Feature extraction,generative adversarial network,Image segmentation,Lighting,night images,Roads,semantic segmentation,Semantics,Task analysis,Transfer learning},
  file = {C:\Users\theun\Zotero\storage\ARG6K6A8\Song et al. - 2022 - Nighttime Road Scene Parsing by Unsupervised Domain Adaptation.pdf}
}

@misc{soriaDenseExtremeInception2020,
  title = {Dense {{Extreme Inception Network}}: {{Towards}} a {{Robust CNN Model}} for {{Edge Detection}}},
  shorttitle = {Dense {{Extreme Inception Network}}},
  author = {Soria, Xavier and Riba, Edgar and Sappa, Angel D.},
  year = 2020,
  month = feb,
  number = {arXiv:1909.01955},
  eprint = {1909.01955},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1909.01955},
  urldate = {2025-01-03},
  abstract = {This paper proposes a Deep Learning based edge detector, which is inspired on both HED (Holistically-Nested Edge Detection) and Xception networks. The proposed approach generates thin edge-maps that are plausible for human eyes; it can be used in any edge detection task without previous training or fine tuning process. As a second contribution, a large dataset with carefully annotated edges has been generated. This dataset has been used for training the proposed approach as well the state-of-the-art algorithms for comparisons. Quantitative and qualitative evaluations have been performed on different benchmarks showing improvements with the proposed method when F-measure of ODS and OIS are considered.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\theun\\Zotero\\storage\\B64VPM3P\\Soria et al. - 2020 - Dense Extreme Inception Network Towards a Robust CNN Model for Edge Detection.pdf;C\:\\Users\\theun\\Zotero\\storage\\FSHA8VQP\\1909.html}
}

@inproceedings{spencerDeFeatNetGeneralMonocular2020,
  title = {{{DeFeat-Net}}: {{General Monocular Depth}} via {{Simultaneous Unsupervised Representation Learning}}},
  shorttitle = {{{DeFeat-Net}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Spencer, Jaime and Bowden, Richard and Hadfield, Simon},
  year = 2020,
  pages = {14402--14413},
  urldate = {2025-06-30},
  file = {C:\Users\theun\Zotero\storage\6E5BAMWB\Spencer et al. - 2020 - DeFeat-Net General Monocular Depth via Simultaneous Unsupervised Representation Learning.pdf}
}

@article{starckImageDecompositionCombination2005,
  title = {Image Decomposition via the Combination of Sparse Representations and a Variational Approach},
  author = {Starck, J.-L. and Elad, M. and Donoho, D.L.},
  year = 2005,
  month = oct,
  journal = {IEEE Transactions on Image Processing},
  volume = {14},
  number = {10},
  pages = {1570--1582},
  issn = {1941-0042},
  doi = {10.1109/TIP.2005.852206},
  urldate = {2025-06-06},
  abstract = {The separation of image content into semantic parts plays a vital role in applications such as compression, enhancement, restoration, and more. In recent years, several pioneering works suggested such a separation be based on variational formulation and others using independent component analysis and sparsity. This paper presents a novel method for separating images into texture and piecewise smooth (cartoon) parts, exploiting both the variational and the sparsity mechanisms. The method combines the basis pursuit denoising (BPDN) algorithm and the total-variation (TV) regularization scheme. The basic idea presented in this paper is the use of two appropriate dictionaries, one for the representation of textures and the other for the natural scene parts assumed to be piecewise smooth. Both dictionaries are chosen such that they lead to sparse representations over one type of image-content (either texture or piecewise smooth). The use of the BPDN with the two amalgamed dictionaries leads to the desired separation, along with noise removal as a by-product. As the need to choose proper dictionaries is generally hard, a TV regularization is employed to better direct the separation process and reduce ringing artifacts. We present a highly efficient numerical scheme to solve the combined optimization problem posed by our model and to show several experimental results that validate the algorithm's performance.},
  keywords = {Basis pursuit denoising (BPDN),curvelet,Dictionaries,Image coding,Image decomposition,Image restoration,Independent component analysis,Layout,local discrete cosine transform (DCT),Noise reduction,piecewise smooth,Pursuit algorithms,ridgelet,Separation processes,sparse representations,texture,total variation,TV,wavelet},
  file = {C:\Users\theun\Zotero\storage\X6X922DQ\Starck et al. - 2005 - Image decomposition via the combination of sparse representations and a variational approach.pdf}
}

@article{stoneDecidingRefiningResearch2002,
  title = {Deciding upon and Refining a Research Question},
  author = {Stone, Patrick},
  year = 2002,
  month = apr,
  journal = {Palliative Medicine},
  volume = {16},
  number = {3},
  pages = {265--267},
  publisher = {SAGE Publications Ltd STM},
  issn = {0269-2163},
  doi = {10.1191/0269216302pm562xx},
  urldate = {2025-03-12},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\6BEHZKR2\Stone - 2002 - Deciding upon and refining a research question.pdf}
}

@inproceedings{sturmBenchmarkEvaluationRGBD2012,
  title = {A Benchmark for the Evaluation of {{RGB-D SLAM}} Systems},
  booktitle = {2012 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Sturm, J{\"u}rgen and Engelhard, Nikolas and Endres, Felix and Burgard, Wolfram and Cremers, Daniel},
  year = 2012,
  month = oct,
  pages = {573--580},
  issn = {2153-0866},
  doi = {10.1109/IROS.2012.6385773},
  urldate = {2025-07-03},
  abstract = {In this paper, we present a novel benchmark for the evaluation of RGB-D SLAM systems. We recorded a large set of image sequences from a Microsoft Kinect with highly accurate and time-synchronized ground truth camera poses from a motion capture system. The sequences contain both the color and depth images in full sensor resolution (640 \texttimes{} 480) at video frame rate (30 Hz). The ground-truth trajectory was obtained from a motion-capture system with eight high-speed tracking cameras (100 Hz). The dataset consists of 39 sequences that were recorded in an office environment and an industrial hall. The dataset covers a large variety of scenes and camera motions. We provide sequences for debugging with slow motions as well as longer trajectories with and without loop closures. Most sequences were recorded from a handheld Kinect with unconstrained 6-DOF motions but we also provide sequences from a Kinect mounted on a Pioneer 3 robot that was manually navigated through a cluttered indoor environment. To stimulate the comparison of different approaches, we provide automatic evaluation tools both for the evaluation of drift of visual odometry systems and the global pose error of SLAM systems. The benchmark website [1] contains all data, detailed descriptions of the scenes, specifications of the data formats, sample code, and evaluation tools.},
  keywords = {/unread,Calibration,Cameras,Simultaneous localization and mapping,Trajectory,Visualization},
  file = {C:\Users\theun\Zotero\storage\NINMZ3JN\Sturm et al. - 2012 - A benchmark for the evaluation of RGB-D SLAM systems.pdf}
}

@article{sunNighttimeImageSemantic2024,
  title = {Nighttime Image Semantic Segmentation with Retinex Theory},
  author = {Sun, Zhichao and Zhu, Huachao and Xiao, Xin and Gu, Yuliang and Xu, Yongchao},
  year = 2024,
  month = aug,
  journal = {Image and Vision Computing},
  volume = {148},
  pages = {105149},
  issn = {0262-8856},
  doi = {10.1016/j.imavis.2024.105149},
  urldate = {2025-06-15},
  abstract = {Nighttime image semantic segmentation is challenging due to low-light and diverse lighting conditions. A straightforward solution is to first enhance nighttime scene images to resemble daytime scene before performing segmentation. This kind of methods heavily rely on the enhancement quality. Inspired by the Retinex theory for low-light image enhancement, which decomposes an image into reflectance and illumination components, we propose a novel nighttime image segmentation method with Retinex theory (RNightSeg). Our core insight is to obtain high-quality illumination-independent reflectance component to enhance segmentation. Specifically, we apply a decomposition decoder to the backbone network for generating the reflectance component. In addition to the fidelity loss and Total Variation loss for the reflectance component regression, we model the brightening illumination component to enhance the nighttime image and apply the color constancy loss on the enhanced image. This helps to cope with the issue of low-light and diverse lighting in the nighttime scene. Finally, we fuse the reflectance decoder feature with the backbone feature and feed the fused feature to the segmentation decoder. Extensive experimental results on two widely used datasets demonstrate that the proposed RNightSeg achieves superior performance over some state-of-the-art segmentation methods. The code of our implementation is available at https://github.com/sunzc-sunny/RNightSeg.},
  keywords = {Color image processing,Decoding,Illumination components,Image enhancement,Image semantics,Lighting,Low light,Nighttime vision,Reflectance components,Reflection,Retinex,Retinex decomposition,Retinex theory,Segmentation methods,Semantic segmentation,Semantic Segmentation,Semantics},
  file = {C\:\\Users\\theun\\Zotero\\storage\\4UG48GPI\\Sun et al. - 2024 - Nighttime image semantic segmentation with retinex theory.pdf;C\:\\Users\\theun\\Zotero\\storage\\MSZKHI2J\\S0262885624002543.html}
}

@inproceedings{sunReadabilityEnhancementLow2017,
  title = {Readability {{Enhancement}} of {{Low Light Videos Based}} on {{Discrete Wavelet Transform}}},
  booktitle = {2017 {{IEEE International Symposium}} on {{Multimedia}} ({{ISM}})},
  author = {Sun, Tingting and Jung, Cheolkon and Ke, Peng and Song, Hyoseob and Hwang, Jungmee},
  year = 2017,
  month = dec,
  pages = {342--345},
  doi = {10.1109/ISM.2017.63},
  urldate = {2025-05-29},
  abstract = {In this paper, we propose readability enhancement of low light videos based on discrete wavelet transform (DWT). Captured videos under the low light condition have a narrow dynamic range (low contrast) with a dark tone as well as are highly corrupted by noise. We achieve both contrast enhancement and noise reduction in low light videos using wavelet coefficients. First, we perform normalization to stretch a dynamic range of an image. Then, we decompose the image into high-pass and low-pass sub-bands using DWT. Next, we perform de-noising and weak edge enhancement in the high-pass sub-bands (LH, HL, HH) and contrast enhancement in the low-pass sub-band (LL). Finally, we conduct color correction to compensate for color distortions caused by contrast enhancement. Experimental results demonstrate that the proposed method outperforms state-of-the-art ones in contrast enhancement, noise reduction, and color reproduction in terms of both subjective and objective evaluations.},
  keywords = {color correction,Colored noise,Contrast enhancement,discrete wavelet transform,Discrete wavelet transforms,Image color analysis,Image edge detection,low light video,noise reduction,Noise reduction,readability,Videos,Wavelet coefficients},
  file = {C:\Users\theun\Zotero\storage\Q8PH2V67\Sun et al. - 2017 - Readability Enhancement of Low Light Videos Based on Discrete Wavelet Transform.pdf}
}

@inproceedings{sunScalabilityPerceptionAutonomous2020,
  title = {Scalability in {{Perception}} for {{Autonomous Driving}}: {{Waymo Open Dataset}}},
  shorttitle = {Scalability in {{Perception}} for {{Autonomous Driving}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Sun, Pei and Kretzschmar, Henrik and Dotiwalla, Xerxes and Chouard, Aurelien and Patnaik, Vijaysai and Tsui, Paul and Guo, James and Zhou, Yin and Chai, Yuning and Caine, Benjamin and Vasudevan, Vijay and Han, Wei and Ngiam, Jiquan and Zhao, Hang and Timofeev, Aleksei and Ettinger, Scott and Krivokon, Maxim and Gao, Amy and Joshi, Aditya and Zhang, Yu and Shlens, Jonathon and Chen, Zhifeng and Anguelov, Dragomir},
  year = 2020,
  pages = {2446--2454},
  urldate = {2025-05-15},
  file = {C:\Users\theun\Zotero\storage\EPJUVAXX\Sun et al. - 2020 - Scalability in Perception for Autonomous Driving Waymo Open Dataset.pdf}
}

@article{suReviewDeeplearningbasedSuperResolution2024,
  title = {A {{Review}} of {{Deep-learning-based Super-Resolution}}: From {{Methods}} to {{Applications}}},
  author = {Su, Hu and Li, Ying and Xu, Yifan and Fu, Xiang and Liu, Song},
  year = 2024,
  journal = {Deep learning},
  abstract = {Super-resolution (SR), aiming to super-resolve degraded low-resolution image to recover the corresponding high-resolution counterpart, is an important and challenging task in computer vision, and with various applications. The emergence of deep learning (DL) has significantly advanced SR methods, surpassing the performance of traditional techniques. This paper presents a comprehensive survey of DL-based SR methods encompassing single image super resolution (SISR) and multiple image super resolution (MISR) methods, along with their applications and limitations. In SISR methods, addressing individual images independently, we review blind and nonblind SR methods. Additionally, within MISR, we delve into multi-frame, multi-view, and reference-based SR methods. DL-based SR methods are categorized from the application perspective and a taxonomy is proposed. Finally, we present research prospects and future directions.},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\Z7736LTD\Su et al. - 2024 - A Review of Deep-learning-based Super-Resolution from Methods to Applications.pdf}
}

@article{suZeroreferenceDeepLearning2023,
  title = {Zero-Reference Deep Learning for Low-Light Image Enhancement of Underground Utilities {{3D}} Reconstruction},
  author = {Su, Yang and Wang, Jun and Wang, Xiangyu and Hu, Lei and Yao, Yuan and Shou, Wenchi and Li, Danqi},
  year = 2023,
  month = aug,
  journal = {Automation in Construction},
  volume = {152},
  pages = {104930},
  issn = {0926-5805},
  doi = {10.1016/j.autcon.2023.104930},
  urldate = {2025-04-02},
  abstract = {Image-based 3D reconstruction has become one of the most promising as-built construction modeling methods for its high cost-efficiency and outstanding performance. However, the quality performance of image-based 3D reconstruction is very sensitive to the illumination conditions. To date, the image-based 3D reconstruction in low-light environment is mainly optimized by traditional approaches that are time-consuming and manual parameters required. And the supervised deep learning methods request suitable paired image data (low-light images and the paired reference images). Therefore, a Zero-reference Deep learning model for the low-light image Enhancement for underground utilities 3D reconstruction (ZDE3D) is proposed in this paper. ZDE3D improved the 3D reconstruction performance of low-light images by unsupervised loss functions design without paired or unpaired training datasets. Field experiments implemented confirms that the capability of ZDE3D for increasing the quantity of sparse reconstruction point cloud by 13.19\% on average and the reconstruction accuracy reached 98.58\%.},
  keywords = {/unread,As-built record,As-built underground utilities,Deep learning,Image enhancement,Image-based 3D reconstruction,Low-light reconstruction},
  file = {C\:\\Users\\theun\\Zotero\\storage\\B86HKQET\\Su et al. - 2023 - Zero-reference deep learning for low-light image enhancement of underground utilities 3D reconstruct.pdf;C\:\\Users\\theun\\Zotero\\storage\\SWX9ZMT8\\S0926580523001905.html}
}

@inproceedings{szegedyGoingDeeperConvolutions2015,
  title = {Going {{Deeper With Convolutions}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  year = 2015,
  pages = {1--9},
  urldate = {2025-06-10},
  file = {C:\Users\theun\Zotero\storage\RY479XAS\Szegedy et al. - 2015 - Going Deeper With Convolutions.pdf}
}

@article{szeliskiComputerVisionAlgorithms,
  title = {Computer {{Vision}}: {{Algorithms}} and {{Applications}}, 2nd {{Edition}}},
  author = {Szeliski, Richard},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\XNZAP9ZX\Szeliski - Computer Vision Algorithms and Applications, 2nd Edition.pdf}
}

@article{talamkhaniUnderwaterVisionenhancedImage2025,
  title = {Underwater Vision-Enhanced Image Segmentation for Supporting Automated Inspection of Underwater Bridge Components},
  author = {Talamkhani, Saeed and Liu, Kaijian},
  year = 2025,
  month = jul,
  journal = {Automation in Construction},
  volume = {175},
  pages = {106230},
  issn = {0926-5805},
  doi = {10.1016/j.autcon.2025.106230},
  urldate = {2025-06-18},
  abstract = {Vision-based robotic systems for automated bridge inspection are limited in analyzing underwater inspection images, which present a set of unique visual challenges caused by light scattering, light attenuation, and low-light conditions in underwater environments. To address this limitation, this paper proposes an underwater vision-enhanced image segmentation method: (1) underwater vision-based quality enhancement is proposed to simultaneously mitigate quality degradations of underwater inspection images caused by light scattering, light attenuation, and low-light conditions; and (2) semantic segmentation is proposed to analyze quality-enhanced underwater images to localize bridge components, enabling effective component localization for subsequent damage detection and characterization in underwater inspection images. Baseline and ablation experiments were conducted for performance evaluation. The results showed that the proposed method achieved a mean, structure, and background IoUs of 91.7~\%, 88.5~\% and 94.8~\% -- outperforming state-of-the-art methods in segmenting underwater inspection images and demonstrating its potential to enable vision-based robotic systems for cost-effective underwater inspection.},
  keywords = {Abutments (bridge),Bridge approaches,Bridge cables,Bridge decks,Bridge piers,Deep learning,Images segmentations,Inspection image,Overwater bridge,Overwater bridges,Photointerpretation,Quality enhancement,Robotic systems,Semantic segmentation,Semantic Segmentation,Underwater inspection,Underwater inspections,Underwater vision,Vision based robotics},
  file = {C:\Users\theun\Zotero\storage\6LLVZ7T6\Talamkhani and Liu - 2025 - Underwater vision-enhanced image segmentation for supporting automated inspection of underwater brid.pdf}
}

@inproceedings{tamEnlightenDepthNovelSelfsupervised2024,
  title = {{{EnlightenDepth}}: A {{Novel Self-supervised Monocular Depth Estimation}} with {{Low-light Enhancement}} in {{Endoscopy}}},
  shorttitle = {{{EnlightenDepth}}},
  booktitle = {2024 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Tam, Chi Kong and Liu, Feng and Xia, Fei and Shantu, Arafat and Zhang, Jun Kui and Luo, Fei},
  year = 2024,
  month = dec,
  pages = {2468--2473},
  issn = {2156-1133},
  doi = {10.1109/BIBM62325.2024.10822069},
  urldate = {2025-06-30},
  abstract = {In minimally invasive surgery, depth estimation is important for enhancing the perceptual abilities of surgeons. Current monocular self-supervised depth estimation methods suffer from the problems of poor illumination and narrow-view field in endoscopy. To address them, we propose a novel self-supervised monocular depth estimation with low-light enhancement, named EnlightenDepth. First, we introduce a GAN-based module to improve the local brightness and visibility of endoscopic images. Then, we leverage the appearance flow to handle the illumination variance between the adjacent image frames, assisting the supervision. Furthermore, to better extract global and local features, we apply MonoVit to combine the advantages of CNN and ViT. Extensive experiments are conducted on the Hamlyn dataset, proving that low-light enhancement can improve depth estimation performance in endoscopic illumination conditions. Compared with other monocular self-supervised methods designed for endoscopy, our method achieves around 10\% improvement.},
  keywords = {Bioinformatics,Brightness,Depth measurement,Endoscopes,endoscopy,Feature extraction,Lighting,low-light enhancement,Minimally invasive surgery,monocular depth estimation,Training},
  file = {C:\Users\theun\Zotero\storage\DKIZS2FZ\Tam et al. - 2024 - EnlightenDepth a Novel Self-supervised Monocular Depth Estimation with Low-light Enhancement in End.pdf}
}

@article{tanDarkSegNetLowlightSemantic2025,
  title = {{{DarkSegNet}}: {{Low-light}} Semantic Segmentation Network Based on Image Pyramid},
  shorttitle = {{{DarkSegNet}}},
  author = {Tan, Jintao and Huang, Longyang and Chen, Zhonghui and Qu, Ruokun and Li, Chenglong},
  year = 2025,
  month = jul,
  journal = {Signal Processing: Image Communication},
  volume = {135},
  pages = {117265},
  issn = {0923-5965},
  doi = {10.1016/j.image.2025.117265},
  urldate = {2025-02-27},
  abstract = {In the domain of computer vision, the task of semantic segmentation for images captured under low-light conditions has proven to be a formidable challenge. To address this challenge, we introduce a novel low-light semantic segmentation model named DarkSegNet. The DarkSegNet model aims to deal with the problem of semantic segmentation of low-light images. It effectively mines potential information in images by combining image pyramid decomposition, spatial low-frequency attention (SLA) module, and channel low-frequency information enhancement (CLIE) module to achieve better low-light semantic segmentation performance. These components work synergistically to effectively extract latent information embedded within the low-light image, ultimately resulting in improved performance of low-light semantic segmentation. We conduct experiments on the UAV indoor low-light LLRGBD-real dataset. Compared to other mainstream semantic segmentation methods, DarkSegNet achieves the highest mIoU of 47.9\% on the UAV indoor low-light LLRGBD-real dataset. It is worth emphasizing that our model implements end-to-end training, avoiding the need to design additional image enhancement modules. The DarkSegNet network holds significant potential for facilitating drone-based rescue operations in disaster-stricken environments.},
  keywords = {Low light,Semantic segmentation,UAV},
  file = {C\:\\Users\\theun\\Zotero\\storage\\J5IHN853\\Tan et al. - 2025 - DarkSegNet Low-light semantic segmentation network based on image pyramid.pdf;C\:\\Users\\theun\\Zotero\\storage\\SP9AQ9AH\\S0923596525000128.html}
}

@misc{tanEfficientNetRethinkingModel2020,
  title = {{{EfficientNet}}: {{Rethinking Model Scaling}} for {{Convolutional Neural Networks}}},
  shorttitle = {{{EfficientNet}}},
  author = {Tan, Mingxing and Le, Quoc V.},
  year = 2020,
  month = sep,
  number = {arXiv:1905.11946},
  eprint = {1905.11946},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1905.11946},
  urldate = {2025-11-07},
  abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\theun\\Zotero\\storage\\GAIJJ5KM\\Tan and Le - 2020 - EfficientNet Rethinking Model Scaling for Convolutional Neural Networks.pdf;C\:\\Users\\theun\\Zotero\\storage\\5PUDZ3SA\\1905.html}
}

@article{tangPIAFusionProgressiveInfrared2022,
  title = {{{PIAFusion}}: {{A}} Progressive Infrared and Visible Image Fusion Network Based on Illumination Aware},
  shorttitle = {{{PIAFusion}}},
  author = {Tang, Linfeng and Yuan, Jiteng and Zhang, Hao and Jiang, Xingyu and Ma, Jiayi},
  year = 2022,
  month = jul,
  journal = {Information Fusion},
  volume = {83--84},
  pages = {79--92},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2022.03.007},
  urldate = {2025-10-17},
  abstract = {Infrared and visible image fusion aims to synthesize a single fused image containing salient targets and abundant texture details even under extreme illumination conditions. However, existing image fusion algorithms fail to take the illumination factor into account in the modeling process. In this paper, we propose a progressive image fusion network based on illumination-aware, termed as PIAFusion, which adaptively maintains the intensity distribution of salient targets and preserves texture information in the background. Specifically, we design an illumination-aware sub-network to estimate the illumination distribution and calculate the illumination probability. Moreover, we utilize the illumination probability to construct an illumination-aware loss to guide the training of the fusion network. The cross-modality differential aware fusion module and halfway fusion strategy completely integrate common and complementary information under the constraint of illumination-aware loss. In addition, a new benchmark dataset for infrared and visible image fusion, i.e., Multi-Spectral Road Scenarios~(available at https://github.com/Linfeng-Tang/MSRS), is released to support network training and comprehensive evaluation. Extensive experiments demonstrate the superiority of our method over state-of-the-art alternatives in terms of target maintenance and texture preservation. Particularly, our progressive fusion framework could round-the-clock integrate meaningful information from source images according to illumination conditions. Furthermore, the application to semantic segmentation demonstrates the potential of our PIAFusion for high-level vision tasks. Our codes will be available at https://github.com/Linfeng-Tang/PIAFusion.},
  keywords = {/unread,Cross-modality differential aware fusion,Deep learning,Illumination aware,Image fusion},
  file = {C\:\\Users\\theun\\Zotero\\storage\\ZS29F2GG\\Tang et al. - 2022 - PIAFusion A progressive infrared and visible image fusion network based on illumination aware.pdf;C\:\\Users\\theun\\Zotero\\storage\\QAYIB5CG\\S156625352200032X.html}
}

@misc{tangSimPBSingleModel2024,
  title = {{{SimPB}}: {{A Single Model}} for {{2D}} and {{3D Object Detection}} from {{Multiple Cameras}}},
  shorttitle = {{{SimPB}}},
  author = {Tang, Yingqi and Meng, Zhaotie and Chen, Guoliang and Cheng, Erkang},
  year = 2024,
  month = jul,
  number = {arXiv:2403.10353},
  eprint = {2403.10353},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.10353},
  urldate = {2024-12-06},
  abstract = {The field of autonomous driving has attracted considerable interest in approaches that directly infer 3D objects in the Bird's Eye View (BEV) from multiple cameras. Some attempts have also explored utilizing 2D detectors from single images to enhance the performance of 3D detection. However, these approaches rely on a two-stage process with separate detectors, where the 2D detection results are utilized only once for token selection or query initialization. In this paper, we present a single model termed SimPB, which simultaneously detects 2D objects in the perspective view and 3D objects in the BEV space from multiple cameras. To achieve this, we introduce a hybrid decoder consisting of several multi-view 2D decoder layers and several 3D decoder layers, specifically designed for their respective detection tasks. A Dynamic Query Allocation module and an Adaptive Query Aggregation module are proposed to continuously update and refine the interaction between 2D and 3D results, in a cyclic 3D-2D-3D manner. Additionally, Query-group Attention is utilized to strengthen the interaction among 2D queries within each camera group. In the experiments, we evaluate our method on the nuScenes dataset and demonstrate promising results for both 2D and 3D detection tasks. Our code is available at: https://github.com/nullmax-vision/SimPB.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\Y9GPYM28\\Tang et al. - 2024 - SimPB A Single Model for 2D and 3D Object Detection from Multiple Cameras.pdf;C\:\\Users\\theun\\Zotero\\storage\\926XZKCT\\2403.html}
}

@inproceedings{tangStereoMatchingReconstruction2019,
  title = {A {{Stereo Matching}} with {{Reconstruction Network}} for {{Low-light Stereo Vision}}},
  booktitle = {Proceedings of the 2019 2nd {{International Conference}} on {{Signal Processing}} and {{Machine Learning}}},
  author = {Tang, Rui and Zhang, Geng and Liu, Xuebin},
  year = 2019,
  month = nov,
  pages = {98--102},
  publisher = {ACM},
  address = {Hangzhou China},
  doi = {10.1145/3372806.3372821},
  urldate = {2025-05-06},
  isbn = {978-1-4503-7221-3},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\JCHI2DYD\Tang et al. - 2019 - A Stereo Matching with Reconstruction Network for Low-light Stereo Vision.pdf}
}

@article{tanNightTimeSceneParsing2021,
  title = {Night-{{Time Scene Parsing With}} a {{Large Real Dataset}}},
  author = {Tan, Xin and Xu, Ke and Cao, Ying and Zhang, Yiheng and Ma, Lizhuang and Lau, Rynson W. H.},
  year = 2021,
  journal = {IEEE Transactions on Image Processing},
  volume = {30},
  pages = {9085--9098},
  issn = {1941-0042},
  doi = {10.1109/TIP.2021.3122004},
  urldate = {2025-01-19},
  abstract = {Although huge progress has been made on scene analysis in recent years, most existing works assume the input images to be in day-time with good lighting conditions. In this work, we aim to address the night-time scene parsing (NTSP) problem, which has two main challenges: 1) labeled night-time data are scarce, and 2) over- and under-exposures may co-occur in the input night-time images and are not explicitly modeled in existing pipelines. To tackle the scarcity of night-time data, we collect a novel labeled dataset, named NightCity, of 4,297 real night-time images with ground truth pixel-level semantic annotations. To our knowledge, NightCity is the largest dataset for NTSP. In addition, we also propose an exposure-aware framework to address the NTSP problem through augmenting the segmentation process with explicitly learned exposure features. Extensive experiments show that training on NightCity can significantly improve NTSP performances and that our exposure-aware model outperforms the state-of-the-art methods, yielding top performances on our dataset as well as existing datasets.},
  keywords = {adverse conditions,Autonomous driving,Autonomous vehicles,Computer science,Image segmentation,night-time vision,scene analysis,Semantics,Streaming media,Urban areas},
  file = {C\:\\Users\\theun\\Zotero\\storage\\93QS7H6J\\Tan et al. - 2021 - Night-Time Scene Parsing With a Large Real Dataset.pdf;C\:\\Users\\theun\\Zotero\\storage\\CTJCSSFM\\9591338.html}
}

@incollection{thanhDepthSegNet24LabelFreeModel2025,
  title = {{{DepthSegNet24}}: {{A Label-Free Model}} for {{Robust Day-Night Depth}} and {{Semantics}}},
  shorttitle = {{{DepthSegNet24}}},
  booktitle = {Computer {{Vision}} -- {{ACCV}} 2024},
  author = {Thanh, Phan Thi Huyen and Nguyen, The Hiep and Nguyen, Minh Huy Vu and Tran, Trung Thai and Pham, Tran Vu and Nguyen, Duc Dung and Duy, Truong Vinh Truong and Naotake, Natori},
  editor = {Cho, Minsu and Laptev, Ivan and Tran, Du and Yao, Angela and Zha, Hongbin},
  year = 2025,
  volume = {15478},
  pages = {38--55},
  publisher = {Springer Nature Singapore},
  address = {Singapore},
  doi = {10.1007/978-981-96-0963-5_3},
  urldate = {2025-06-30},
  abstract = {This paper presents a novel multi-task model combining selfsupervised monocular depth estimation and knowledge-distilled semantic segmentation that can perform both tasks simultaneously and consistently in both daytime and nighttime conditions. By leveraging the joint self-supervised and supervised knowledge distillation learning, the model can learn consistent and complementary representations of the two tasks to improve the generalization ability without relying on annotated ground-truth data. To address the extremely varying lighting conditions between day and night, we first synthesize night and day images from their corresponding real day and night images, and then train the model with the day-night image pairs to provide explicit correspondences between the two lighting conditions for capturing the contextual and detailed information in both scenarios. We also augment the model with a light enhancement module and a daytime depth pseudo-labels for achieving more accurate and robust depth and segmentation. Experimental results on Oxford RobotCar and nuScenes demonstrate the robustness of our model in diverse challenging lighting conditions.},
  isbn = {978-981-96-0962-8 978-981-96-0963-5},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\Z9QVTS2L\Thanh et al. - 2025 - DepthSegNet24 A Label-Free Model for Robust Day-Night Depth and Semantics.pdf}
}

@article{tosiSurveyDeepStereo2025,
  title = {A {{Survey}} on {{Deep Stereo Matching}} in the {{Twenties}}},
  author = {Tosi, Fabio and Bartolomei, Luca and Poggi, Matteo},
  year = 2025,
  month = feb,
  journal = {International Journal of Computer Vision},
  pages = {1--32},
  publisher = {Springer US},
  issn = {1573-1405},
  doi = {10.1007/s11263-024-02331-0},
  urldate = {2025-05-29},
  abstract = {Stereo matching is close to hitting a half-century of history, yet witnessed a rapid evolution in the last decade thanks to deep learning. While previous surveys in the late 2010s covered the first stage of this revolution, the last five years of research brought further ground-breaking advancements to the field. This paper aims to fill this gap in a two-fold manner: first, we offer an in-depth examination of the latest developments in deep stereo matching, focusing on the pioneering architectural designs and groundbreaking paradigms that have redefined the field in the 2020s; second, we present a thorough analysis of the critical challenges that have emerged alongside these advances, providing a comprehensive taxonomy of these issues and exploring the state-of-the-art techniques proposed to address them. By reviewing both the architectural innovations and the key challenges, we offer a holistic view of deep stereo matching and highlight the specific areas that require further investigation. To accompany this survey, we maintain a regularly updated project page that catalogs papers on deep stereo matching in our Awesome-Deep-Stereo-Matching repository.},
  copyright = {2025 The Author(s)},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\T7S8R6IF\Tosi et al. - 2025 - A Survey on Deep Stereo Matching in the Twenties.pdf}
}

@inproceedings{treibleCATSColorThermal2017,
  title = {{{CATS}}: {{A Color}} and {{Thermal Stereo Benchmark}}},
  shorttitle = {{{CATS}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Treible, Wayne and Saponaro, Philip and Sorensen, Scott and Kolagunda, Abhishek and ONeal, Michael and Phelan, Brian and Sherbondy, Kelly and Kambhamettu, Chandra},
  year = 2017,
  month = jul,
  pages = {134--142},
  publisher = {IEEE},
  address = {Honolulu, HI},
  doi = {10.1109/CVPR.2017.22},
  urldate = {2025-05-15},
  abstract = {Stereo matching is a well researched area using visibleband color cameras. Thermal images are typically lower resolution, have less texture, and are noisier compared to their visible-band counterparts and are more challenging for stereo matching algorithms. Previous benchmarks for stereo matching either focus entirely on visible-band cameras or contain only a single thermal camera. We present the Color And Thermal Stereo (CATS) benchmark, a dataset consisting of stereo thermal, stereo color, and cross-modality image pairs with high accuracy ground truth ({$<$} 2mm) generated from a LiDAR. We scanned 100 cluttered indoor and 80 outdoor scenes featuring challenging environments and conditions. CATS contains approximately 1400 images of pedestrians, vehicles, electronics, and other thermally interesting objects in different environmental conditions, including nighttime, daytime, and foggy scenes. Ground truth was projected to each of the four cameras to generate color-color, thermal-thermal, and cross-modality disparity maps. A semi-automatic LiDAR to camera alignment procedure was developed that does not require a calibration target. We compare state-of-the-art algorithms to baseline the dataset and show that in the thermal and cross modalities there is still much room for improvement. We expect our dataset to provide researchers with a more diverse set of imaged locations, objects, and modalities than previous benchmarks for stereo matching.},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\37ZYCJ3D\Treible et al. - 2017 - CATS A Color and Thermal Stereo Benchmark.pdf}
}

@inproceedings{trungDarkMDEExcavatingSynthetic2023,
  title = {{{DarkMDE}}: {{Excavating Synthetic Images}} for {{Nighttime Depth Estimation Using Cross-Domain Supervision}}},
  shorttitle = {{{DarkMDE}}},
  booktitle = {Intelligence of {{Things}}: {{Technologies}} and {{Applications}}},
  author = {Trung, Thai Tran and Xuan, Huy Le and Nguyen, Minh Huy Vu and The, Hiep Nguyen and Hoang, Nhat Huy Tran and Nguyen, Duc Dung},
  year = 2023,
  pages = {428--438},
  publisher = {Springer, Cham},
  issn = {2367-4520},
  doi = {10.1007/978-3-031-46573-4_39},
  urldate = {2025-06-30},
  abstract = {Self-supervised monocular depth estimation (MDE) has recently gained significant attention, demonstrating remarkable results, particularly in daytime scenarios. However, MDE in nighttime images remains challenging due to the sensitivity of photometric loss to noise...},
  isbn = {978-3-031-46573-4},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\FDUCSEI5\Trung et al. - 2023 - DarkMDE Excavating Synthetic Images for Nighttime Depth Estimation Using Cross-Domain Supervision.pdf}
}

@inproceedings{tuMAXIMMultiAxisMLP2022,
  title = {{{MAXIM}}: {{Multi-Axis MLP}} for {{Image Processing}}},
  shorttitle = {{{MAXIM}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Tu, Zhengzhong and Talebi, Hossein and Zhang, Han and Yang, Feng and Milanfar, Peyman and Bovik, Alan and Li, Yinxiao},
  year = 2022,
  month = jun,
  pages = {5759--5770},
  publisher = {IEEE},
  address = {New Orleans, LA, USA},
  doi = {10.1109/CVPR52688.2022.00568},
  urldate = {2025-03-12},
  abstract = {Recent progress on Transformers and multi-layer perceptron (MLP) models provide new network architectural designs for computer vision tasks. Although these models proved to be effective in many vision tasks such as image recognition, there remain challenges in adapting them for low-level vision. The inflexibility to support high-resolution images and limitations of local attention are perhaps the main bottlenecks. In this work, we present a multi-axis MLP based architecture called MAXIM, that can serve as an efficient and flexible general-purpose vision backbone for image processing tasks. MAXIM uses a UNet-shaped hierarchical structure and supports long-range interactions enabled by spatially-gated MLPs. Specifically, MAXIM contains two MLP-based building blocks: a multi-axis gated MLP that allows for efficient and scalable spatial mixing of local and global visual cues, and a cross-gating block, an alternative to cross-attention, which accounts for crossfeature conditioning. Both these modules are exclusively based on MLPs, but also benefit from being both global and `fully-convolutional', two properties that are desirable for image processing. Our extensive experimental results show that the proposed MAXIM model achieves state-ofthe-art performance on more than ten benchmarks across a range of image processing tasks, including denoising, deblurring, deraining, dehazing, and enhancement while requiring fewer or comparable numbers of parameters and FLOPs than competitive models. The source code and trained models will be available at https://github. com/google-research/maxim.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-6946-3},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\59HF8JT5\Tu et al. - 2022 - MAXIM Multi-Axis MLP for Image Processing.pdf}
}

@misc{vankadariDuskTillDawn2024,
  title = {Dusk {{Till Dawn}}: {{Self-supervised Nighttime Stereo Depth Estimation}} Using {{Visual Foundation Models}}},
  shorttitle = {Dusk {{Till Dawn}}},
  author = {Vankadari, Madhu and Hodgson, Samuel and Shin, Sangyun and Markham, Kaichen Zhou Andrew and Trigoni, Niki},
  year = 2024,
  month = may,
  number = {arXiv:2405.11158},
  eprint = {2405.11158},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.11158},
  urldate = {2025-05-06},
  abstract = {Self-supervised depth estimation algorithms rely heavily on frame-warping relationships, exhibiting substantial performance degradation when applied in challenging circumstances, such as low-visibility and nighttime scenarios with varying illumination conditions. Addressing this challenge, we introduce an algorithm designed to achieve accurate self-supervised stereo depth estimation focusing on nighttime conditions. Specifically, we use pretrained visual foundation models to extract generalised features across challenging scenes and present an efficient method for matching and integrating these features from stereo frames. Moreover, to prevent pixels violating photometric consistency assumption from negatively affecting the depth predictions, we propose a novel masking approach designed to filter out such pixels. Lastly, addressing weaknesses in the evaluation of current depth estimation algorithms, we present novel evaluation metrics. Our experiments, conducted on challenging datasets including Oxford RobotCar and Multi-Spectral Stereo, demonstrate the robust improvements realized by our approach. Code is available at: https://github.com/madhubabuv/dtd},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C\:\\Users\\theun\\Zotero\\storage\\LHVLN4UF\\Vankadari et al. - 2024 - Dusk Till Dawn Self-supervised Nighttime Stereo Depth Estimation using Visual Foundation Models.pdf;C\:\\Users\\theun\\Zotero\\storage\\3WKJJG5R\\2405.html}
}

@inproceedings{vankadariUnsupervisedMonocularDepth2020,
  title = {Unsupervised {{Monocular Depth Estimation}} for {{Night-Time Images Using Adversarial Domain Feature Adaptation}}},
  booktitle = {Lect. {{Notes Comput}}. {{Sci}}.},
  author = {Vankadari, M. and Garg, S. and Majumder, A. and Kumar, S. and Behera, A.},
  editor = {{Vedaldi A.} and {Bischof H.} and {Brox T.} and {Frahm J.}},
  year = 2020,
  volume = {12373 LNCS},
  pages = {443--459},
  publisher = {{Springer Science and Business Media Deutschland GmbH}},
  doi = {10.1007/978-3-030-58604-1_27},
  abstract = {In this paper, we look into the problem of estimating per-pixel depth maps from unconstrained RGB monocular night-time images which is a difficult task that has not been addressed adequately in the literature. The state-of-the-art day-time depth estimation methods fail miserably when tested with night-time images due to a large domain shift between them. The usual photometric losses used for training these networks may not work for night-time images due to the absence of uniform lighting which is commonly present in day-time images, making it a difficult problem to solve. We propose to solve this problem by posing it as a domain adaptation problem where a network trained with day-time images is adapted to work for night-time images. Specifically, an encoder is trained to generate features from night-time images that are indistinguishable from those obtained from day-time images by using a PatchGAN-based adversarial discriminative learning method. Unlike the existing methods that directly adapt depth prediction (network output), we propose to adapt feature maps obtained from the encoder network so that a pre-trained day-time depth decoder can be directly used for predicting depth from these adapted features. Hence, the resulting method is termed as ``Adversarial Domain Feature Adaptation (ADFA)'' and its efficacy is demonstrated through experimentation on the challenging Oxford night driving dataset. To the best of our knowledge, this work is a first of its kind to estimate depth from unconstrained night-time monocular RGB images that uses a completely unsupervised learning process. The modular encoder-decoder architecture for the proposed ADFA method allows us to use the encoder module as a feature extractor which can be used in many other applications. One such application is demonstrated where the features obtained from our adapted encoder network are shown to outperform other state-of-the-art methods in a visual place recognition problem, thereby, further establishing the usefulness and effectiveness of the proposed approach. \copyright{} 2020, Springer Nature Switzerland AG.},
  isbn = {03029743 (ISSN); 978-303058603-4 (ISBN)},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\7D8Q6WKX\Vankadari et al. - 2020 - Unsupervised Monocular Depth Estimation for Night-Time Images Using Adversarial Domain Feature Adapt.pdf}
}

@inproceedings{vankadariWhenSunGoes2023,
  title = {When the {{Sun Goes Down}}: {{Repairing Photometric Losses}} for {{All-Day Depth Estimation}}},
  shorttitle = {When the {{Sun Goes Down}}},
  booktitle = {Proceedings of {{The}} 6th {{Conference}} on {{Robot Learning}}},
  author = {Vankadari, Madhu and Golodetz, Stuart and Garg, Sourav and Shin, Sangyun and Markham, Andrew and Trigoni, Niki},
  year = 2023,
  month = mar,
  pages = {1992--2003},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-07-04},
  abstract = {Self-supervised deep learning methods for joint depth and ego-motion estimation can yield accurate trajectories without needing ground-truth training data. However, as they typically use photometric losses, their performance can degrade significantly when the assumptions these losses make (e.g. temporal illumination consistency, a static scene, and the absence of noise and occlusions) are violated. This limits their use for e.g. nighttime sequences, which tend to contain many point light sources (including on dynamic objects) and low signal-to-noise ratio (SNR) in darker image regions. In this paper, we show how to use a combination of three techniques to allow the existing photometric losses to work for both day and nighttime images. First, we introduce a per-pixel neural intensity transformation to compensate for the light changes that occur between successive frames. Second, we predict a per-pixel residual flow map that we use to correct the reprojection correspondences induced by the estimated ego-motion and depth from the networks. And third, we denoise the training images to improve the robustness and accuracy of our approach. These changes allow us to train a single model for both day and nighttime images without needing separate encoders or extra feature networks like existing methods. We perform extensive experiments and ablation studies on the challenging Oxford RobotCar dataset to demonstrate the efficacy of our approach for both day and nighttime sequences.},
  langid = {english},
  file = {C\:\\Users\\theun\\Zotero\\storage\\JXIHR854\\Vankadari et al. - 2023 - When the Sun Goes Down Repairing Photometric Losses for All-Day Depth Estimation.pdf;C\:\\Users\\theun\\Zotero\\storage\\VVCTN4S3\\Vankadari et al. - 2023 - When the Sun Goes Down Repairing Photometric Losses for All-Day Depth Estimation.pdf}
}

@article{vargasOverviewAutonomousVehicles2021,
  title = {An {{Overview}} of {{Autonomous Vehicles Sensors}} and {{Their Vulnerability}} to {{Weather Conditions}}},
  author = {Vargas, Jorge and Alsweiss, Suleiman and Toker, Onur and Razdan, Rahul and Santos, Joshua},
  year = 2021,
  month = jan,
  journal = {Sensors},
  volume = {21},
  number = {16},
  pages = {5397},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s21165397},
  urldate = {2025-01-14},
  abstract = {Autonomous vehicles (AVs) rely on various types of sensor technologies to perceive the environment and to make logical decisions based on the gathered information similar to humans. Under ideal operating conditions, the perception systems (sensors onboard AVs) provide enough information to enable autonomous transportation and mobility. In practice, there are still several challenges that can impede the AV sensors' operability and, in turn, degrade their performance under more realistic conditions that actually occur in the physical world. This paper specifically addresses the effects of different weather conditions (precipitation, fog, lightning, etc.) on the perception systems of AVs. In this work, the most common types of AV sensors and communication modules are included, namely: RADAR, LiDAR, ultrasonic, camera, and global navigation satellite system (GNSS). A comprehensive overview of their physical fundamentals, electromagnetic spectrum, and principle of operation is used to quantify the effects of various weather conditions on the performance of the selected AV sensors. This quantification will lead to several advantages in the simulation world by creating more realistic scenarios and by properly fusing responses from AV sensors in any object identification model used in AVs in the physical world. Moreover, it will assist in selecting the appropriate fading or attenuation models to be used in any X-in-the-loop (XIL, e.g., hardware-in-the-loop, software-in-the-loop, etc.) type of experiments to test and validate the manner AVs perceive the surrounding environment under certain conditions.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {autonomous vehicles,camera,GNSS,LiDAR,perception,RADAR,sensors,weather},
  file = {C:\Users\theun\Zotero\storage\L6ZQNMNZ\Vargas et al. - 2021 - An Overview of Autonomous Vehicles Sensors and Their Vulnerability to Weather Conditions.pdf}
}

@article{vaswaniAttentionAllYou,
  title = {Attention Is {{All}} You {{Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\NYZH2ILH\Vaswani et al. - Attention is All you Need.pdf}
}

@incollection{venugopalAnalyzingDeepLearning2025,
  title = {Analyzing {{Deep Learning}} in {{IoT Platform}}},
  booktitle = {Advances in {{Information Communication Technology}} and {{Computing}}},
  author = {Venugopal, Anita and Goar, Vishal Kumar},
  editor = {Goar, Vishal and Kuri, Manoj and Kumar, Rajesh and Senjyu, Tomonobu},
  year = 2025,
  volume = {1075},
  pages = {521--531},
  publisher = {Springer Nature Singapore},
  address = {Singapore},
  doi = {10.1007/978-981-97-6106-7_32},
  urldate = {2024-12-20},
  abstract = {Internet of Things (IoT), a modern technology, is a network of devices with sensors and embedded processors which can work even without the use of internet. It can digitally control the real world wired or wirelessly. It receives attention as IoT-enabled products make human more intelligent and can be involved in different aspects of our lives. Continuous development of IoT domain and large number of devices connected to internet is one of the challenges faced by this technology. Many modern approaches such as computer vision and machine learning have been devised to cope up to meet the challenges. In this work, basic IoT elements are discussed and provide a review of modern addressing method: deep learning (DL) in IoT platform.},
  isbn = {978-981-97-6105-0 978-981-97-6106-7},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\CQ3GVGRI\Venugopal and Goar - 2025 - Analyzing Deep Learning in IoT Platform.pdf}
}

@article{voicuPracticalConsiderationsColor1997,
  title = {Practical Considerations on Color Image Enhancement Using Homomorphic Filtering},
  author = {Voicu, Liviu I. and Myler, Harley R. and Weeks, Arthur Robert},
  year = 1997,
  month = jan,
  journal = {Journal of Electronic Imaging},
  volume = {6},
  number = {1},
  pages = {108--113},
  publisher = {SPIE},
  issn = {1017-9909, 1560-229X},
  doi = {10.1117/12.251157},
  urldate = {2025-05-29},
  abstract = {We present a study concerning the practical possibilities of using the homomorphic filtering for color image enhancement. Two of the most popular color models, RGB and C-Y (color difference), are employed and the results are comparatively discussed. The homomorphic filtering has proven to be a viable tool for both color models considered.},
  file = {C:\Users\theun\Zotero\storage\F59FWU64\Voicu et al. - 1997 - Practical considerations on color image enhancement using homomorphic filtering.pdf}
}

@inproceedings{vunguyenMonoVINISeeingDark2023,
  title = {{{MonoVINI}}: {{Seeing}} in the {{Dark}} with {{Virtual-World Supervision}}},
  shorttitle = {{{MonoVINI}}},
  booktitle = {2023 {{RIVF International Conference}} on {{Computing}} and {{Communication Technologies}} ({{RIVF}})},
  author = {Vu Nguyen, Minh Huy and Tran, Trung Thai and Nguyen, The Hiep and Nguyen, Duc Dung},
  year = 2023,
  month = dec,
  pages = {272--277},
  issn = {2473-0130},
  doi = {10.1109/RIVF60135.2023.10471862},
  urldate = {2025-06-30},
  abstract = {Self-supervised learning draws much attention to Monocular depth estimation (MDE) since it is free of LiDAR annotations and addresses the daytime domain impressively. However, its performance degrades in challenging environments such as night-time scenes, where the assumptions about uniform lighting are no longer valid. Most methods alleviate this problem by adversarial discriminative learning, e.g., closing the gap between the daytime and night-time domain. This paper will address MDE for the night-time domain utilizing simulation data. We overcome the equivalent camera constraints by an image warping technique, making this approach not require a new benchmark dataset. Since a significant domain shift exists between real-world and synthetic data, we use a novel adversarial learning method to relieve this problem. This work is a pioneer in using synthetic data to solve the MDE problem for night-time scenarios. The experimental results demonstrate that our approach produces a comparable effect to state-of-the-art methods, which proves this approach has potential for future research.},
  keywords = {Annotations,Benchmark testing,Cameras,Estimation,Laser radar,Lighting,Monocular depth estimation,self-supervised,Self-supervised learning,synthetic scene},
  file = {C:\Users\theun\Zotero\storage\678DSJN9\Vu Nguyen et al. - 2023 - MonoVINI Seeing in the Dark with Virtual-World Supervision.pdf}
}

@article{wangAdaptiveSemanticSegmentation2024,
  title = {An {{Adaptive Semantic Segmentation Network}} for {{Adversarial Learning Domain Based}} on {{Low-Light Enhancement}} and {{Decoupled Generation}}},
  author = {Wang, Meng and Zhang, Zhuoran and Liu, Haipeng},
  year = 2024,
  month = jan,
  journal = {Applied Sciences},
  volume = {14},
  number = {8},
  pages = {3295},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  doi = {10.3390/app14083295},
  urldate = {2025-02-27},
  abstract = {Nighttime semantic segmentation due to issues such as low contrast, fuzzy imaging, and low-quality annotation results in significant degradation of masks. In this paper, we introduce a domain adaptive approach for nighttime semantic segmentation that overcomes the reliance on low-light image annotations to transfer the source domain model to the target domain. On the front end, a low-light image enhancement sub-network combining lightweight deep learning with mapping curve iteration is adopted to enhance nighttime foreground contrast. In the segmentation network, the body generation and edge preservation branches are implemented to generate consistent representations within the same semantic region. Additionally, a pixel weighting strategy is embedded to increase the prediction accuracy for small targets. During the training, a discriminator is implemented to distinguish features between the source and target domains, thereby guiding the segmentation network for adversarial transfer learning. The proposed approach's effectiveness is verified through testing on Dark Zurich, Nighttime Driving, and CityScapes, including evaluations of mIoU, PSNR, and SSIM. They confirm that our approach surpasses existing baselines in segmentation scenarios.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {/unread,adversarial learning,domain adaptation,low-light enhancement,nighttime semantic segmentation},
  file = {C:\Users\theun\Zotero\storage\3AZENAD7\Wang et al. - 2024 - An Adaptive Semantic Segmentation Network for Adversarial Learning Domain Based on Low-Light Enhance.pdf}
}

@article{wangAWDepthMonocularDepth2024,
  title = {{{AWDepth}}: {{Monocular Depth Estimation}} for {{Adverse Weather}} via {{Masked Encoding}}},
  shorttitle = {{{AWDepth}}},
  author = {Wang, Meng and Qin, Yunchuan and Li, Ruihui and Liu, Zhizhong and Tang, Zhuo and Li, Kenli},
  year = 2024,
  month = sep,
  journal = {IEEE Transactions on Industrial Informatics},
  volume = {20},
  number = {9},
  pages = {10873--10882},
  issn = {1941-0050},
  doi = {10.1109/TII.2024.3397355},
  urldate = {2025-06-30},
  abstract = {Monocular depth estimation has made considerable advances under clear weather conditions. However, how to learn accurate scene depth under rain and fog conditions and alleviate the negative influence of occlusion, light, visibility, etc., is an open problem. To address this problem, in this article, we split the adverse weather depth estimation network into two subbranches: the depth prediction branch and the masked encoding branch. The depth prediction branch is used for depth estimation. The masked encoding branch, inspired by masked image modeling, uses random masks to simulate occlusion or low visibility often seen in rain and fog, forcing this branch to learn to infer the prediction of masked regions from the context. In order to make the masked encoding better enhance the depth prediction, we designed the mask feature fusion module, which can fuse the depth and spatial context features of the two branches to produce a fine-level depth map. The experimental results on the Foggy Cityscapes and RainCityscapes datasets demonstrate that our method achieves state-of-the-art performance, significantly outperforming previous methods across all evaluation metrics.},
  keywords = {Depth estimation,Estimation,Feature extraction,Image coding,Image reconstruction,masked image modeling,Meteorology,swin transformer,Task analysis,Transformers},
  file = {C:\Users\theun\Zotero\storage\MYQGVPIV\Wang et al. - 2024 - AWDepth Monocular Depth Estimation for Adverse Weather via Masked Encoding.pdf}
}

@incollection{wangCDAF3DCrossDimensionalAttention2025,
  title = {{{CDAF3D}}: {{Cross-Dimensional Attention Fusion}} for {{Indoor 3D Object Detection}}},
  shorttitle = {{{CDAF3D}}},
  booktitle = {Pattern {{Recognition}} and {{Computer Vision}}},
  author = {Wang, Shilin and Huang, Hai and Zhu, Yueyan and Tang, Zhenqi},
  editor = {Lin, Zhouchen and Cheng, Ming-Ming and He, Ran and Ubul, Kurban and Silamu, Wushouer and Zha, Hongbin and Zhou, Jie and Liu, Cheng-Lin},
  year = 2025,
  volume = {15043},
  pages = {165--177},
  publisher = {Springer Nature Singapore},
  address = {Singapore},
  doi = {10.1007/978-981-97-8493-6_12},
  urldate = {2024-12-20},
  isbn = {978-981-97-8492-9 978-981-97-8493-6},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\2FZ4F3YF\Wang et al. - 2025 - CDAF3D Cross-Dimensional Attention Fusion for Indoor 3D Object Detection.pdf}
}

@article{wangChannelSelfAttentionBased2024,
  title = {Channel {{Self-Attention Based Low-Light Image Enhancement Network}}},
  author = {Wang, Yan and Su, Peng and Pan, Xiaoying and Wang, Hongyu and Gao, Yuan},
  year = 2024,
  month = may,
  journal = {Computers \& Graphics},
  volume = {120},
  pages = {103921},
  issn = {0097-8493},
  doi = {10.1016/j.cag.2024.103921},
  urldate = {2025-02-27},
  abstract = {Low-light image enhancement is crucial in applications such as traffic safety and medical imaging. Besides having characteristics like low luminance and poor visibility, low-light images are inevitably affected by noise. Noise not only covers image details, introduces artifacts, and decreases image quality, but also interferes with downstream computer vision tasks like object detection, image segmentation, and object tracking. Previous methods in image enhancement often overlook noise handling or fail to accurately suppress noise in adaptive denoising processes, resulting in more severe noise artifacts in the enhanced images. To effectively suppress noise, this paper proposes a Channel Self-Attention Based Low-Light Image Enhancement Network (CAENet), which leverages Transformers and CNNs to model long-range and short-range pixel dependencies, extract global and local features, and construct a Noise Suppression Transformer Block that adaptively suppresses noise regions guided by signal-to-noise ratio priors and attention maps. After adaptive noise suppression, the resulting images exhibit fewer noise artifacts and improved details. The experimental results show that the network in this paper outperforms other state-of-the-art methods overall on five representative paired datasets as well as six unpaired datasets, improving the image quality while effectively suppressing the noise.},
  keywords = {Channel self-attention,Dual branch fusion,Low-light image enhancement,Noise suppression},
  file = {C\:\\Users\\theun\\Zotero\\storage\\IW4ZD5R9\\Wang et al. - 2024 - Channel Self-Attention Based Low-Light Image Enhancement Network.pdf;C\:\\Users\\theun\\Zotero\\storage\\VE87Y7U8\\S0097849324000566.html}
}

@misc{wangComprehensiveReview3D2024,
  title = {A {{Comprehensive Review}} of {{3D Object Detection}} in {{Autonomous Driving}}: {{Technological Advances}} and {{Future Directions}}},
  shorttitle = {A {{Comprehensive Review}} of {{3D Object Detection}} in {{Autonomous Driving}}},
  author = {Wang, Yu and Wang, Shaohua and Li, Yicheng and Liu, Mingchun},
  year = 2024,
  month = aug,
  number = {arXiv:2408.16530},
  eprint = {2408.16530},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.16530},
  urldate = {2024-12-04},
  abstract = {In recent years, 3D object perception has become a crucial component in the development of autonomous driving systems, providing essential environmental awareness. However, as perception tasks in autonomous driving evolve, their variants have increased, leading to diverse insights from industry and academia. Currently, there is a lack of comprehensive surveys that collect and summarize these perception tasks and their developments from a broader perspective. This review extensively summarizes traditional 3D object detection methods, focusing on camera-based, LiDARbased, and fusion detection techniques. We provide a comprehensive analysis of the strengths and limitations of each approach, highlighting advancements in accuracy and robustness. Furthermore, we discuss future directions, including methods to improve accuracy such as temporal perception, occupancy grids, and end-to-end learning frameworks. We also explore cooperative perception methods that extend the perception range through collaborative communication. By providing a holistic view of the current state and future developments in 3D object perception, we aim to offer a more comprehensive understanding of perception tasks for autonomous driving. Additionally, we have established an active repository to provide continuous updates on the latest advancements in this field, accessible at: https://github.com/Fishsoup0/Autonomous-Driving-Perception.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {/unread,Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\theun\Zotero\storage\5UBSMH88\Wang et al. - 2024 - A Comprehensive Review of 3D Object Detection in Autonomous Driving Technological Advances and Futu.pdf}
}

@article{wangDeepHighResolutionRepresentation2021,
  title = {Deep {{High-Resolution Representation Learning}} for {{Visual Recognition}}},
  author = {Wang, Jingdong and Sun, Ke and Cheng, Tianheng and Jiang, Borui and Deng, Chaorui and Zhao, Yang and Liu, Dong and Mu, Yadong and Tan, Mingkui and Wang, Xinggang and Liu, Wenyu and Xiao, Bin},
  year = 2021,
  month = oct,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {43},
  number = {10},
  pages = {3349--3364},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2020.2983686},
  urldate = {2025-06-17},
  abstract = {High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions in series (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams in parallel and (ii) repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at https://github.com/HRNet.},
  keywords = {Convolutional codes,high-resolution representations,HRNet,human pose estimation,Image segmentation,Indexes,low-resolution representations,object detection,Object detection,Pose estimation,semantic segmentation,Semantics,Spatial resolution},
  file = {C:\Users\theun\Zotero\storage\WR85ZFDR\Wang et al. - 2021 - Deep High-Resolution Representation Learning for Visual Recognition.pdf}
}

@inproceedings{wangDETR3D3DObject2022,
  title = {{{DETR3D}}: {{3D Object Detection}} from {{Multi-view Images}} via {{3D-to-2D Queries}}},
  shorttitle = {{{DETR3D}}},
  booktitle = {Proceedings of the 5th {{Conference}} on {{Robot Learning}}},
  author = {Wang, Yue and Guizilini, Vitor Campagnolo and Zhang, Tianyuan and Wang, Yilun and Zhao, Hang and Solomon, Justin},
  year = 2022,
  month = jan,
  pages = {180--191},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-01-03},
  abstract = {We introduce a framework for multi-camera 3D object detection. In contrast to existing works, which estimate 3D bounding boxes directly from monocular images or use depth prediction networks to generate input for 3D object detection from 2D information, our method manipulates predictions directly in 3D space. Our architecture extracts 2D  features from multiple camera images and then uses a sparse set of 3D object queries to index into these 2D features, linking 3D positions to multi-view images using camera transformation matrices. Finally, our model makes a bounding box prediction per object query, using a set-to-set loss to measure the discrepancy between the ground-truth and the prediction. This top-down approach outperforms its bottom-up counterpart in which object bounding box prediction follows per-pixel depth estimation, since it does not suffer from the compounding error introduced by a depth prediction model. Moreover, our method does not require  post-processing such as non-maximum suppression, dramatically improving inference speed. We achieve state-of-the-art performance on the nuScenes autonomous driving benchmark.},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\HU292PT2\Wang et al. - 2022 - DETR3D 3D Object Detection from Multi-view Images via 3D-to-2D Queries.pdf}
}

@inproceedings{wangEffectCameraData2023,
  title = {The {{Effect}} of {{Camera Data Degradation Factors}} on {{Panoptic Segmentation}} for {{Automated Driving}}},
  booktitle = {2023 {{IEEE}} 26th {{International Conference}} on {{Intelligent Transportation Systems}} ({{ITSC}})},
  author = {Wang, Yiting and Zhao, Haonan and Debattista, Kurt and Donzella, Valentina},
  year = 2023,
  month = sep,
  pages = {2351--2356},
  issn = {2153-0017},
  doi = {10.1109/ITSC57777.2023.10421803},
  urldate = {2025-06-18},
  abstract = {Precise scene understanding based on perception sensors' data is important for assisted and automated driving (AAD) functions, to enable accurate decision-making processes and safe navigation. Among various perception tasks using camera images (e.g. object detection, semantic segmentation), panoptic segmentation shows promising scene understanding capability in terms of recognizing and classifying different types and objects, imminent obstacles, and drivable space at a pixel level. While current panoptic segmentation methods exhibit good potential for AAD perception under `ideal' conditions, there are no systematic studies investigating the effects that various degradation factors can have on the quality of the data generated by automotive cameras. Therefore, in this paper, we consider 5 categories of camera data degradation models, namely light level, adverse weather, internal sensor noises, motion blur and compression artefacts. These 5 categories include 11 potential degradation models, with different degradation levels. Based on these 11 models and multiple degradation levels, we synthesize an augmented version of Cityscape, named the Degraded-Cityscapes (D-Cityscapes). Moreover, for the environmental light level, we propose a new synthetic method with generative adversarial learning and zero-reference deep curve estimation to simulate 3 degraded light levels including low light, night light, and extreme light. To compare the effect of the implemented camera degradation factors, we run extensive tests using a panoptic segmentation network (i.e. EfficientPS), quantifying how the performance metrics vary when the data are degraded. Based on the evaluation results, we demonstrate that extreme snow, blur, and light are the most threatening conditions for panoptic segmentation in AAD, while EfficientPS can cope well with light fog, compression, and blur, which can provide insights for future research directions.},
  keywords = {Assisted drivings,Automated driving,Cameras,Condition,Data degradation,Decision making,Degradation,Degradation factor,Degradation model,Driving functions,Light level,Meteorology,Motion segmentation,Object detection,Object recognition,Petroleum reservoir evaluation,Robustness,Scene understanding,Semantic Segmentation,Semantics,Sensors,Sensors data,Snow},
  file = {C:\Users\theun\Zotero\storage\ECCIHY24\Wang et al. - 2023 - The Effect of Camera Data Degradation Factors on Panoptic Segmentation for Automated Driving.pdf}
}

@article{wangExperimentBasedReviewLowLight2020,
  title = {An {{Experiment-Based Review}} of {{Low-Light Image Enhancement Methods}}},
  author = {Wang, Wencheng and Wu, Xiaojin and Yuan, Xiaohui and Gao, Zairui},
  year = 2020,
  journal = {IEEE Access},
  volume = {8},
  pages = {87884--87917},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2992749},
  urldate = {2024-12-04},
  abstract = {Images captured under poor illumination conditions often exhibit characteristics such as low brightness, low contrast, a narrow gray range, and color distortion, as well as considerable noise, which seriously affect the subjective visual effect on human eyes and greatly limit the performance of various machine vision systems. The role of low-light image enhancement is to improve the visual effect of such images for the benefit of subsequent processing. This paper reviews the main techniques of low-light image enhancement developed over the past decades. First, we present a new classification of these algorithms, dividing them into seven categories: gray transformation methods, histogram equalization methods, Retinex methods, frequency-domain methods, image fusion methods, defogging model methods and machine learning methods. Then, all the categories of methods, including subcategories, are introduced in accordance with their principles and characteristics. In addition, various quality evaluation methods for enhanced images are detailed, and comparisons of different algorithms are discussed. Finally, the current research progress is summarized, and future research directions are suggested.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\DGZSKAY4\Wang et al. - 2020 - An Experiment-Based Review of Low-Light Image Enhancement Methods.pdf}
}

@inproceedings{wangExposureDiffusionLearningExpose2023,
  title = {{{ExposureDiffusion}}: {{Learning}} to {{Expose}} for {{Low-light Image Enhancement}}},
  shorttitle = {{{ExposureDiffusion}}},
  booktitle = {2023 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Wang, Yufei and Yu, Yi and Yang, Wenhan and Guo, Lanqing and Chau, Lap-Pui and Kot, Alex C. and Wen, Bihan},
  year = 2023,
  month = oct,
  pages = {12404--12414},
  publisher = {IEEE},
  address = {Paris, France},
  doi = {10.1109/ICCV51070.2023.01143},
  urldate = {2025-03-10},
  abstract = {Previous raw image-based low-light image enhancement methods predominantly relied on feed-forward neural networks to learn deterministic mappings from low-light to normally-exposed images. However, they failed to capture critical distribution information, leading to visually undesirable results. This work addresses the issue by seamlessly integrating a diffusion model with a physics-based exposure model. Different from a vanilla diffusion model that has to perform Gaussian denoising, with the injected physics-based exposure model, our restoration process can directly start from a noisy image instead of pure noise. As such, our method obtains significantly improved performance and reduced inference time compared with vanilla diffusion models. To make full use of the advantages of different intermediate steps, we further propose an adaptive residual layer that effectively screens out the side-effect in the iterative refinement when the intermediate results have been already well-exposed. The proposed framework can work with both real-paired datasets, SOTA noise models, and different backbone networks. We evaluate the proposed method on various public benchmarks, achieving promising results with consistent improvements using different exposure models and backbones. Besides, the proposed method achieves better generalization capacity for unseen amplifying ratios and better performance than a larger feedforward neural model when few parameters are adopted. The code is released at https://github.com/wyf0912/ ExposureDiffusion.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-0718-4},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\SF5LJUZ2\Wang et al. - 2023 - ExposureDiffusion Learning to Expose for Low-light Image Enhancement.pdf}
}

@article{wangILENetIlluminationModulatedLaplacianPyramid2025,
  title = {{{ILENet}}: {{Illumination-Modulated Laplacian-Pyramid Enhancement Network}} for Low-Light Object Detection},
  shorttitle = {{{ILENet}}},
  author = {Wang, Xiaofeng and Yang, Rentao and Wu, Zhize and Sun, Lingma and Liu, Jiashan and Zou, Le},
  year = 2025,
  month = may,
  journal = {Expert Systems with Applications},
  volume = {271},
  pages = {126504},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2025.126504},
  urldate = {2025-06-18},
  abstract = {With the rapid advancement of deep learning, object detection methods have made significant progress on traditional datasets. However, these methods still face considerable challenges when applied to images captured in low-light conditions. To address this issue, we propose a new method called the Illumination-Modulated Laplacian-Pyramid Enhancement Network (ILENet), specifically for object detection in low-light environments. In ILENet, we design two key components: the Laplacian Enhancement Pyramid (LEP) and the Illumination Modulation Module (IMM). LEP enhances images by optimizing information at different frequencies in low-light images and leveraging local features to improve brightness. IMM further enhances the image by globally querying multi-scale information within the same semantics to generate correction parameters. We integrate the proposed ILENet with a standard YOLO detector, forming a new detection framework. This framework employs a joint training strategy to effectively balance low-light image enhancement (LLIE) and object detection tasks, without requiring paired low-light and normal-light images for pre-training the enhancement network. Quantitative experiments on low-light object detection datasets demonstrate that ILENet outperforms other mainstream LLIE and low-light object detection methods, achieving state-of-the-art (SOTA) performance with an accuracy of 78.5\%. Additionally, experimental results on low-light semantic segmentation tasks further validate the effectiveness of ILENet.},
  keywords = {Illumination correction,Image enhancement,Laplace transforms,Laplacian pyramid,Laplacian Pyramid,Laplacians,Learning objects,Low light,Low-light image enhancement,Low-light images,Object detection,Object detection method,Object recognition,Objects detection,Semantic segmentation,Semantic Segmentation},
  file = {C:\Users\theun\Zotero\storage\WBRYFTYY\Wang et al. - 2025 - ILENet Illumination-Modulated Laplacian-Pyramid Enhancement Network for low-light object detection.pdf}
}

@article{wangImageQualityAssessment2004,
  title = {Image Quality Assessment: From Error Visibility to Structural Similarity},
  shorttitle = {Image Quality Assessment},
  author = {Wang, Zhou and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  year = 2004,
  month = apr,
  journal = {IEEE Transactions on Image Processing},
  volume = {13},
  number = {4},
  pages = {600--612},
  issn = {1941-0042},
  doi = {10.1109/TIP.2003.819861},
  urldate = {2025-07-03},
  abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.},
  keywords = {/unread,Data mining,Degradation,Humans,Image quality,Indexes,Layout,Quality assessment,Transform coding,Visual perception,Visual system},
  file = {C:\Users\theun\Zotero\storage\H4XC2JCI\Wang et al. - 2004 - Image quality assessment from error visibility to structural similarity.pdf}
}

@inproceedings{wangInformativeClassesMatter2023,
  title = {Informative {{Classes Matter}}: {{Towards Unsupervised Domain Adaptive Nighttime Semantic Segmentation}}},
  shorttitle = {Informative {{Classes Matter}}},
  booktitle = {Proceedings of the 31st {{ACM International Conference}} on {{Multimedia}}},
  author = {Wang, Shiqin and Xu, Xin and Ma, Xianzheng and Jiang, Kui and Wang, Zheng},
  year = 2023,
  month = oct,
  series = {{{MM}} '23},
  pages = {163--172},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3581783.3611956},
  urldate = {2025-06-18},
  abstract = {Unsupervised Domain Adaptive Nighttime Semantic Segmentation (UDA-NSS) aims to adapt a robust model from a labeled daytime domain to an unlabeled nighttime domain. However, current advanced segmentation methods ignore the illumination effect and class discrepancies of different semantic classes during domain adaptation, showing an uneven prediction phenomenon. It is the completely ignored and underexplored issues of ''hard-to-adapt'' classes that some classes have a large performance gap between existing UDA-NSS methods and supervised learning counterparts while others have a very low performance gap. To realize ''hard-to-adapt'' classes' more sufficient learning and facilitate the UDA-NSS task, we present an Online Informative Class Sampling (OICS) strategy to adaptively mine informative classes from the target nighttime domain according to the corresponding spectrogram mean and the class frequency via our Informative Mixture of Experts. Furthermore, an Informativeness-based cross-domain Mixed Sampling (InforMS) framework is designed to focus on informative classes from the target nighttime domain by vesting their higher sampling probabilities when cross-domain mixing sampling and achieves better performance in UDA-NSS tasks. Consequently, our method outperforms state-of-the-art UDA-NSS methods by large margins on three widely-used benchmarks (e.g., ACDC, Dark Zurich, and Nighttime Driving). Notably, our method achieves state-of-the-art performance with 65.1\% mIoU on ACDC-night-test and 55.4\% mIoU on ACDC-night-val.},
  isbn = {979-8-4007-0108-5},
  keywords = {'current,class sampling,Class sampling,Cross-domain,Illumination effect,Performance gaps,Robust modeling,Segmentation methods,Semantic class,Semantic segmentation,Semantic Segmentation,Semantics,unsupervised domain adaptive nighttime semantic segmentation,Unsupervised domain adaptive nighttime semantic segmentation},
  file = {C:\Users\theun\Zotero\storage\ZQ8Q7YG5\Wang et al. - 2023 - Informative Classes Matter Towards Unsupervised Domain Adaptive Nighttime Semantic Segmentation.pdf}
}

@misc{wangLearningbasedMultiViewStereo2024,
  title = {Learning-Based {{Multi-View Stereo}}: {{A Survey}}},
  shorttitle = {Learning-Based {{Multi-View Stereo}}},
  author = {Wang, Fangjinhua and Zhu, Qingtian and Chang, Di and Gao, Quankai and Han, Junlin and Zhang, Tong and Hartley, Richard and Pollefeys, Marc},
  year = 2024,
  month = dec,
  number = {arXiv:2408.15235},
  eprint = {2408.15235},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.15235},
  urldate = {2025-03-19},
  abstract = {3D reconstruction aims to recover the dense 3D structure of a scene. It plays an essential role in various applications such as Augmented/Virtual Reality (AR/VR), autonomous driving and robotics. Leveraging multiple views of a scene captured from different viewpoints, Multi-View Stereo (MVS) algorithms synthesize a comprehensive 3D representation, enabling precise reconstruction in complex environments. Due to its efficiency and effectiveness, MVS has become a pivotal method for image-based 3D reconstruction. Recently, with the success of deep learning, many learning-based MVS methods have been proposed, achieving impressive performance against traditional methods. We categorize these learning-based methods as: depth map-based, voxel-based, NeRF-based, 3D Gaussian Splatting-based, and large feed-forward methods. Among these, we focus significantly on depth map-based methods, which are the main family of MVS due to their conciseness, flexibility and scalability. In this survey, we provide a comprehensive review of the literature at the time of this writing. We investigate these learning-based methods, summarize their performances on popular benchmarks, and discuss promising future research directions in this area.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\HB8ECLGY\\Wang et al. - 2024 - Learning-based Multi-View Stereo A Survey.pdf;C\:\\Users\\theun\\Zotero\\storage\\2STG49PL\\2408.html}
}

@inproceedings{wangLearningParallaxAttention2019,
  title = {Learning {{Parallax Attention}} for {{Stereo Image Super-Resolution}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Wang, Longguang and Wang, Yingqian and Liang, Zhengfa and Lin, Zaiping and Yang, Jungang and An, Wei and Guo, Yulan},
  year = 2019,
  pages = {12250--12259},
  urldate = {2025-03-23},
  file = {C:\Users\theun\Zotero\storage\ZW9AIAR9\Wang et al. - 2019 - Learning Parallax Attention for Stereo Image Super-Resolution.pdf}
}

@article{wangLoliMVSEndtoEndNetwork2024,
  title = {{{LoliMVS}}: {{An End-to-End Network}} for {{Multiview Stereo With Low-Light Images}}},
  shorttitle = {{{LoliMVS}}},
  author = {Wang, Yangang and Jiang, Qingfang},
  year = 2024,
  journal = {IEEE Transactions on Instrumentation and Measurement},
  volume = {73},
  pages = {1--11},
  issn = {1557-9662},
  doi = {10.1109/TIM.2024.3351255},
  urldate = {2025-05-05},
  abstract = {Performing multiview stereo (MVS) reconstruction under a low-light environment is challenging. Different from traditional MVS methods that work with images captured under normal lighting conditions, we focus on reconstructing 3-D models in low-light situations. To address this, we propose a learning-based MVS framework consisting of two steps: low-light image enhancement and MVS reconstruction. At first, we adopt an encoder-decoder network to enhance the low-light images, making them more visually discernible. Then, the encoder is fixed and shared, facilitating to train a decoder for MVS reconstruction. To validate our approach, we have created a new dataset called LoLi100, specifically designed for low-light reconstruction. In our experiments, we train and test our method on this dataset, demonstrating that our pipeline generates 3-D models with fine details and clear texture. Compared to other methods, our approach significantly improves completeness and overall quality of the depth maps. The dataset is publicly available at https://www.yangangwang.com.},
  keywords = {3-D dataset,3-D reconstruction,Cameras,Costs,Feature extraction,image enhancement,Image enhancement,Image reconstruction,low light,multiview stereo (MVS),Three-dimensional displays},
  file = {C:\Users\theun\Zotero\storage\PQC5IAMC\Wang and Jiang - 2024 - LoliMVS An End-to-End Network for Multiview Stereo With Low-Light Images.pdf}
}

@article{wangLowLightImageEnhancement2019,
  title = {Low-{{Light Image Enhancement}} via the {{Absorption Light Scattering Model}}},
  author = {Wang, Yun-Fei and Liu, He-Ming and Fu, Zhao-Wang},
  year = 2019,
  month = nov,
  journal = {IEEE Transactions on Image Processing},
  volume = {28},
  number = {11},
  pages = {5679--5690},
  issn = {1941-0042},
  doi = {10.1109/TIP.2019.2922106},
  urldate = {2025-02-19},
  abstract = {Low light often leads to poor image visibility, which can easily affect the performance of computer vision algorithms. First, this paper proposes the absorption light scattering model (ALSM), which can be used to reasonably explain the absorbed light imaging process for low-light images. In addition, the absorbing light scattering image obtained via ALSM under a sufficient and uniform illumination can reproduce hidden outlines and details from the low-light image. Then, we identify that the minimum channel of ALSM obtained above exhibits high local similarity. This similarity can be constrained by superpixels, which effectively prevent the use of gradient operations at the edges so that the noise is not amplified quickly during enhancement. Finally, by analyzing the monotonicity between the scene reflection and the atmospheric light or transmittance in ALSM, a new low-light image enhancement method is identified. We replace atmospheric light with inverted atmospheric light to reduce the contribution of atmospheric light in the imaging results. Moreover, a soft jointed mean-standard-deviation (MSD) mechanism is proposed that directly acts on the patches represented by the superpixels. The MSD can obtain a smaller transmittance than that obtained by the minimum strategy, and it can be automatically adjusted according to the information of the image. The experiments on challenging low-light images are conducted to reveal the advantages of our method compared with other powerful techniques.},
  keywords = {Absorption,absorption-light-scattering-model,Atmospheric modeling,Image color analysis,Image enhancement,Imaging,Lighting,Low-light image enhancement,Mathematical model,mean-standard-deviation,minimal channel},
  file = {C\:\\Users\\theun\\Zotero\\storage\\KIMSB9DA\\Wang et al. - 2019 - Low-Light Image Enhancement via the Absorption Light Scattering Model.pdf;C\:\\Users\\theun\\Zotero\\storage\\TTLWCEKW\\8737871.html}
}

@inproceedings{wangLowLightImageEnhancement2023,
  title = {Low-{{Light Image Enhancement}} with {{Illumination-Aware Gamma Correction}} and {{Complete Image Modelling Network}}},
  booktitle = {2023 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Wang, Yinglong and Liu, Zhen and Liu, Jianzhuang and Xu, Songcen and Liu, Shuaicheng},
  year = 2023,
  month = oct,
  pages = {13082--13091},
  publisher = {IEEE},
  address = {Paris, France},
  doi = {10.1109/ICCV51070.2023.01207},
  urldate = {2025-10-17},
  abstract = {This paper presents a novel network structure with illumination-aware gamma correction and complete image modelling to solve the low-light image enhancement problem. Low-light environments usually lead to less informative large-scale dark areas, directly learning deep representations from low-light images is insensitive to recovering normal illumination. We propose to integrate the effectiveness of gamma correction with the strong modelling capacities of deep networks, which enables the correction factor gamma to be learned in a coarse to elaborate manner via adaptively perceiving the deviated illumination. Because exponential operation introduces high computational complexity, we propose to use Taylor Series to approximate gamma correction, accelerating the training and inference speed. Dark areas usually occupy large scales in low-light images, common local modelling structures, e.g., CNN, SwinIR, are thus insufficient to recover accurate illumination across whole low-light images. We propose a novel Transformer block to completely simulate the dependencies of all pixels across images via a local-to-global hierarchical attention mechanism, so that dark areas could be inferred by borrowing the information from far informative regions in a highly effective manner. Extensive experiments on several benchmark datasets demonstrate that our approach outperforms state-of-the-art methods.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-0718-4},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\5QIAPWHW\Wang et al. - 2023 - Low-Light Image Enhancement with Illumination-Aware Gamma Correction and Complete Image Modelling Ne.pdf}
}

@inproceedings{wangMultimodalLowlightImage2024,
  title = {Multimodal {{Low-light Image Enhancement}} with {{Depth Information}}},
  booktitle = {Proceedings of the 32nd {{ACM International Conference}} on {{Multimedia}}},
  author = {Wang, Zhen and Li, Dongyuan and Li, Guang and Zhang, Ziqing and Jiang, Renhe},
  year = 2024,
  month = oct,
  series = {{{MM}} '24},
  pages = {4976--4985},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3664647.3680741},
  urldate = {2025-03-15},
  abstract = {Low-light image enhancement has been researched several years. However, current image restoration methods predominantly focus on recovering images from RGB images, overlooking the potential of incorporating more modalities. With the advancements in personal handheld devices, we can now easily capture images with depth information using devices such as mobile phones. The integration of depth information into image restoration is a research question worthy of exploration. Therefore, in this paper, we propose a multimodal low-light image enhancement task based on depth information and establish a dataset named LED (Low-light Image Enhanced with Depth Map), consisting of 1,365 samples. Each sample in our dataset includes a low-light image, a normal-light image, and the corresponding depth map. Moreover, for the LED dataset, we design a corresponding multimodal method, which can processes the input images and depth map information simultaneously to generate the predicted normal-light images. Experimental results and detailed ablation studies proves the efficiency of our method which exceeds previous single-modal state-of-the arts methods from relevant field.},
  isbn = {979-8-4007-0686-8},
  file = {C:\Users\theun\Zotero\storage\5VFZ9LY9\Wang et al. - 2024 - Multimodal Low-light Image Enhancement with Depth Information.pdf}
}

@article{wangMultiviewStereoDeep2021,
  title = {Multi-View Stereo in the {{Deep Learning Era}}: {{A}} Comprehensive Review},
  shorttitle = {Multi-View Stereo in the {{Deep Learning Era}}},
  author = {Wang, Xiang and Wang, Chen and Liu, Bing and Zhou, Xiaoqing and Zhang, Liang and Zheng, Jin and Bai, Xiao},
  year = 2021,
  month = dec,
  journal = {Displays},
  volume = {70},
  pages = {102102},
  issn = {0141-9382},
  doi = {10.1016/j.displa.2021.102102},
  urldate = {2025-03-19},
  abstract = {Multi-view stereo infers the 3D geometry from a set of images captured from several known positions and viewpoints. It is one of the most important components of 3D reconstruction. Recently, deep learning has been increasingly used to solve several 3D vision problems due to the predominating performance, including the multi-view stereo problem. This paper presents a comprehensive review, covering recent deep learning methods for multi-view stereo. These methods are mainly categorized into depth map based and volumetric based methods according to the 3D representation form, and representative methods are reviewed in detail. Specifically, the plane sweep based methods leveraging depth maps are presented following the stage of approaches, i.e. feature extraction, cost volume construction, cost volume regularization, depth map regression and post-processing. This review also summarizes several widely used datasets and their corresponding metrics for evaluation. Finally, several insightful observations and challenges are put forward enlightening future research directions.},
  keywords = {/unread,3D Reconstruction,Deep Learning,Multi-view Stereo,Plane Sweep,Volumetric Representation},
  file = {C\:\\Users\\theun\\Zotero\\storage\\25R5L5KX\\Wang et al. - 2021 - Multi-view stereo in the Deep Learning Era A comprehensive review.pdf;C\:\\Users\\theun\\Zotero\\storage\\SIXVAUYG\\S0141938221001062.html}
}

@article{wangNaturalnessPreservedEnhancement2013,
  title = {Naturalness {{Preserved Enhancement Algorithm}} for {{Non-Uniform Illumination Images}}},
  author = {Wang, Shuhang and Zheng, Jin and Hu, Hai-Miao and Li, Bo},
  year = 2013,
  month = sep,
  journal = {IEEE Transactions on Image Processing},
  volume = {22},
  number = {9},
  pages = {3538--3548},
  issn = {1941-0042},
  doi = {10.1109/TIP.2013.2261309},
  urldate = {2025-03-11},
  abstract = {Image enhancement plays an important role in image processing and analysis. Among various enhancement algorithms, Retinex-based algorithms can efficiently enhance details and have been widely adopted. Since Retinex-based algorithms regard illumination removal as a default preference and fail to limit the range of reflectance, the naturalness of non-uniform illumination images cannot be effectively preserved. However, naturalness is essential for image enhancement to achieve pleasing perceptual quality. In order to preserve naturalness while enhancing details, we propose an enhancement algorithm for non-uniform illumination images. In general, this paper makes the following three major contributions. First, a lightness-order-error measure is proposed to access naturalness preservation objectively. Second, a bright-pass filter is proposed to decompose an image into reflectance and illumination, which, respectively, determine the details and the naturalness of the image. Third, we propose a bi-log transformation, which is utilized to map the illumination to make a balance between details and naturalness. Experimental results demonstrate that the proposed algorithm can not only enhance the details but also preserve the naturalness for non-uniform illumination images.},
  keywords = {/unread,Bi-log transformation,bright-pass filter,Brightness,Histograms,Image coding,Image color analysis,image enhancement,Image enhancement,Light sources,Lighting,lightness-order-error measure,naturalness},
  file = {C\:\\Users\\theun\\Zotero\\storage\\WRLBVS9F\\Wang et al. - 2013 - Naturalness Preserved Enhancement Algorithm for Non-Uniform Illumination Images.pdf;C\:\\Users\\theun\\Zotero\\storage\\SBAW8DI6\\6512558.html}
}

@article{wangPerceptualEnhancementUnsupervised2025,
  title = {Perceptual {{Enhancement}} for {{Unsupervised Monocular Visual Odometry}}},
  author = {Wang, Zhongyi and Shen, Mengjiao and Liu, Chengju and Chen, Qijun},
  year = 2025,
  month = jan,
  journal = {International Journal of Control, Automation and Systems},
  volume = {23},
  number = {1},
  pages = {346--357},
  issn = {2005-4092},
  doi = {10.1007/s12555-024-0495-y},
  urldate = {2025-02-17},
  abstract = {Visual odometry is pivotal in robotics and autonomous driving, serving as a key component of visual simultaneous localization and mapping technology. In real-world scenarios, humans in local low-light conditions perceive less information, which can impact our judgments and actions. Similarly, visual odometry can become confused under these conditions, leading to compromised performance. To address the challenges posed by local low-light images on monocular visual odometry, we propose an unsupervised framework for monocular visual odometry. To the best of our knowledge, this is the first instance of unsupervised monocular visual odometry and local low-light image enhancement accomplished within a unified framework. Initially, we employ retinex theory and the discrete Fourier transform to decompose, filter, and synthesize the original image. For the filtering process, we propose a novel learnable global filtering network. Subsequently, we input the enhanced images into the depth and pose networks, generating the corresponding depth maps and inter-frame poses. Ultimately, we construct a photometric consistency loss, a depth loss, and a novel low-light smoothness loss to train the entire network. Through experimental validation, our method exhibits superior performance on the KITTI dataset. Furthermore, it demonstrates satisfactory generalization ability in unseen environments from the Oxford RobotCar dataset.},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\M29NWEUS\Wang et al. - 2025 - Perceptual Enhancement for Unsupervised Monocular Visual Odometry.pdf}
}

@article{wangPerformanceChallenges3D2023,
  title = {Performance and {{Challenges}} of {{3D Object Detection Methods}} in {{Complex Scenes}} for {{Autonomous Driving}}},
  author = {Wang, Ke and Zhou, Tianqiang and Li, Xingcan and Ren, Fan},
  year = 2023,
  month = feb,
  journal = {IEEE Transactions on Intelligent Vehicles},
  volume = {8},
  number = {2},
  pages = {1699--1716},
  issn = {2379-8904},
  doi = {10.1109/TIV.2022.3213796},
  urldate = {2025-01-15},
  abstract = {How to ensure robust and accurate 3D object detection under various environment is essential for autonomous driving (AD) environment perception. While, until now, most of the existing 3D object detection methods are based on the ordinary driving scenes provided by the mainstream dataset. The researches on actual complex scenes (adverse illumination, inclement weather, distant or small objects, etc.) have been ignored and there is still no comprehensive review of the recent progress in this field. Thence, this paper aims to gain a deep insight on the performance and challenges of 3D object detection methods under complex scenes for AD. Firstly, we discuss the complex driving environments in actual and the perception limitations of mainstream sensors (LIDAR and camera). Then we analyze the performance and challenges of single-modality 3D object detection methods. Therefore, in order to improve the accuracy and robustness of 3D object detection methods in some complex AD scenes, the fusion of L-C (LIDAR-camera) is recommended and systematically analyzed. Finally, some suitable datasets and potential directions are comparatively summarized to support the relative research in complex driving scenes. We hope that this review could facilitate people's research and look forward to more progress in this timely and crucial problem field.},
  keywords = {3D object detection,autonomous driving,Autonomous vehicles,Cameras,Complex scenes,datasets,Laser radar,Lighting,Meteorology,multimodal fusion,Object detection,Three-dimensional displays},
  file = {C\:\\Users\\theun\\Zotero\\storage\\WKN2K9GZ\\Wang et al. - 2023 - Performance and Challenges of 3D Object Detection Methods in Complex Scenes for Autonomous Driving.pdf;C\:\\Users\\theun\\Zotero\\storage\\D49UU5DW\\9917362.html}
}

@inproceedings{wangRegularizingNighttimeWeirdness2021,
  title = {Regularizing {{Nighttime Weirdness}}: {{Efficient Self-Supervised Monocular Depth Estimation}} in the {{Dark}}},
  shorttitle = {Regularizing {{Nighttime Weirdness}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Wang, Kun and Zhang, Zhenyu and Yan, Zhiqiang and Li, Xiang and Xu, Baobei and Li, Jun and Yang, Jian},
  year = 2021,
  pages = {16055--16064},
  urldate = {2025-06-29},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\M3KUTIWV\Wang et al. - 2021 - Regularizing Nighttime Weirdness Efficient Self-Supervised Monocular Depth Estimation in the Dark.pdf}
}

@article{wangReviewVehicleDetection2023,
  title = {A {{Review}} of {{Vehicle Detection Techniques}} for {{Intelligent Vehicles}}},
  author = {Wang, Zhangu and Zhan, Jun and Duan, Chunguang and Guan, Xin and Lu, Pingping and Yang, Kai},
  year = 2023,
  month = aug,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {34},
  number = {8},
  pages = {3811--3831},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2021.3128968},
  urldate = {2025-01-02},
  abstract = {Robust and efficient vehicle detection is an important task of environment perception of intelligent vehicles, which directly affects the behavior decision-making and motion planning of intelligent vehicles. Due to the rapid development of sensor and computer technology, the algorithm and technology of vehicle detection have been updated rapidly. But, there are few reviews on vehicle detection of intelligent vehicles, especially covering all kinds of sensors and algorithms in recent years. This article presents a comprehensive review of vehicle detection approaches and their applications in intelligent vehicle systems to analyze the development of vehicle detection, with a specific focus on sensor types and algorithm classification. First, more than 300 research contributions are summarized in this review, including all kinds of vehicle detection sensors (machine vision, millimeter-wave radar, lidar, and multisensor fusion), and the performance of the classic and latest algorithms was compared in detail. Then, the application scenarios of vehicle detection with different sensors and algorithms were analyzed according to their performance and applicability. Moreover, we also systematically summarized the methods of vehicle detection in adverse weather. Finally, the remaining challenges and future research trends were analyzed according to the development of intelligent vehicle sensors and algorithms.},
  keywords = {/unread,Deep learning,Feature extraction,Image color analysis,information fusion,intelligent vehicle,Intelligent vehicles,Machine vision,Semantics,sensors,vehicle detection,Vehicle detection},
  file = {C\:\\Users\\theun\\Zotero\\storage\\WGAL93C6\\Wang et al. - 2023 - A Review of Vehicle Detection Techniques for Intelligent Vehicles.pdf;C\:\\Users\\theun\\Zotero\\storage\\7Z4K58B8\\9670465.html}
}

@article{wangSFNetNImprovedSFNet2022,
  title = {{{SFNet-N}}: {{An Improved SFNet Algorithm}} for {{Semantic Segmentation}} of {{Low-Light Autonomous Driving Road Scenes}}},
  shorttitle = {{{SFNet-N}}},
  author = {Wang, Hai and Chen, Yanyan and Cai, Yingfeng and Chen, Long and Li, Yicheng and Sotelo, Miguel Angel and Li, Zhixiong},
  year = 2022,
  month = nov,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {23},
  number = {11},
  pages = {21405--21417},
  issn = {1558-0016},
  doi = {10.1109/TITS.2022.3177615},
  urldate = {2025-02-27},
  abstract = {In recent years, considerable progress has been made in semantic segmentation of images with favorable environments. However, the environmental perception of autonomous driving under adverse weather conditions is still very challenging. In particular, the low visibility at nighttime greatly affects driving safety. In this paper, we aim to explore image segmentation in low-light scenarios, thereby expanding the application range of autonomous vehicles. The segmentation algorithms for road scenes based on deep learning are highly dependent on the volume of images with pixel-level annotations. Considering the scarcity of labeled large-scale nighttime data, we performed synthetic data collection and data style transfer using images acquired in daytime based on the autonomous driving simulation platform and generative adversarial network, respectively. In addition, we also proposed a novel nighttime segmentation framework (SFNET-N) to effectively recognize objects in dark environments, aiming at the boundary blurring caused by low semantic contrast in low-illumination images. Specifically, the framework comprises a light enhancement network which introduces semantic information for the first time and a segmentation network with strong feature extraction capability. Extensive experiments with Dark Zurich-test and Nighttime Driving-test datasets show the effectiveness of our method compared with existing state-of-the art approaches, with 56.9\% and 57.4\% mIoU (mean of category-wise intersection-over-union) respectively. Finally, we also performed real-vehicle verification of the proposed models in road scenes of Zhenjiang city with poor lighting. The datasets are available at https://github.com/pupu-chenyanyan/semantic-segmentation-on-nightime.},
  keywords = {Annotations,autonomous driving,Autonomous vehicles,deep learning,Image segmentation,Lighting,low visible light,Meteorology,Roads,Semantic segmentation,Semantics},
  file = {C\:\\Users\\theun\\Zotero\\storage\\55HSHSIA\\Wang et al. - 2022 - SFNet-N An Improved SFNet Algorithm for Semantic Segmentation of Low-Light Autonomous Driving Road.pdf;C\:\\Users\\theun\\Zotero\\storage\\4EDFHVJZ\\9784832.html}
}

@inproceedings{wangSymmetricParallaxAttention2021,
  title = {Symmetric {{Parallax Attention}} for {{Stereo Image Super-Resolution}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Wang, Yingqian and Ying, Xinyi and Wang, Longguang and Yang, Jungang and An, Wei and Guo, Yulan},
  year = 2021,
  pages = {766--775},
  urldate = {2025-03-23},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\3TDMKEQE\Wang et al. - 2021 - Symmetric Parallax Attention for Stereo Image Super-Resolution.pdf}
}

@inproceedings{wangUformerGeneralUShaped2022,
  title = {Uformer: {{A General U-Shaped Transformer}} for {{Image Restoration}}},
  shorttitle = {Uformer},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wang, Zhendong and Cun, Xiaodong and Bao, Jianmin and Zhou, Wengang and Liu, Jianzhuang and Li, Houqiang},
  year = 2022,
  month = jun,
  pages = {17662--17672},
  publisher = {IEEE},
  address = {New Orleans, LA, USA},
  doi = {10.1109/CVPR52688.2022.01716},
  urldate = {2025-03-12},
  abstract = {In this paper, we present Uformer, an effective and efficient Transformer-based architecture for image restoration, in which we build a hierarchical encoder-decoder network using the Transformer block. In Uformer, there are two core designs. First, we introduce a novel locally-enhanced window (LeWin) Transformer block, which performs nonoverlapping window-based self-attention instead of global self-attention. It significantly reduces the computational complexity on high resolution feature map while capturing local context. Second, we propose a learnable multi-scale restoration modulator in the form of a multi-scale spatial bias to adjust features in multiple layers of the Uformer decoder. Our modulator demonstrates superior capability for restoring details for various image restoration tasks while introducing marginal extra parameters and computational cost. Powered by these two designs, Uformer enjoys a high capability for capturing both local and global dependencies for image restoration. To evaluate our approach, extensive experiments are conducted on several image restoration tasks, including image denoising, motion deblurring, defocus deblurring and deraining. Without bells and whistles, our Uformer achieves superior or comparable performance compared with the state-of-the-art algorithms. The code and models are available at https: //github.com/ZhendongWang6/Uformer.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-6946-3},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\YI2RHSW3\Wang et al. - 2022 - Uformer A General U-Shaped Transformer for Image Restoration.pdf}
}

@article{wangUltraHighDefinitionLowLightImage2023,
  title = {Ultra-{{High-Definition Low-Light Image Enhancement}}: {{A Benchmark}} and {{Transformer-Based Method}}},
  shorttitle = {Ultra-{{High-Definition Low-Light Image Enhancement}}},
  author = {Wang, Tao and Zhang, Kaihao and Shen, Tianrun and Luo, Wenhan and Stenger, Bjorn and Lu, Tong},
  year = 2023,
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {37},
  number = {3},
  pages = {2654--2662},
  issn = {2374-3468},
  doi = {10.1609/aaai.v37i3.25364},
  urldate = {2025-08-07},
  abstract = {As the quality of optical sensors improves, there is a need for processing large-scale images. In particular, the ability of devices to capture ultra-high definition (UHD) images and video places new demands on the image processing pipeline. In this paper, we consider the task of low-light image enhancement (LLIE) and introduce a large-scale database consisting of images at 4K and 8K resolution. We conduct systematic benchmarking studies and provide a comparison of current LLIE algorithms. As a second contribution, we introduce LLFormer, a transformer-based low-light enhancement method. The core components of LLFormer are the axis-based multi-head self-attention and cross-layer attention fusion block, which significantly reduces the linear complexity. Extensive experiments on the new dataset and existing public datasets show that LLFormer outperforms state-of-the-art methods. We also show that employing existing LLIE methods trained on our benchmark as a pre-processing step significantly improves the performance of downstream tasks, e.g., face detection in low-light conditions. The source code and pre-trained models are available at https://github.com/TaoWangzj/LLFormer.},
  copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {CMS: Applications},
  file = {C:\Users\theun\Zotero\storage\2ML3GPNX\Wang et al. - 2023 - Ultra-High-Definition Low-Light Image Enhancement A Benchmark and Transformer-Based Method.pdf}
}

@inproceedings{wangUnderexposedPhotoEnhancement2019,
  title = {Underexposed {{Photo Enhancement Using Deep Illumination Estimation}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wang, Ruixing and Zhang, Qing and Fu, Chi-Wing and Shen, Xiaoyong and Zheng, Wei-Shi and Jia, Jiaya},
  year = 2019,
  month = jun,
  pages = {6842--6850},
  publisher = {IEEE},
  address = {Long Beach, CA, USA},
  doi = {10.1109/CVPR.2019.00701},
  urldate = {2025-03-11},
  abstract = {This paper presents a new neural network for enhancing underexposed photos. Instead of directly learning an image-to-image mapping as previous work, we introduce intermediate illumination in our network to associate the input with expected enhancement result, which augments the network's capability to learn complex photographic adjustment from expert-retouched input/output image pairs. Based on this model, we formulate a loss function that adopts constraints and priors on the illumination, prepare a new dataset of 3,000 underexposed image pairs, and train the network to effectively learn a rich variety of adjustment for diverse lighting conditions. By these means, our network is able to recover clear details, distinct contrast, and natural color in the enhancement results. We perform extensive experiments on the benchmark MIT-Adobe FiveK dataset and our new dataset, and show that our network is effective to deal with previously challenging images.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-7281-3293-8},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\HIPASCGU\Wang et al. - 2019 - Underexposed Photo Enhancement Using Deep Illumination Estimation.pdf}
}

@article{wangWaveCRNetWaveletTransformGuided2025,
  title = {{{WaveCRNet}}: {{Wavelet Transform-Guided Learning}} for {{Semantic Segmentation}} in {{Adverse Railway Scenes}}},
  shorttitle = {{{WaveCRNet}}},
  author = {Wang, Zhangyu and Liao, Zhihao and Wang, Pengcheng and Chen, Peng and Luo, Wenwen},
  year = 2025,
  month = jun,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {26},
  number = {6},
  pages = {8794--8809},
  issn = {1558-0016},
  doi = {10.1109/TITS.2025.3543925},
  urldate = {2025-06-18},
  abstract = {Semantic segmentation is pivotal in autonomous train perception, significantly impacting the system's intelligence and reliability. However, its performance in railway scenes is hindered by various challenges, including severe weather conditions, low-light situations, tunnel settings, and diverse and dynamic unstructured scenes. To address these challenges, this study proposes WaveCRNet, a novel architecture for real-time semantic segmentation in challenging conditions, simulating wavelet-constrained PID controller in feature and wavelet space. This study first designs an effective wavelet information enhancement algorithm using a differentiable wavelet transform to bridge the gap between the wavelet and feature information domains. Then, the wavelet-guided attention pag module (WAPM) is introduced to guide the learning and fusion of detailed features based on wavelet priors. Moreover, the traditional wavelet transforms are spectral aliasing and shift sensitivity. Inspired by dual-tree complex wavelet transform (DTCWT), the DTCWT-based channel reconstruction module (DCRM) is designed to assist the channel-based learning of boundary information from coarse to fine. Finally, the proposed architecture is evaluated by the public dataset RailSem19. The experimental results validate the consistent performance gains between accuracy and inference speed, improving by 63.7\% mIoU and 87 FPS, surpassing those of the advanced methods for real-time segmentation.},
  keywords = {Accuracy,Adverse condition,adverse conditions,Condition,Discrete wavelet transforms,Dual-tree complex wavelet transform,Feature extraction,Image coding,Image compression,Image reconstruction,Inference engines,Rail transportation,Railroad transportation,Railroad tunnels,Railroads,Railway scene,real-time semantic segmentation,Real-time semantic segmentation,Real-time semantics,Real-time systems,Semantic segmentation,Semantic Segmentation,Semantics,System intelligence,System reliability,Trees (mathematics),Wavelet analysis,Wavelet domain,wavelet transform,Wavelet transforms,Wavelets transform},
  file = {C:\Users\theun\Zotero\storage\JUCNGPDS\Wang et al. - 2025 - WaveCRNet Wavelet Transform-Guided Learning for Semantic Segmentation in Adverse Railway Scenes.pdf}
}

@inproceedings{wangWebStereoVideo2019,
  title = {Web {{Stereo Video Supervision}} for {{Depth Prediction}} from {{Dynamic Scenes}}},
  booktitle = {2019 {{International Conference}} on {{3D Vision}} ({{3DV}})},
  author = {Wang, Chaoyang and Lucey, Simon and Perazzi, Federico and Wang, Oliver},
  year = 2019,
  month = sep,
  pages = {348--357},
  issn = {2475-7888},
  doi = {10.1109/3DV.2019.00046},
  urldate = {2025-07-03},
  abstract = {We present a fully data-driven method to compute depth from diverse monocular video sequences that contain large amounts of non-rigid objects, e.g., people. In order to learn reconstruction cues for non-rigid scenes, we introduce a new dataset consisting of stereo videos scraped in-the-wild. This dataset has a wide variety of scene types, and features large amounts of nonrigid objects, especially people. From this, we compute disparity maps to be used as supervision to train our approach. We propose a loss function that allows us to generate a depth prediction even with unknown camera intrinsics and stereo baselines in the dataset. We validate the use of large amounts of Internet video by evaluating our method on existing video datasets with depth supervision, including SINTEL, and KITTI, and show that our approach generalizes better to natural scenes.},
  keywords = {/unread,Cameras,depth estimation,dynamic scene reconstruction,Image reconstruction,self supervised learning,Stereo image processing,Three-dimensional displays,Training,Video sequences,web stereo video,YouTube},
  file = {C:\Users\theun\Zotero\storage\IJYKTKR7\Wang et al. - 2019 - Web Stereo Video Supervision for Depth Prediction from Dynamic Scenes.pdf}
}

@misc{WeatherLightingDatasets,
  title = {Weather \& {{Lighting Datasets}} for {{Autonomous Vehicle Training}} \textbar{} {{Shaip}}},
  urldate = {2025-01-18},
  abstract = {High-resolution dash cam video datasets covering sunny, rainy, cloudy \& low-light conditions. Annotated data with bounding boxes for autonomous driving AI training.},
  howpublished = {https://www.shaip.com/offerings/weather-lighting-condition-datasets/},
  langid = {american},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\YXMSQPTX\weather-lighting-condition-datasets.html}
}

@misc{weiDeepRetinexDecomposition2018,
  title = {Deep {{Retinex Decomposition}} for {{Low-Light Enhancement}}},
  author = {Wei, Chen and Wang, Wenjing and Yang, Wenhan and Liu, Jiaying},
  year = 2018,
  month = aug,
  number = {arXiv:1808.04560},
  eprint = {1808.04560},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1808.04560},
  urldate = {2025-01-20},
  abstract = {Retinex model is an effective tool for low-light image enhancement. It assumes that observed images can be decomposed into the reflectance and illumination. Most existing Retinex-based methods have carefully designed hand-crafted constraints and parameters for this highly ill-posed decomposition, which may be limited by model capacity when applied in various scenes. In this paper, we collect a LOw-Light dataset (LOL) containing low/normal-light image pairs and propose a deep Retinex-Net learned on this dataset, including a Decom-Net for decomposition and an Enhance-Net for illumination adjustment. In the training process for Decom-Net, there is no ground truth of decomposed reflectance and illumination. The network is learned with only key constraints including the consistent reflectance shared by paired low/normal-light images, and the smoothness of illumination. Based on the decomposition, subsequent lightness enhancement is conducted on illumination by an enhancement network called Enhance-Net, and for joint denoising there is a denoising operation on reflectance. The Retinex-Net is end-to-end trainable, so that the learned decomposition is by nature good for lightness adjustment. Extensive experiments demonstrate that our method not only achieves visually pleasing quality for low-light enhancement but also provides a good representation of image decomposition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\F9VYAK8T\\Wei et al. - 2018 - Deep Retinex Decomposition for Low-Light Enhancement.pdf;C\:\\Users\\theun\\Zotero\\storage\\38XIWSBW\\1808.html}
}

@inproceedings{weiDisentangleThenParse2023,
  title = {Disentangle Then {{Parse}}: {{Night-time Semantic Segmentation}} with {{Illumination Disentanglement}}},
  shorttitle = {Disentangle Then {{Parse}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Wei, Zhixiang and Chen, Lin and Tu, Tao and Ling, Pengyang and Chen, Huaian and Jin, Yi},
  year = 2023,
  pages = {21593--21603},
  urldate = {2025-06-16},
  langid = {english},
  file = {C\:\\Users\\theun\\Zotero\\storage\\L3XFRCTM\\Wei et al. - 2023 - Disentangle then Parse Night-time Semantic Segmentation with Illumination Disentanglement.pdf;C\:\\Users\\theun\\Zotero\\storage\\SKVWG6HR\\Wei_Disentangle_then_Parse_ICCV_2023_supplemental.pdf}
}

@misc{weiIllumFlowIlluminationAdaptiveLowLight2025,
  title = {{{IllumFlow}}: {{Illumination-Adaptive Low-Light Enhancement}} via {{Conditional Rectified Flow}} and {{Retinex Decomposition}}},
  shorttitle = {{{IllumFlow}}},
  author = {Wei, Wenyang and {yang}, Yang and Jia, Xixi and Feng, Xiangchu and Wang, Weiwei and Wang, Renzhen},
  year = 2025,
  month = nov,
  number = {arXiv:2511.02411},
  eprint = {2511.02411},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2511.02411},
  urldate = {2025-11-16},
  abstract = {We present IllumFlow, a novel framework that synergizes conditional Rectified Flow (CRF) with Retinex theory for low-light image enhancement (LLIE). Our model addresses low-light enhancement through separate optimization of illumination and reflectance components, effectively handling both lighting variations and noise. Specifically, we first decompose an input image into reflectance and illumination components following Retinex theory. To model the wide dynamic range of illumination variations in low-light images, we propose a conditional rectified flow framework that represents illumination changes as a continuous flow field. While complex noise primarily resides in the reflectance component, we introduce a denoising network, enhanced by flow-derived data augmentation, to remove reflectance noise and chromatic aberration while preserving color fidelity. IllumFlow enables precise illumination adaptation across lighting conditions while naturally supporting customizable brightness enhancement. Extensive experiments on low-light enhancement and exposure correction demonstrate superior quantitative and qualitative performance over existing methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\DY8838VX\\Wei et al. - 2025 - IllumFlow Illumination-Adaptive Low-Light Enhancement via Conditional Rectified Flow and Retinex De.pdf;C\:\\Users\\theun\\Zotero\\storage\\NYMHJX3X\\2511.html}
}

@article{weiLNFormerLightweightDesign2025,
  title = {{{LNFormer}}: {{Lightweight Design}} for {{Nighttime Semantic Segmentation With Transformer}}},
  shorttitle = {{{LNFormer}}},
  author = {Wei, Longsheng and Liao, Yuhang},
  year = 2025,
  journal = {IEEE Transactions on Instrumentation and Measurement},
  volume = {74},
  pages = {1--12},
  issn = {1557-9662},
  doi = {10.1109/TIM.2025.3547130},
  urldate = {2025-06-26},
  abstract = {General image semantic segmentation methods mainly focus on daytime images with sufficient light, nighttime images have low contrast and blurred details compared to daytime images, and this difference makes it difficult for conventional semantic segmentation algorithms to effectively distinguish between the target and the background. Moreover, semantic segmentation needs to function in real time as part of visual perception. As a result, achieving high-quality segmentation of nighttime scenes while ensuring speed is a particularly challenging task. In this article, we present a lightweight design for nighttime semantic segmentation with transformer (LNFormer) to implement real-time semantic segmentation. Targeting the problem of blurred details and low contrast in nighttime images, LNFormer proposes to utilize the lightweight transformer module and token pyramid to construct global information and enhance the feature representation capability of the model in nighttime situations. Furthermore, LNFormer presents the mutual fusion module based on spatial attention under the coding and decoding system to further enhance the feature display of the target spatial location. Extensive experiments have shown that LNFormer outperforms other real-time methods in nighttime and daytime datasets, striking a new balance between speed and accuracy. Our proposed approach offers an innovative direction for nighttime semantic segmentation and has potential applications in technologies such as autonomous driving, surveillance, and robotics.},
  keywords = {Accuracy,Data mining,Daylighting,Decoding,Feature extraction,Image coding,Image semantics,Lighting,Lightweight design,Low contrast,Nighttime scene,nighttime scenes,real-time semantic segmentation,Real-time semantic segmentation,Real-time semantics,Real-time systems,Segmentation algorithms,Segmentation methods,Semantic segmentation,Semantic Segmentation,Semantics,Training,transformer,Transformer,Transformers},
  file = {C:\Users\theun\Zotero\storage\6SHJ6JHW\Wei and Liao - 2025 - LNFormer Lightweight Design for Nighttime Semantic Segmentation With Transformer.pdf}
}

@article{wenIlluminationguidedDualAttention2025,
  title = {An Illumination-Guided Dual Attention Vision Transformer for Low-Light Image Enhancement},
  author = {Wen, Yanjie and Xu, Ping and Li, Zhihong and Xu(ATO), Wangtu},
  year = 2025,
  month = feb,
  journal = {Pattern Recognition},
  volume = {158},
  pages = {111033},
  issn = {00313203},
  doi = {10.1016/j.patcog.2024.111033},
  urldate = {2024-12-04},
  abstract = {Existing Retinex-based low-light image enhancement methods often overlook corruptions hidden in darkness or pattern collapse caused by the lit-up process. Recent deep learning approaches suggest the use of Ushaped networks with Vision in Transformer (VIT) to address these issues. However, most VIT-based methods focus on channel modeling to reduce expensive computational costs, but in which the restored images suffer from spatial illumination inconsistencies, artifacts, and blurriness. To end for this, we propose a novel onestage Retinex-based Illumination-Guided Dual transformer model (IGDFormer) to lit up low-light images. The model consists of an estimator and a restorer. The estimator generates a light-up feature map and a lit-up image through pure Convolutional Neural Networks (CNNs). The restorer denoises the lit-up image with a U-shaped network equipped with an Illumination-Guided Dual Attention Block (IGDAB). Specifically, IGDAB consists of cascaded channel attention and window attention that achieves cross-channel/spatial modeling. Channel attention alleviates inductive bias through the CNN-Transformer collaborative layer, and window attention introduces spatial domains knowledge by partitioning and shifting. In addition, the light-up features act as values guide the interaction modeling of non-local illumination intensities in both the channel and spatial domains. Extensive experiments were conducted on 5 low-light image enhancement benchmarks and 1 dark object detection benchmark, which demonstrate that the efficacy of our IGDFormer and its superiority in restoring spatial details compared to other state-of-the-art methods. The code is available at https://github.com/YanJieWen/IGDFormer- light- up- dark.},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\WWH5CMTF\Wen et al. - 2025 - An illumination-guided dual attention vision transformer for low-light image enhancement.pdf}
}

@inproceedings{wenIlluminationInsensitiveMonocular2024,
  title = {Illumination {{Insensitive Monocular Depth Estimation Based}} on {{Scene Object Attention}} and {{Depth Map Fusion}}},
  booktitle = {Pattern {{Recognition}} and {{Computer Vision}}},
  author = {Wen, Jing and Ma, Haojiang and Yang, Jie and Zhang, Songsong},
  year = 2024,
  pages = {358--370},
  publisher = {Springer, Singapore},
  issn = {1611-3349},
  doi = {10.1007/978-981-99-8549-4_30},
  urldate = {2025-06-30},
  abstract = {Monocular depth estimation (MDE) is a crucial but challenging computer vision (CV) task which suffers from lighting sensitivity, blurring of neighboring depth edges, and object omissions. To address these problems, we propose an illumination insensitive monocular...},
  isbn = {978-981-99-8549-4},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\KTRFLFNR\Wen et al. - 2024 - Illumination Insensitive Monocular Depth Estimation Based on Scene Object Attention and Depth Map Fu.pdf}
}

@inproceedings{wischnewskiIndyAutonomousChallenge2022,
  title = {Indy {{Autonomous Challenge}} - {{Autonomous Race Cars}} at the {{Handling Limits}}},
  booktitle = {12th {{International Munich Chassis Symposium}} 2021},
  author = {Wischnewski, Alexander and Geisslinger, Maximilian and Betz, Johannes and Betz, Tobias and Fent, Felix and Heilmeier, Alexander and Hermansdorfer, Leonhard and Herrmann, Thomas and Huch, Sebastian and Karle, Phillip and Nobis, Felix and {\"O}gretmen, Levent and Rowold, Matthias and Sauerbeck, Florian and Stahl, Tim and Trauth, Rainer and Lienkamp, Markus and Lohmann, Boris},
  editor = {Pfeffer, Peter},
  year = 2022,
  pages = {163--182},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-64550-5_10},
  abstract = {Motorsport has always been an enabler for technological advancement, and the same applies to the autonomous driving industry. The team TUM Autonomous Motorsports will participate in the Indy Autonomous Challenge in October 2021 to benchmark its self-driving software-stack by racing one out of ten autonomous Dallara AV-21 racecars at the Indianapolis Motor Speedway. The first part of this paper explains the reasons for entering an autonomous vehicle race from an academic perspective: It allows focusing on several edge cases encountered by autonomous vehicles, such as challenging evasion maneuvers and unstructured scenarios. At the same time, it is inherently safe due to the motorsport related track safety precautions. It is therefore an ideal testing ground for the development of autonomous driving algorithms capable of mastering the most challenging and rare situations. In addition, we provide insight into our software development workflow and present our Hardware-in-the-Loop simulation setup. It is capable of running simulations of up to eight autonomous vehicles in real time. The second part of the paper gives a high-level overview of the software architecture and covers our development priorities in building a high-performance autonomous racing software: maximum sensor detection range, reliable handling of multi-vehicle situations, as well as reliable motion control under uncertainty.},
  isbn = {978-3-662-64550-5},
  langid = {english},
  keywords = {/unread,Autonomous driving,Control,Perception,Planning,Racing},
  file = {C:\Users\theun\Zotero\storage\UPYC3CEE\Wischnewski et al. - 2022 - Indy Autonomous Challenge - Autonomous Race Cars at the Handling Limits.pdf}
}

@inproceedings{wofkFastDepthFastMonocular2019,
  title = {{{FastDepth}}: {{Fast Monocular Depth Estimation}} on {{Embedded Systems}}},
  shorttitle = {{{FastDepth}}},
  booktitle = {2019 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Wofk, Diana and Ma, Fangchang and Yang, Tien-Ju and Karaman, Sertac and Sze, Vivienne},
  year = 2019,
  month = may,
  pages = {6101--6108},
  issn = {2577-087X},
  doi = {10.1109/ICRA.2019.8794182},
  urldate = {2025-06-29},
  abstract = {Depth sensing is a critical function for robotic tasks such as localization, mapping and obstacle detection. There has been a significant and growing interest in depth estimation from a single RGB image, due to the relatively low cost and size of monocular cameras. However, state-of-the-art single-view depth estimation algorithms are based on fairly complex deep neural networks that are too slow for real-time inference on an embedded platform, for instance, mounted on a micro aerial vehicle. In this paper, we address the problem of fast depth estimation on embedded systems. We propose an efficient and lightweight encoder-decoder network architecture and apply network pruning to further reduce computational complexity and latency. In particular, we focus on the design of a low-latency decoder. Our methodology demonstrates that it is possible to achieve similar accuracy as prior work on depth estimation, but at inference speeds that are an order of magnitude faster. Our proposed network, FastDepth, runs at 178 fps on an NVIDIA Jetson TX2 GPU and at 27 fps when using only the TX2 CPU, with active power consumption under 10 W. FastDepth achieves close to state-of-the-art accuracy on the NYU Depth v2 dataset. To the best of the authors' knowledge, this paper demonstrates real-time monocular depth estimation using a deep neural network with the lowest latency and highest throughput on an embedded platform that can be carried by a micro aerial vehicle.},
  keywords = {Complexity theory,Convolution,Decoding,Estimation,Neural networks,Runtime,Task analysis},
  file = {C:\Users\theun\Zotero\storage\2QEKWWFJ\Wofk et al. - 2019 - FastDepth Fast Monocular Depth Estimation on Embedded Systems.pdf}
}

@inproceedings{wuDANNetOneStageDomain2021,
  title = {{{DANNet}}: {{A One-Stage Domain Adaptation Network}} for {{Unsupervised Nighttime Semantic Segmentation}}},
  shorttitle = {{{DANNet}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wu, Xinyi and Wu, Zhenyao and Guo, Hao and Ju, Lili and Wang, Song},
  year = 2021,
  month = jun,
  pages = {15764--15773},
  publisher = {IEEE},
  address = {Nashville, TN, USA},
  doi = {10.1109/CVPR46437.2021.01551},
  urldate = {2025-05-05},
  abstract = {Semantic segmentation of nighttime images plays an equally important role as that of daytime images in autonomous driving, but the former is much more challenging due to poor illuminations and arduous human annotations. In this paper, we propose a novel domain adaptation network (DANNet) for nighttime semantic segmentation without using labeled nighttime image data. It employs an adversarial training with a labeled daytime dataset and an unlabeled dataset that contains coarsely aligned day-night image pairs. Specifically, for the unlabeled day-night image pairs, we use the pixel-level predictions of static object categories on a daytime image as a pseudo supervision to segment its counterpart nighttime image. We further design a re-weighting strategy to handle the inaccuracy caused by misalignment between day-night image pairs and wrong predictions of daytime images, as well as boost the prediction accuracy of small objects. The proposed DANNet is the first one-stage adaptation framework for nighttime semantic segmentation, which does not train additional day-night image transfer models as a separate pre-processing stage. Extensive experiments on Dark Zurich and Nighttime Driving datasets show that our method achieves state-of-the-art performance for nighttime semantic segmentation.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-6654-4509-2},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\SH36QH38\Wu et al. - 2021 - DANNet A One-Stage Domain Adaptation Network for Unsupervised Nighttime Semantic Segmentation.pdf}
}

@article{wuEdgeComputingDriven2023,
  title = {Edge {{Computing Driven Low-Light Image Dynamic Enhancement}} for {{Object Detection}}},
  author = {Wu, Yirui and Guo, Haifeng and Chakraborty, Chinmay and Khosravi, Mohammad R. and Berretti, Stefano and Wan, Shaohua},
  year = 2023,
  month = sep,
  journal = {IEEE Transactions on Network Science and Engineering},
  volume = {10},
  number = {5},
  pages = {3086--3098},
  issn = {2327-4697},
  doi = {10.1109/TNSE.2022.3151502},
  urldate = {2025-01-14},
  abstract = {With fast increase in volume of mobile multimedia data, how to apply powerful deep learning methods to process data with real-time response becomes a major issue. Meanwhile, edge computing structure helps improve response time and user experience by bringing flexible computation and storage capabilities. Considering both technologies for successful AI-based applications, we propose an edge-computing driven and end-to-end framework to perform tasks of image enhancement and object detection under low-light conditions. The framework consists of a cloud-based enhancement and an edge-based detection stage. In the first stage, we establish connections between edge devices and cloud servers to input re-scaled illumination parts of low-light images, where enhancement subnetworks are dynamically and parallel coupled to compute enhanced illumination parts based on low-light context. During the edge-based detection stage, edge devices could accurately and rapidly detect objects based on cloud-computed informative feature map. Experimental results show the proposed method significantly improves detection performance in low-light conditions with low latency running on edge devices.},
  keywords = {Cloud computing,Edge computing,edge-driven deep learning method.,Image edge detection,Image enhancement,Low-light image enhancement,object detection,Object detection,Performance evaluation,Task analysis},
  file = {C\:\\Users\\theun\\Zotero\\storage\\SZDWKJHA\\Wu et al. - 2023 - Edge Computing Driven Low-Light Image Dynamic Enhancement for Object Detection.pdf;C\:\\Users\\theun\\Zotero\\storage\\SUU4XACD\\9714014.html}
}

@article{wuFlexibleSemanticGuided2024,
  title = {Towards a {{Flexible Semantic Guided Model}} for {{Single Image Enhancement}} and {{Restoration}}},
  author = {Wu, Yuhui and Wang, Guoqing and Liu, Shaochong and Yang, Yang and Li, Wei and Tang, Xiongxin and Gu, Shuhang and Li, Chongyi and Shen, Heng Tao},
  year = 2024,
  month = dec,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {46},
  number = {12},
  pages = {9921--9939},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2024.3432308},
  urldate = {2025-03-11},
  abstract = {Low-light image enhancement (LLIE) investigates how to improve the brightness of an image captured in illumination-insufficient environments. The majority of existing methods enhance low-light images in a global and uniform manner, without taking into account the semantic information of different regions. Consequently, a network may easily deviate from the original color of local regions. To address this issue, we propose a semantic-aware knowledge-guided framework (SKF) that can assist a low-light enhancement model in learning rich and diverse priors encapsulated in a semantic segmentation model. We concentrate on incorporating semantic knowledge from three key aspects: a semantic-aware embedding module that adaptively integrates semantic priors in feature representation space, a semantic-guided color histogram loss that preserves color consistency of various instances, and a semantic-guided adversarial loss that produces more natural textures by semantic priors. Our SKF is appealing in acting as a general framework in the LLIE task. We further present a refined framework SKF++ with two new techniques: (a) Extra convolutional branch for intra-class illumination and color recovery through extracting local information and (b) Equalization-based histogram transformation for contrast enhancement and high dynamic range adjustment. Extensive experiments on various benchmarks of LLIE task and other image processing tasks show that models equipped with the SKF/SKF++ significantly outperform the baselines and our SKF/SKF++ generalizes to different models and scenes well. Besides, the potential benefits of our method in face detection and semantic segmentation in low-light conditions are discussed.},
  keywords = {Histograms,Image color analysis,Image enhancement,Low-level vision,low-light image enhancement,region-aware color constraints,semantic guidance,Semantics,Task analysis,Training,Visualization},
  file = {C\:\\Users\\theun\\Zotero\\storage\\JZTJIWBP\\Wu et al. - 2024 - Towards a Flexible Semantic Guided Model for Single Image Enhancement and Restoration.pdf;C\:\\Users\\theun\\Zotero\\storage\\PU4XLX3T\\10607918.html}
}

@inproceedings{wuLearningSemanticAwareKnowledge2023,
  title = {Learning {{Semantic-Aware Knowledge Guidance}} for {{Low-Light Image Enhancement}}},
  booktitle = {2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wu, Yuhui and Pan, Chen and Wang, Guoqing and Yang, Yang and Wei, Jiwei and Li, Chongyi and Shen, Heng Tao},
  year = 2023,
  month = jun,
  pages = {1662--1671},
  publisher = {IEEE},
  address = {Vancouver, BC, Canada},
  doi = {10.1109/CVPR52729.2023.00166},
  urldate = {2025-02-27},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-0129-8},
  langid = {english},
  keywords = {Adversarial machine learning,Image enhancement,Knowledge guidances,Latent semantic analysis,Learning semantics,Light enhancement,Low light,Low-level vision,Low-light images,Segmentation models,Semantic segmentation,Semantic Segmentation,Semantic-aware,Semantics Information},
  file = {C:\Users\theun\Zotero\storage\XXTVR4FU\Wu et al. - 2023 - Learning Semantic-Aware Knowledge Guidance for Low-Light Image Enhancement.pdf}
}

@article{wuOneStageDomainAdaptation2023,
  title = {A {{One-Stage Domain Adaptation Network With Image Alignment}} for {{Unsupervised Nighttime Semantic Segmentation}}},
  author = {Wu, Xinyi and Wu, Zhenyao and Ju, Lili and Wang, Song},
  year = 2023,
  month = jan,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {45},
  number = {1},
  pages = {58--72},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2021.3138829},
  urldate = {2025-06-24},
  abstract = {In this paper, we tackle the problem of semantic segmentation for nighttime images that plays an equally important role as that for daytime images in autonomous driving, but is also much more challenging due to very poor illuminations and scarce annotated datasets. It can be treated as an unsupervised domain adaptation (UDA) problem, i.e., applying other labeled dataset taken in the daytime to guide the network training meanwhile reducing the domain shift, so that the trained model can generalize well to the desired domain of nighttime images. However, current general-purpose UDA approaches are insufficient to address the significant appearance difference between the day and night domains. To overcome such a large domain gap, we propose a novel domain adaptation network ``DANIA'' for nighttime semantic image segmentation by leveraging a labeled daytime dataset (the source domain) and an unlabeled dataset that contains coarsely aligned day-night image pairs (the target daytime and nighttime domains). These three domains are used to perform a multi-target adaptation via adversarial training in the network. Specifically, for the unlabeled day-night image pairs, we use the pixel-level predictions of static object categories on a daytime image as a pseudo supervision to segment its counterpart nighttime image. We also include a step of image alignment to relieve the inaccuracy caused by the misalignment between day-night image pairs by estimating a flow to refine the pseudo supervision produced by daytime images. Finally, a re-weighting strategy is applied to further improve the predictions, especially boosting the prediction accuracy of small objects. The proposed DANIA is a one-stage adaptation framework for nighttime semantic segmentation, which does not train additional day-night image transfer models as a separate pre-processing stage. Extensive experiments on Dark Zurich and Nighttime Driving datasets show that our DANIA achieves state-of-the-art performance for nighttime semantic segmentation.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\WDCV2DVN\A_One-Stage_Domain_Adaptation_Network_With_Image_Alignment_for_Unsupervised_Nighttime_Semantic_Segmentation.pdf}
}

@article{wuS3MNetJointLearning2024,
  title = {S{\textsuperscript{3}}{{M-Net}}: {{Joint Learning}} of {{Semantic Segmentation}} and {{Stereo Matching}} for {{Autonomous Driving}}},
  shorttitle = {S{\textsuperscript{3}}{{M-Net}}},
  author = {Wu, Zhiyuan and Feng, Yi and Liu, Chuang-Wei and Yu, Fisher and Chen, Qijun and Fan, Rui},
  year = 2024,
  month = feb,
  journal = {IEEE Transactions on Intelligent Vehicles},
  volume = {9},
  number = {2},
  pages = {3940--3951},
  issn = {2379-8904},
  doi = {10.1109/TIV.2024.3357056},
  urldate = {2025-05-20},
  abstract = {Semantic segmentation and stereo matching are two essential components of 3D environmental perception systems for autonomous driving. Nevertheless, conventional approaches often address these two problems independently, employing separate models for each task. This approach poses practical limitations in real-world scenarios, particularly when computational resources are scarce or real-time performance is imperative. Hence, in this article, we introduce S\textsuperscript{3}M-Net, a novel joint learning framework developed to perform semantic segmentation and stereo matching simultaneously. Specifically, S\textsuperscript{3}M-Net shares the features extracted from RGB images between both tasks, resulting in an improved overall scene understanding capability. This feature sharing process is realized using a feature fusion adaption (FFA) module, which effectively transforms the shared features into semantic space and subsequently fuses them with the encoded disparity features. The entire joint learning framework is trained by minimizing a novel semantic consistency-guided (SCG) loss, which places emphasis on the structural consistency in both tasks. Extensive experimental results conducted on the vKITTI2 and KITTI datasets demonstrate the effectiveness of our proposed joint learning framework and its superior performance compared to other state-of-the-art single-task networks. Our project webpage is accessible at mias.group/S3M-Net.},
  keywords = {Autonomous driving,Correlation,environmental perception,Feature extraction,joint learning,semantic segmentation,Semantic segmentation,Semantics,stereo matching,Task analysis,Three-dimensional displays,Training},
  file = {C:\Users\theun\Zotero\storage\8UYXI5F2\Wu et al. - 2024 - S3M-Net Joint Learning of Semantic Segmentation and Stereo Matching for Autonomous Drivi.pdf}
}

@inproceedings{wuSim2realTransferLearning2023,
  title = {Sim2real {{Transfer Learning}} for {{Point Cloud Segmentation}}: {{An Industrial Application Case}} on {{Autonomous Disassembly}}},
  shorttitle = {Sim2real {{Transfer Learning}} for {{Point Cloud Segmentation}}},
  booktitle = {2023 {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Wu, Chengzhi and Bi, Xuelei and Pfrommer, Julius and Cebulla, Alexander and Mangold, Simon and Beyerer, Jurgen},
  year = 2023,
  month = jan,
  pages = {4520--4529},
  publisher = {IEEE},
  address = {Waikoloa, HI, USA},
  doi = {10.1109/WACV56688.2023.00451},
  urldate = {2025-01-03},
  abstract = {On robotics computer vision tasks, generating and annotating large amounts of data from real-world for the use of deep learning-based approaches is often difficult or even impossible. A common strategy for solving this problem is to apply simulation-to-reality (sim2real) approaches with the help of simulated scenes. While the majority of current robotics vision sim2real work focuses on image data, we present an industrial application case that uses sim2real transfer learning for point cloud data. We provide insights on how to generate and process synthetic point cloud data in order to achieve better performance when the learned model is transferred to real-world data. The issue of imbalanced learning is investigated using multiple strategies. A novel patch-based attention network is proposed additionally to tackle this problem.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-9346-8},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\HZRNN5J8\Wu et al. - 2023 - Sim2real Transfer Learning for Point Cloud Segmentation An Industrial Application Case on Autonomou.pdf}
}

@inproceedings{wuURetinexNetRetinexBasedDeep2022,
  title = {{{URetinex-Net}}: {{Retinex-Based Deep Unfolding Network}} for {{Low-Light Image Enhancement}}},
  shorttitle = {{{URetinex-Net}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Wu, Wenhui and Weng, Jian and Zhang, Pingping and Wang, Xu and Yang, Wenhan and Jiang, Jianmin},
  year = 2022,
  pages = {5901--5910},
  urldate = {2025-03-10},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\YM6XFWAT\Wu et al. - 2022 - URetinex-Net Retinex-Based Deep Unfolding Network for Low-Light Image Enhancement.pdf}
}

@article{xianCROSELowlightEnhancement2024,
  title = {{{CROSE}}: {{Low-light}} Enhancement by {{CROss-SEnsor}} Interaction for Nighttime Driving Scenes},
  shorttitle = {{{CROSE}}},
  author = {Xian, Xiaoyu and Zhou, Qi and Qin, Jinghui and Yang, Xiaojun and Tian, Yin and Shi, Yukai and Tian, Daxin},
  year = 2024,
  month = aug,
  journal = {Expert Systems with Applications},
  volume = {248},
  pages = {123470},
  issn = {09574174},
  doi = {10.1016/j.eswa.2024.123470},
  urldate = {2024-12-06},
  abstract = {An increasing number of image perception models are being utilized in the field of autonomous driving. During nighttime driving, the visual perception capabilities of a single-modality RGB sensor become compromised. To address this issue, image fusion methods that utilize multi-modality data to provide a more comprehensive visual representation for nighttime scenes. Nevertheless, the existing image fusion methods are limited by poor generalization abilities and lighting conditions, resulting in inadequate handling of the contrast and spectral corruption. In this paper, we propose a cross-sensor low-light enhancement algorithm that provides more accurate visual perceptions. Our approach employs multiple sensors as preceptors for nighttime driving scenes. Compared with typical fusion algorithms that use a simple one-stage workflow for cross-sensor data, our proposed method adopts an advanced content-enhancement strategy by recursively and interactively scaling up informative pixels. More specifically, our approach employs information measurement to describe the information and then utilizes a cross-sensor content enhancement module to dynamically enhance the mutual information between the infrared spectrum and the RGB streams. Our experiments show that CROSE effectively preserves texture details, resulting in clearer and more solid fusion results for nighttime driving scenes.},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\5RJ75Q2K\Xian et al. - 2024 - CROSE Low-light enhancement by CROss-SEnsor interaction for nighttime driving scenes.pdf}
}

@article{xiangMultiSensorFusionCooperative2023,
  title = {Multi-{{Sensor Fusion}} and {{Cooperative Perception}} for {{Autonomous Driving}}: {{A Review}}},
  shorttitle = {Multi-{{Sensor Fusion}} and {{Cooperative Perception}} for {{Autonomous Driving}}},
  author = {Xiang, Chao and Feng, Chen and Xie, Xiaopo and Shi, Botian and Lu, Hao and Lv, Yisheng and Yang, Mingchuan and Niu, Zhendong},
  year = 2023,
  month = sep,
  journal = {IEEE Intelligent Transportation Systems Magazine},
  volume = {15},
  number = {5},
  pages = {36--58},
  issn = {1941-1197},
  doi = {10.1109/MITS.2023.3283864},
  urldate = {2025-01-02},
  abstract = {Autonomous driving (AD), including single-vehicle intelligent AD and vehicle--infrastructure cooperative AD, has become a current research hot spot in academia and industry, and multi-sensor fusion is a fundamental task for AD system perception. However, the multi-sensor fusion process faces the problem of differences in the type and dimensionality of sensory data acquired using different sensors (cameras, lidar, millimeter-wave radar, and so on) as well as differences in the performance of environmental perception caused by using different fusion strategies. In this article, we study multiple papers on multi-sensor fusion in the field of AD and address the problem that the category division in current multi-sensor fusion perception is not detailed and clear enough and is more subjective, which makes the classification strategies differ significantly among similar algorithms. We innovatively propose a multi-sensor fusion taxonomy, which divides the fusion perception classification strategies into two categories---symmetric fusion and asymmetric fusion---and seven subcategories of strategy combinations, such as data, features, and results. In addition, the reliability of current AD perception is limited by its insufficient environment perception capability and the robustness of data-driven methods in dealing with extreme situations (e.g., blind areas). This article also summarizes the innovative applications of multi-sensor fusion classification strategies in AD cooperative perception.},
  keywords = {/unread,Autonomous driving,Classification algorithms,Intelligent transportation systems,Laser radar,Multisensor systems,Point cloud compression,Sensor fusion,Sensors,Taxonomy,Three-dimensional displays},
  file = {C\:\\Users\\theun\\Zotero\\storage\\PRQE2CQX\\Xiang et al. - 2023 - Multi-Sensor Fusion and Cooperative Perception for Autonomous Driving A Review.pdf;C\:\\Users\\theun\\Zotero\\storage\\ZYKLWY8H\\10208208.html}
}

@inproceedings{xianMonocularRelativeDepth2018,
  title = {Monocular {{Relative Depth Perception With Web Stereo Data Supervision}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Xian, Ke and Shen, Chunhua and Cao, Zhiguo and Lu, Hao and Xiao, Yang and Li, Ruibo and Luo, Zhenbo},
  year = 2018,
  pages = {311--320},
  urldate = {2025-07-03},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\BRNXAHBK\Xian et al. - 2018 - Monocular Relative Depth Perception With Web Stereo Data Supervision.pdf}
}

@article{xiaoNighttimeSemanticSegmentation2024,
  title = {Nighttime {{Semantic Segmentation}} with {{Attention}} and {{Low-Light Enhancement}}},
  author = {Xiao, C. and Xu, Y. and Zhang, Y. and Feng, M. and Huang, Y.},
  year = 2024,
  journal = {Jisuanji Gongcheng/Computer Engineering},
  volume = {50},
  number = {7},
  pages = {271--281},
  publisher = {Editorial Office of Computer Engineering},
  doi = {10.19678/j.issn.1000-3428.0068104},
  abstract = {With the development of deep learning technology and the improvements in computing power, semantic segmentation of natural scene images captured during the day shows promising results. However, in nighttime image semantic segmentation tasks, models trained on daytime data often fail to deliver satisfactory performance due to challenges such as imbalanced exposure and a lack of labeled data. To address these challenges, a new unsupervised nighttime image semantic segmentation network called AI-USeg is proposed. First, a lightweight Self-Calibrating Illumination (SCI) network is used to enhance nighttime images, thereby mitigating the impact of lighting variations on subsequent semantic segmentation networks. Next, a Domain Adaptation (DA) method is introduced to transition the model from Cityscapes containing a large amount of labeled data to Dark Zurich-D, addressing the lack of labeled data. Subsequently, AI-USeg introduces a Squeeze-and-Excitation Network (SENet) into the discriminator, built upon a Fully Convolutional Network (FCN). This adaptation facilitates the adjustment of image features in low-light nighttime settings through adversarial learning in the output space, ultimately improving the performance of semantic segmentation tasks for nighttime images. The experiment used two sets of 2 416 day and night image pairs sourced from Cityscapes and Dark Zurich-train for unsupervised training. The results show that AI-USeg achieved Mean Intersection over Union (mIoU) values of 47.9\% and 51.5\% on the Dark Zurich-test and Nighttime Driving-test datasets, respectively. These values were 5.4 and 2.1 percentage points higher than those obtained using the MGCDA method. In conclusion, AI-USeg displayed stronger adaptability to nighttime image features and higher robustness than previous segmentation models, providing an effective solution for image segmentation tasks in nighttime scenes. \copyright{} 2024, Editorial Office of Computer Engineering. All rights reserved.},
  keywords = {/unread}
}

@article{xiaoSELLMethodLowLight2025,
  title = {{{SELL}}:{{A Method}} for {{Low-Light Image Enhancement}} by {{Predicting Semantic Priors}}},
  shorttitle = {{{SELL}}},
  author = {Xiao, Quanquan and Jin, Haiyan and Su, Haonan and Yan, Ruixia},
  year = 2025,
  journal = {IEEE Signal Processing Letters},
  volume = {32},
  pages = {1785--1789},
  issn = {1558-2361},
  doi = {10.1109/LSP.2025.3562822},
  urldate = {2025-10-28},
  abstract = {In recent years, low-light image enhancement techniques have made significant progress in generating reasonable visual details. However, current methods have not yet fully utilized the full semantic prior of visual elements in low-light environments. Therefore, images generated by these low-light image enhancement methods often suffer from degraded visual quality and may even be distorted. To address this problem, we propose a method to guide low-light image enhancement by predicting semantic priors. Specifically, we train a semantic prior predictor under standard lighting conditions, which is made to learn and predict semantic prior features for low-light images by knowledge distillation on high-quality standard images. Subsequently, we utilize a semantic-aware module that enables the model to adaptively integrate these learned semantic priors, thus ensuring semantic consistency of the enhanced images. Experiments show that the method outperforms several current state-of-the-art methods in terms of visual performance on the LOL-v2 and SICE benchmark datasets.},
  keywords = {Decoding,Feature extraction,Image color analysis,Image enhancement,Image reconstruction,low-light image,Predictive models,semantic awareness,semantic priors,Semantics,Standards,Training,Visualization},
  file = {C:\Users\theun\Zotero\storage\H7MG6LGJ\Xiao et al. - 2025 - SELLA Method for Low-Light Image Enhancement by Predicting Semantic Priors.pdf}
}

@inproceedings{xiaoUnifiedPerceptualParsing2018,
  title = {Unified {{Perceptual Parsing}} for {{Scene Understanding}}},
  booktitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Xiao, Tete and Liu, Yingcheng and Zhou, Bolei and Jiang, Yuning and Sun, Jian},
  year = 2018,
  pages = {418--434},
  urldate = {2025-06-20},
  file = {C:\Users\theun\Zotero\storage\TDLQP57N\Xiao et al. - 2018 - Unified Perceptual Parsing for Scene Understanding.pdf}
}

@article{xieBoostingNightTimeScene2023,
  title = {Boosting {{Night-Time Scene Parsing With Learnable Frequency}}},
  author = {Xie, Zhifeng and Wang, Sen and Xu, Ke and Zhang, Zhizhong and Tan, Xin and Xie, Yuan and Ma, Lizhuang},
  year = 2023,
  journal = {IEEE Transactions on Image Processing},
  volume = {32},
  pages = {2386--2398},
  issn = {1941-0042},
  doi = {10.1109/TIP.2023.3267044},
  urldate = {2025-06-16},
  abstract = {Night-Time Scene Parsing (NTSP) is essential to many vision applications, especially for autonomous driving. Most of the existing methods are proposed for day-time scene parsing. They rely on modeling pixel intensity-based spatial contextual cues under even illumination. Hence, these methods do not perform well in night-time scenes as such spatial contextual cues are buried in the over-/under-exposed regions in night-time scenes. In this paper, we first conduct an image frequency-based statistical experiment to interpret the day-time and night-time scene discrepancies. We find that image frequency distributions differ significantly between day-time and night-time scenes, and understanding such frequency distributions is critical to NTSP problem. Based on this, we propose to exploit the image frequency distributions for night-time scene parsing. First, we propose a Learnable Frequency Encoder (LFE) to model the relationship between different frequency coefficients to measure all frequency components dynamically. Second, we propose a Spatial Frequency Fusion module (SFF) that fuses both spatial and frequency information to guide the extraction of spatial context features. Extensive experiments show that our method performs favorably against the state-of-the-art methods on the NightCity, NightCity+ and BDD100K-night datasets. In addition, we demonstrate that our method can be applied to existing day-time scene parsing methods and boost their performance on night-time scenes. The code is available at https://github.com/wangsen99/FDLNet.},
  keywords = {Context modeling,frequency analysis,Frequency conversion,Image coding,Image segmentation,Night-time vision,scene parsing,Spectrogram,Time-frequency analysis,Transformers},
  file = {C:\Users\theun\Zotero\storage\5HDLGN55\Xie et al. - 2023 - Boosting Night-Time Scene Parsing With Learnable Frequency.pdf}
}

@article{xieLightnessawareLossLowlight2024,
  title = {A Lightness-Aware Loss for Low-Light Image Enhancement},
  author = {Xie, Dian and Xing, Huajun and Chen, Liangyu and Hao, Shijie},
  year = 2024,
  month = mar,
  journal = {Pattern Recognition Letters},
  volume = {179},
  pages = {123--129},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2024.02.011},
  urldate = {2025-10-17},
  abstract = {Current low-light image enhancement methods have made great progress on improving the visibility of low-light images. Nevertheless, they pay less attention to preserving visual naturalness and therefore often introduce over-enhancement and local artifacts into their results. To address this issue, it is useful to introduce additional multi-view information of an image into enhancement models, such as illumination distribution. In this context, we propose a simple but effective loss term that expects the originally bright regions in input images and their corresponding enhanced images to be as similar as possible. Via fully exploring the illumination distribution of an image, the loss term makes enhancement models to know which regions should be preserved during training. Therefore, the unnatural effects in output images can be effectively relieved. In our experiments, we incorporate our loss term into several recently proposed low-light image enhancement models. The experimental results on multiple datasets show that over-enhancement and local artifacts can be effectively suppressed by using our loss term.},
  keywords = {/unread,Illumination distribution,Loss function,Low-light image enhancement},
  file = {C\:\\Users\\theun\\Zotero\\storage\\4BGAVWRG\\Xie et al. - 2024 - A lightness-aware loss for low-light image enhancement.pdf;C\:\\Users\\theun\\Zotero\\storage\\LNPYDICX\\S0167865524000448.html}
}

@article{xiePIGPromptImages2024,
  title = {{{PIG}}: {{Prompt Images Guidance}} for {{Night-Time Scene Parsing}}},
  shorttitle = {{{PIG}}},
  author = {Xie, Zhifeng and Qiu, Rui and Wang, Sen and Tan, Xin and Xie, Yuan and Ma, Lizhuang},
  year = 2024,
  journal = {IEEE Transactions on Image Processing},
  volume = {33},
  pages = {3921--3934},
  issn = {1941-0042},
  doi = {10.1109/TIP.2024.3415963},
  urldate = {2025-06-16},
  abstract = {Night-time scene parsing aims to extract pixel-level semantic information in night images, aiding downstream tasks in understanding scene object distribution. Due to limited labeled night image datasets, unsupervised domain adaptation (UDA) has become the predominant method for studying night scenes. UDA typically relies on paired day-night image pairs to guide adaptation, but this approach hampers dataset construction and restricts generalization across night scenes in different datasets. Moreover, UDA, focusing on network architecture and training strategies, faces difficulties in handling classes with few domain similarities. In this paper, we leverage Prompt Images Guidance (PIG) to enhance UDA with supplementary night knowledge. We propose a Night-Focused Network (NFNet) to learn night-specific features from both target domain images and prompt images. To generate high-quality pseudo-labels, we propose Pseudo-label Fusion via Domain Similarity Guidance (FDSG). Classes with fewer domain similarities are predicted by NFNet, which excels in parsing night features, while classes with more domain similarities are predicted by UDA, which has rich labeled semantics. Additionally, we propose two data augmentation strategies: the Prompt Mixture Strategy (PMS) and the Alternate Mask Strategy (AMS), aimed at mitigating the overfitting of the NFNet to a few prompt images. We conduct extensive experiments on four night-time datasets: NightCity, NightCity+, Dark Zurich, and ACDC. The results indicate that utilizing PIG can enhance the parsing accuracy of UDA. The code is available at https://github.com/qiurui4shu/PIG.},
  keywords = {Accuracy,Adaptation models,Knowledge engineering,Motion pictures,Night-time vision,prompt learning,scene parsing,Semantics,Task analysis,Training,unsupervised domain adaptation},
  file = {C:\Users\theun\Zotero\storage\WR46AP3D\Xie et al. - 2024 - PIG Prompt Images Guidance for Night-Time Scene Parsing.pdf}
}

@misc{xieSegFormerSimpleEfficient2021,
  title = {{{SegFormer}}: {{Simple}} and {{Efficient Design}} for {{Semantic Segmentation}} with {{Transformers}}},
  shorttitle = {{{SegFormer}}},
  author = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M. and Luo, Ping},
  year = 2021,
  month = oct,
  number = {arXiv:2105.15203},
  eprint = {2105.15203},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.15203},
  urldate = {2025-06-28},
  abstract = {We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3\% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2\% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0\% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code will be released at: github.com/NVlabs/SegFormer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\theun\\Zotero\\storage\\988KPWPA\\Xie et al. - 2021 - SegFormer Simple and Efficient Design for Semantic Segmentation with Transformers.pdf;C\:\\Users\\theun\\Zotero\\storage\\ALCBQJ3Y\\2105.html}
}

@inproceedings{xiongSemanticSegmentationAlgorithm2024,
  title = {Semantic {{Segmentation Algorithm}} of {{Nighttime Road Surface Water Based}} on {{Improved Low-Light Image Enhancement}}},
  booktitle = {2024 5th {{International Conference}} on {{Intelligent Computing}} and {{Human-Computer Interaction}} ({{ICHCI}})},
  author = {Xiong, Lei and Feng, XiangSheng and Zhang, LingLin and Sun, Quan and Ren, HuCheng},
  year = 2024,
  month = sep,
  pages = {104--108},
  doi = {10.1109/ICHCI63580.2024.10807993},
  urldate = {2025-06-18},
  abstract = {At present, when semantic segmentation algorithms with high segmentation performance in daytime scenes are directly applied to nighttime scenes, it is difficult to annotate data due to the influence of light and low visibility, especially in the segmentation task of road surface water logging in urban waterlogging monitoring, the effect of water reflection is enhanced, resulting in poor segmentation results. Therefore, this paper proposes a semantic segmentation algorithm for nighttime road flooding. The algorithm introduces SCI low-light image enhancement network to enhance the nighttime road surface water images to reduce the influence of light, and adds a feature enhancement module to solve the problem of image detail loss. At the same time, the spatial attention mechanism is introduced to solve the problem of excessive enhancement and distortion in local areas with good light. Finally, the fusion effect is verified by the Upernet-Swein-T semantic segmentation model. The results show that the mIoU of the fused algorithm reaches 90.03\%, which is 1.7\% higher than that of the original model algorithm, and the ability of predicting night road surface water is improved.},
  keywords = {Attention mechanisms,feature enhancement,Feature enhancement,Image enhancement,Low visibility,low-light image enhancement,Low-light image enhancement,Low-light images,Monitoring,Prediction algorithms,Problem solving,Reflection,Road surfaces,Roads,Segmentation algorithms,Segmentation performance,semantic segmentation,Semantic segmentation,Semantic Segmentation,Semantics,spatial attention,Spatial attention,Training,Water,Water based},
  file = {C:\Users\theun\Zotero\storage\T46S6WBK\Xiong et al. - 2024 - Semantic Segmentation Algorithm of Nighttime Road Surface Water Based on Improved Low-Light Image En.pdf}
}

@inproceedings{xuCDAdaCurriculumDomain2021,
  title = {{{CDAda}}: {{A Curriculum Domain Adaptation}} for {{Nighttime Semantic Segmentation}}},
  shorttitle = {{{CDAda}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision Workshops}} ({{ICCVW}})},
  author = {Xu, Qi and Ma, Yinan and Wu, Jing and Long, Chengnian and Huang, Xiaolin},
  year = 2021,
  month = oct,
  pages = {2962--2971},
  publisher = {IEEE},
  address = {Montreal, BC, Canada},
  doi = {10.1109/ICCVW54120.2021.00331},
  urldate = {2025-06-03},
  abstract = {Autonomous driving needs to ensure all-weather safety, especially in unfavorable environments such as night and rain. However, the current daytime-trained semantic segmentation networks face significant performance degradation at night because of the huge domain divergence. In this paper, we propose a novel Curriculum Domain Adaptation method (CDAda) to realize the smooth semantic knowledge transfer from daytime to nighttime. Specifically, it consists of two steps: 1) inter-domain style adaptation: fine-tune the daytime-trained model on the labeled synthetic nighttime images through the proposed frequency-based style transformation method (replace the low-frequency components of daytime images with those of nighttime images); 2) intra-domain gradual self-training: separate the nighttime domain into the easy split nighttime domain and hard split nighttime domain based on the ``entropy + illumination'' ranking principle, then gradually adapt the model to the two sub-domains through pseudo supervision on easy split data and entropy minimization on hard split data. To the best of our knowledge, we first extend the idea of intra-domain adaptation to self-training and prove different treatments on two parts can reduce the distribution divergence in the nighttime domain itself. In particular, aimed at the adopted unlabeled day-night image pairs, the prediction of the daytime images can guide the segmentation on the nighttime images by ensuring patch-level consistency. Extensive experiments on Nighttime Driving, Dark Zurich, and BDD100K-night dataset highlight the effectiveness of our approach with the more favorable performance 50.9\%, 45.0\%, and 33.8\% Mean IoU against existing state-of-theart approaches.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-6654-0191-3},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\RC9CY8RY\Xu et al. - 2021 - CDAda A Curriculum Domain Adaptation for Nighttime Semantic Segmentation.pdf}
}

@inproceedings{xueBestBothWorlds2022,
  title = {Best of {{Both Worlds}}: {{See}} and {{Understand Clearly}} in the {{Dark}}},
  shorttitle = {Best of {{Both Worlds}}},
  booktitle = {Proceedings of the 30th {{ACM International Conference}} on {{Multimedia}}},
  author = {Xue, Xinwei and He, Jia and Ma, Long and Wang, Yi and Fan, Xin and Liu, Risheng},
  year = 2022,
  month = oct,
  series = {{{MM}} '22},
  pages = {2154--2162},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3503161.3548259},
  urldate = {2025-06-15},
  abstract = {Recently, with the development of intelligent technology, the perception of low-light scenes has been gaining widespread attention. However, existing techniques usually focus on only one task (e.g., enhancement) and lose sight of the others (e.g., detection), making it difficult to perform all of them well at the same time. To overcome this limitation, we propose a new method that can handle visual quality enhancement and semantic-related tasks (e.g., detection, segmentation) simultaneously in a unified framework. Specifically, we build a cascaded architecture to meet the task requirements. To better enhance the entanglement in both tasks and achieve mutual guidance, we develop a new contrastive-alternative learning strategy for learning the model parameters, to largely improve the representational capacity of the cascaded architecture. Notably, the contrastive learning mechanism establishes the communication between two objective tasks in essence, which actually extends the capability of contrastive learning to some extent. Finally, extensive experiments are performed to fully validate the advantages of our method over other state-of-the-art works in enhancement, detection, and segmentation. A series of analytical evaluations are also conducted to reveal our effectiveness. The code is available at https://github.com/k914/contrastive-alternative-learning.},
  isbn = {978-1-4503-9203-7},
  keywords = {Architecture,contrastive alternative learning,Contrastive alternative learning,dark face detection,Dark face detection,Face recognition,Faces detection,Image enhancement,Intelligent technology,Learning systems,Low light,low-light image enhancement,Low-light image enhancement,Low-light images,nighttime semantic segmentation,Nighttime semantic segmentation,Semantic segmentation,Semantic Segmentation,Semantics,Visual qualities},
  file = {C:\Users\theun\Zotero\storage\BBQD759T\Xue et al. - 2022 - Best of Both Worlds See and Understand Clearly in the Dark.pdf}
}

@inproceedings{xueDANetDivergentActivation2019,
  title = {{{DANet}}: {{Divergent Activation}} for {{Weakly Supervised Object Localization}}},
  shorttitle = {{{DANet}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Xue, Haolan and Liu, Chang and Wan, Fang and Jiao, Jianbin and Ji, Xiangyang and Ye, Qixiang},
  year = 2019,
  pages = {6589--6598},
  urldate = {2025-01-03},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\MF9NT7VS\Xue et al. - 2019 - DANet Divergent Activation for Weakly Supervised Object Localization.pdf}
}

@article{xuGNSSINSOD2023,
  title = {{{GNSS}}/{{INS}}/{{OD}}/{{NHC Adaptive Integrated Navigation Method Considering}} the {{Vehicle Motion State}}},
  author = {Xu, Ying and Wang, Kun and Yang, Cheng and Li, Zeyu and Zhou, Feng and Liu, Dun},
  year = 2023,
  month = jun,
  journal = {IEEE Sensors Journal},
  volume = {23},
  number = {12},
  pages = {13511--13523},
  issn = {1558-1748},
  doi = {10.1109/JSEN.2023.3272507},
  urldate = {2025-01-02},
  abstract = {Global navigation satellite systems (GNSSs) integrated with inertial navigation systems (INSs) have been widely applied in many intelligent transport systems. At present, integrating GNSS, microelectromechanical system (MEMS), on- board controller area network (CAN) sensors, and vehicle motion constraint information is the most practical and low-cost vehicle multifusion navigation scheme when GNSS outages. It is especially true when using 3-D velocity from nonholonomic constraints (NHC) and odometer (OD). The GNSS/INS/OD/NHC integration, however, has the problem of inaccuracy in lateral velocity constraint parameters. To overcome this problem, a back propagation (BP) neural network-based GNSS/INS/OD/NHC adaptive integrated navigation method considering the vehicle motion state is proposed in this article. The relationship between the forward velocity, the heading angular velocity, and the lateral velocity is analyzed and considered when the NHC lateral velocity constraint modeling is established by using the BP neural network. To assess the performance of this method, three sets of real land vehicle data are tested with intentional GNSS signal interruption at different vehicle states. The performances of the classic INS/NHC model, INS/OD/NHC integration, and the proposed method are compared, respectively. Experimental results show that the mean error and RMSE of the lateral velocity predicted by the proposed method is 0.007 and 0.049 m/s. The mean 3-D RMSE of the positioning errors and the velocity errors of the proposed method is 1.515 m and 0.182 m/s respectively, which are improved by 54.86\% and 44.85\% compared with that of the classic INS/OD/NHC integration.},
  keywords = {/unread,Adaptation models,Aircraft navigation,Back propagation (BP) neural network,Global navigation satellite system,global navigation satellite system (GNSS) outages,inertial navigation system (INS),lateral velocity constraint,Mathematical models,Navigation,Neural networks,nonholonomic constraints (NHC),Sensors},
  file = {C\:\\Users\\theun\\Zotero\\storage\\QJMZG43W\\Xu et al. - 2023 - GNSSINSODNHC Adaptive Integrated Navigation Method Considering the Vehicle Motion State.pdf;C\:\\Users\\theun\\Zotero\\storage\\UY8HV6SA\\10120704.html}
}

@inproceedings{xuLearningRestoreLowLight2020,
  title = {Learning to {{Restore Low-Light Images}} via {{Decomposition-and-Enhancement}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Xu, Ke and Yang, Xin and Yin, Baocai and Lau, Rynson W.H.},
  year = 2020,
  month = jun,
  pages = {2278--2287},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR42600.2020.00235},
  urldate = {2025-06-19},
  abstract = {Low-light images typically suffer from two problems. First, they have low visibility (i.e., small pixel values). Second, noise becomes significant and disrupts the image content, due to low signal-to-noise ratio. Most existing lowlight image enhancement methods, however, learn from noise-negligible datasets. They rely on users having good photographic skills in taking images with low noise. Unfortunately, this is not the case for majority of the low-light images. While concurrently enhancing a low-light image and removing its noise is ill-posed, we observe that noise exhibits different levels of contrast in different frequency layers, and it is much easier to detect noise in the lowfrequency layer than in the high one. Inspired by this observation, we propose a frequency-based decompositionand-enhancement model for low-light image enhancement. Based on this model, we present a novel network that first learns to recover image objects in the low-frequency layer and then enhances high-frequency details based on the recovered image objects. In addition, we have prepared a new low-light image dataset with real noise to facilitate learning. Finally, we have conducted extensive experiments to show that the proposed method outperforms state-of-the-art approaches in enhancing practical noisy low-light images.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-7281-7168-5},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\DG8MAZA6\Xu et al. - 2020 - Learning to Restore Low-Light Images via Decomposition-and-Enhancement.pdf}
}

@inproceedings{xuPointFusionDeepSensor2018,
  title = {{{PointFusion}}: {{Deep Sensor Fusion}} for {{3D Bounding Box Estimation}}},
  shorttitle = {{{PointFusion}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Xu, Danfei and Anguelov, Dragomir and Jain, Ashesh},
  year = 2018,
  pages = {244--253},
  urldate = {2025-01-03},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\HVFTYRN9\Xu et al. - 2018 - PointFusion Deep Sensor Fusion for 3D Bounding Box Estimation.pdf}
}

@misc{xuRegNetSelfRegulatedNetwork2021,
  title = {{{RegNet}}: {{Self-Regulated Network}} for {{Image Classification}}},
  shorttitle = {{{RegNet}}},
  author = {Xu, Jing and Pan, Yu and Pan, Xinglin and Hoi, Steven and Yi, Zhang and Xu, Zenglin},
  year = 2021,
  month = jan,
  number = {arXiv:2101.00590},
  eprint = {2101.00590},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2101.00590},
  urldate = {2025-01-09},
  abstract = {The ResNet and its variants have achieved remarkable successes in various computer vision tasks. Despite its success in making gradient flow through building blocks, the simple shortcut connection mechanism limits the ability of re-exploring new potentially complementary features due to the additive function. To address this issue, in this paper, we propose to introduce a regulator module as a memory mechanism to extract complementary features, which are further fed to the ResNet. In particular, the regulator module is composed of convolutional RNNs (e.g., Convolutional LSTMs or Convolutional GRUs), which are shown to be good at extracting Spatio-temporal information. We named the new regulated networks as RegNet. The regulator module can be easily implemented and appended to any ResNet architecture. We also apply the regulator module for improving the Squeeze-and-Excitation ResNet to show the generalization ability of our method. Experimental results on three image classification datasets have demonstrated the promising performance of the proposed architecture compared with the standard ResNet, SE-ResNet, and other state-of-the-art architectures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C\:\\Users\\theun\\Zotero\\storage\\HWGK5F9B\\Xu et al. - 2021 - RegNet Self-Regulated Network for Image Classification.pdf;C\:\\Users\\theun\\Zotero\\storage\\5NV259J2\\2101.html}
}

@inproceedings{xuSNRAwareLowlightImage2022,
  title = {{{SNR-Aware Low-light Image Enhancement}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Xu, Xiaogang and Wang, Ruixing and Fu, Chi-Wing and Jia, Jiaya},
  year = 2022,
  month = jun,
  pages = {17693--17703},
  publisher = {IEEE},
  address = {New Orleans, LA, USA},
  doi = {10.1109/CVPR52688.2022.01719},
  urldate = {2025-03-10},
  abstract = {This paper presents a new solution for low-light image enhancement by collectively exploiting Signal-to-NoiseRatio-aware transformers and convolutional models to dynamically enhance pixels with spatial-varying operations. They are long-range operations for image regions of extremely low Signal-to-Noise-Ratio (SNR) and short-range operations for other regions. We propose to take an SNR prior to guide the feature fusion and formulate the SNRaware transformer with a new self-attention model to avoid tokens from noisy image regions of very low SNR. Extensive experiments show that our framework consistently achieves better performance than SOTA approaches on seven representative benchmarks with the same structure. Also, we conducted a large-scale user study with 100 participants to verify the superior perceptual quality of our results. The code is available at https://github.com/dvlab-research/SNRAware-Low-Light-Enhance.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-6946-3},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\KK8WUKKP\Xu et al. - 2022 - SNR-Aware Low-light Image Enhancement.pdf}
}

@article{xuUnifyingFlowStereo2023,
  title = {Unifying {{Flow}}, {{Stereo}} and {{Depth Estimation}}},
  author = {Xu, Haofei and Zhang, Jing and Cai, Jianfei and Rezatofighi, Hamid and Yu, Fisher and Tao, Dacheng and Geiger, Andreas},
  year = 2023,
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {45},
  number = {11},
  pages = {13941--13958},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2023.3298645},
  urldate = {2025-05-08},
  abstract = {We present a unified formulation and model for three motion and 3D perception tasks: optical flow, rectified stereo matching and unrectified stereo depth estimation from posed images. Unlike previous specialized architectures for each specific task, we formulate all three tasks as a unified dense correspondence matching problem, which can be solved with a single model by directly comparing feature similarities. Such a formulation calls for discriminative feature representations, which we achieve using a Transformer, in particular the cross-attention mechanism. We demonstrate that cross-attention enables integration of knowledge from another image via cross-view interactions, which greatly improves the quality of the extracted features. Our unified model naturally enables cross-task transfer since the model architecture and parameters are shared across tasks. We outperform RAFT with our unified model on the challenging Sintel dataset, and our final model that uses a few additional task-specific refinement steps outperforms or compares favorably to recent state-of-the-art methods on 10 popular flow, stereo and depth datasets, while being simpler and more efficient in terms of model design and inference speed.},
  keywords = {Costs,Cross-attention,dense correspondence,depth,Estimation,optical flow,Optical flow,Solid modeling,stereo,Task analysis,Three-dimensional displays,transformer,Transformers},
  file = {C:\Users\theun\Zotero\storage\NQC4NFXV\Xu et al. - 2023 - Unifying Flow, Stereo and Depth Estimation.pdf}
}

@inproceedings{yanDisparityAwareDomainAdaptation2020,
  title = {Disparity-{{Aware Domain Adaptation}} in {{Stereo Image Restoration}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Yan, Bo and Ma, Chenxi and Bare, Bahetiyaer and Tan, Weimin and Hoi, Steven C. H.},
  year = 2020,
  pages = {13179--13187},
  urldate = {2025-03-24},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\DQ9WCZHC\Yan et al. - 2020 - Disparity-Aware Domain Adaptation in Stereo Image Restoration.pdf}
}

@inproceedings{yangDepthAnythingUnleashing2024,
  title = {Depth {{Anything}}: {{Unleashing}} the {{Power}} of {{Large-Scale Unlabeled Data}}},
  shorttitle = {Depth {{Anything}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Yang, Lihe and Kang, Bingyi and Huang, Zilong and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},
  year = 2024,
  pages = {10371--10381},
  urldate = {2025-07-01},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\BB4FZNDT\Yang et al. - 2024 - Depth Anything Unleashing the Power of Large-Scale Unlabeled Data.pdf}
}

@article{yangDepthAnythingV22024,
  title = {Depth {{Anything V2}}},
  author = {Yang, Lihe and Kang, Bingyi and Huang, Zilong and Zhao, Zhen and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},
  year = 2024,
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {37},
  pages = {21875--21911},
  urldate = {2025-07-01},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\P9DMPXW9\Yang et al. - 2024 - Depth Anything V2.pdf}
}

@inproceedings{yangFDAFourierDomain2020,
  title = {{{FDA}}: {{Fourier Domain Adaptation}} for {{Semantic Segmentation}}},
  shorttitle = {{{FDA}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yang, Yanchao and Soatto, Stefano},
  year = 2020,
  month = jun,
  pages = {4084--4094},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR42600.2020.00414},
  urldate = {2025-06-05},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-7281-7168-5},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\CMCZMXBS\Yang and Soatto - 2020 - FDA Fourier Domain Adaptation for Semantic Segmentation.pdf}
}

@inproceedings{yangFidelityPerceptualQuality2020,
  title = {From {{Fidelity}} to {{Perceptual Quality}}: {{A Semi-Supervised Approach}} for {{Low-Light Image Enhancement}}},
  shorttitle = {From {{Fidelity}} to {{Perceptual Quality}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yang, Wenhan and Wang, Shiqi and Fang, Yuming and Wang, Yue and Liu, Jiaying},
  year = 2020,
  month = jun,
  pages = {3060--3069},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR42600.2020.00313},
  urldate = {2025-03-12},
  abstract = {Under-exposure introduces a series of visual degradation, i.e. decreased visibility, intensive noise, and biased color, etc. To address these problems, we propose a novel semi-supervised learning approach for low-light image enhancement. A deep recursive band network (DRBN) is proposed to recover a linear band representation of an enhanced normal-light image with paired low/normal-light images, and then obtain an improved one by recomposing the given bands via another learnable linear transformation based on a perceptual quality-driven adversarial learning with unpaired data. The architecture is powerful and flexible to have the merit of training with both paired and unpaired data. On one hand, the proposed network is well designed to extract a series of coarse-to-fine band representations, whose estimations are mutually beneficial in a recursive process. On the other hand, the extracted band representation of the enhanced image in the first stage of DRBN (recursive band learning) bridges the gap between the restoration knowledge of paired data and the perceptual quality preference to real high-quality images. Its second stage (band recomposition) learns to recompose the band representation towards fitting perceptual properties of highquality images via adversarial learning. With the help of this two-stage design, our approach generates the enhanced results with well reconstructed details and visually promising contrast and color distributions. Extensive evaluations demonstrate the superiority of our DRBN.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-7281-7168-5},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\39J995BV\Yang et al. - 2020 - From Fidelity to Perceptual Quality A Semi-Supervised Approach for Low-Light Image Enhancement.pdf}
}

@article{yangLightingNetIntegratedLearning2023,
  title = {{{LightingNet}}: {{An Integrated Learning Method}} for {{Low-Light Image Enhancement}}},
  shorttitle = {{{LightingNet}}},
  author = {Yang, Shaoliang and Zhou, Dongming and Cao, Jinde and Guo, Yanbu},
  year = 2023,
  journal = {IEEE Transactions on Computational Imaging},
  volume = {9},
  pages = {29--42},
  issn = {2333-9403},
  doi = {10.1109/TCI.2023.3240087},
  urldate = {2025-05-04},
  abstract = {Images captured in low-light environments suffer from serious degradation due to insufficient light, leading to the performance decline of industrial and civilian devices. To address the problems of noise, chromatic aberration, and detail distortion for enhancing low-light images using existing enhancement methods, this paper proposes an integrated learning approach (LightingNet) for low-light image enhancement. The LightingNet consists of two core components: 1) the complementary learning sub-network and 2) the vision transformer (VIT) low-light enhancement sub-network. VIT low-light enhancement sub-network is designed to learn and fit the current data to provide local high-level features through a full-scale architecture, and the complementary learning sub-network is utilized to provide global fine-tuned features through learning transfer. Extensive experiments confirm the effectiveness of the proposed LightingNet.},
  keywords = {/unread,Deep learning,Generative adversarial network,Histograms,Image enhancement,learning transfer,Lighting,low-light enhancement,Performance evaluation,Reflection,Task analysis,vision transformer},
  file = {C:\Users\theun\Zotero\storage\FGT9XQZC\Yang et al. - 2023 - LightingNet An Integrated Learning Method for Low-Light Image Enhancement.pdf}
}

@article{yangMachineLearningEnabledCooperativePerception2021,
  title = {Machine-{{Learning-Enabled Cooperative Perception}} for {{Connected Autonomous Vehicles}}: {{Challenges}} and {{Opportunities}}},
  shorttitle = {Machine-{{Learning-Enabled Cooperative Perception}} for {{Connected Autonomous Vehicles}}},
  author = {Yang, Qing and Fu, Song and Wang, Honggang and Fang, Hua},
  year = 2021,
  month = may,
  journal = {IEEE Network},
  volume = {35},
  number = {3},
  pages = {96--101},
  issn = {1558-156X},
  doi = {10.1109/MNET.011.2000560},
  urldate = {2025-01-02},
  abstract = {Connected and autonomous vehicles is a disruptive technology that has the potential to transform the current transportation system by reducing traffic accidents and enhancing driving safety. One major challenge of building such a system is how to realize effective and efficient cooperative perception among vehicles, which enables them to share local (raw or processed) perception data with each other or roadside infrastructures through wireless communications. As machine learning techniques become prevalent in autonomous vehicles, particularly in their perception subsystem, we articulate the possibility to design a machine-learning-enabled cooperative perception system for connected autonomous vehicles. Not only are the research challenges in designing cooperative perception presented, but we also focus on how to reduce communication and data processing latency in order to meet the stringent time requirements posed by autonomous driving applications. The article outlines the research challenges and opportunities in designing cooperative perception for autonomous vehicles, leveraging the recent research outcomes from machine learning, feature map quantification, millimeter-wave communications, and vehicular edge computing.},
  keywords = {/unread,Buildings,Edge computing,Internet of Vehicles,Machine learning,Reliability engineering,Safety,Transforms,Transportation,Wireless communication},
  file = {C\:\\Users\\theun\\Zotero\\storage\\C698WMX6\\Yang et al. - 2021 - Machine-Learning-Enabled Cooperative Perception for Connected Autonomous Vehicles Challenges and Op.pdf;C\:\\Users\\theun\\Zotero\\storage\\UJZ4RAAV\\9454591.html}
}

@article{yangSelfcalibratedAccelerationDetail2025,
  title = {Self-Calibrated Acceleration and Detail Preserving for Semantic Segmentation of Lactating Sows and Piglets under Low-Light Conditions},
  author = {Yang, Aqing and Xue, Yueju and Han, Na and Zheng, Jiabi and Zhang, Lei and Luo, Yizhi},
  year = 2025,
  month = jul,
  journal = {Scientific Reports},
  volume = {15},
  number = {1},
  pages = {23795},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-025-09146-0},
  urldate = {2025-10-17},
  abstract = {Pixel-wise semantic segmentation of lactating sows and piglets is critical to explore maternal traits in smart livestock breeding and production. However, semantic segmentation algorithms do not yield satisfactory results under low-light conditions because these methods are highly dependent on image quality. Therefore, an efficient and detail-preserving low-light image enhancement is necessary and crucial for animal monitoring under low-light conditions. In this paper, a low-light enhancement method called self-calibrated acceleration and detail preserving (SADP) was proposed to improve the semantic segmentation performance of lactating sows and piglets. Specifically, a self-calibration acceleration module that accelerated the convergence among all stages was proposed to improve the computational efficiency and a semantic perceptual loss term was proposed for a high detail and semantic information preservation. Plenty of experiments demonstrated that SADP outperformed the existing well-known methods in both visual quality and efficiency (-0.013s in execution time from 0.0163s to 0.0033s, -0.0031~M in mode size from 0.0034~M to 0.0003~M), and even more improved the performance of semantic segmentation of lactating sows and piglets, raising the mean IOU from 0.8686 to 0.8872. Obviously, SADP also can be used to improve the performance of other high-level visual tasks. This may build a good foundation for the following visual tasks and further promote the livestock breeding and production.},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {/unread,Image processing,Machine learning},
  file = {C:\Users\theun\Zotero\storage\T3H9ZHBU\Yang et al. - 2025 - Self-calibrated acceleration and detail preserving for semantic segmentation of lactating sows and p.pdf}
}

@inproceedings{yangSelfSupervisedMonocularDepth2024,
  title = {Self-{{Supervised Monocular Depth Estimation}} in the {{Dark}}: {{Towards Data Distribution Compensation}}},
  booktitle = {{{IJCAI Int}}. {{Joint Conf}}. {{Artif}}. {{Intell}}.},
  author = {Yang, H. and Zhao, C. and Sheng, L. and Tang, Y.},
  editor = {{Larson K.}},
  year = 2024,
  pages = {1561--1569},
  publisher = {International Joint Conferences on Artificial Intelligence},
  abstract = {Nighttime self-supervised monocular depth estimation has received increasing attention in recent years.However, using night images for self-supervision is unreliable because the photometric consistency assumption is usually violated in the videos taken under complex lighting conditions.Even with domain adaptation or photometric loss repair, performance is still limited by the poor supervision of night images on trainable networks.In this paper, we propose a self-supervised nighttime monocular depth estimation method that does not use any night images during training.Our framework utilizes day images as a stable source for self-supervision and applies physical priors (e.g., wave optics, reflection model and read-shot noise model) to compensate for some key day-night differences.With day-to-night data distribution compensation, our framework can be trained in an efficient one-stage self-supervised manner.Though no nighttime images are considered during training, qualitative and quantitative results demonstrate that our method achieves SoTA depth estimating results on the challenging nuScenes-Night and RobotCar-Night compared with existing methods. \copyright{} 2024 International Joint Conferences on Artificial Intelligence. All rights reserved.},
  isbn = {10450823 (ISSN); 978-195679204-1 (ISBN)},
  langid = {english},
  file = {C\:\\Users\\theun\\Zotero\\storage\\83RXZGLZ\\Yang et al. - 2024 - Self-Supervised Monocular Depth Estimation in the Dark Towards Data Distribution Compensation.pdf;C\:\\Users\\theun\\Zotero\\storage\\KUDAXK69\\2404.html}
}

@inproceedings{yangSIRFormerStereoImage2022,
  title = {{{SIR-Former}}: {{Stereo Image Restoration Using Transformer}}},
  booktitle = {{{MM}} 2022 - {{Proceedings}} of the 30th {{ACM International Conference}} on {{Multimedia}}},
  author = {Yang, Z. and Yao, M. and Huang, J. and Zhou, M. and Zhao, F.},
  year = 2022,
  pages = {6377--6385},
  publisher = {Association for Computing Machinery, Inc},
  doi = {10.1145/3503161.3548177},
  abstract = {Stereo image pairs record the scene from two different views and introduce cross-view information for image restoration. However, there are two challenges in utilizing the cross-view information for stereo image restoration: cross-view alignment and information fusion. Most existing methods adopt convolutional neural networks to align the views and fuse the information locally, which has difficulty in capturing the global correspondence across stereo images for view alignment and makes it hard to integrate the long-term information across views. In this paper, we propose to address the stereo image restoration with transformer by leveraging its powerful capability of modeling long-range context dependencies. Specifically, we construct a stereo image restoration transformer (SIR-Former) to effectively exploit the cross-view correlations. First, to explore the global correspondence for view alignment effectively, we devise a stereo alignment transformer (SAT) module across stereo images, enabling robust alignment under the epipolar constraint. Then, we design a stereo fusion transformer (SFT) module for aggregating the cross-view information in a small horizontal neighborhood, aiming to enhance important features for succeeding restoration. Extensive experiments show that SIR-Former can remarkably boost quantitative and qualitative quality on various image restoration tasks (e.g., super-resolution, deblurring, deraining, and low-light enhancement), which demonstrate the effectiveness of the proposed framework.  \copyright{} 2022 ACM.},
  keywords = {/unread,Computer vision,Context dependency,Convolutional neural network,Convolutional neural networks,Epipolar constraints,Image enhancement,Image reconstruction,Image super resolutions,image super-resolution,Important features,Neighbourhood,Optical resolving power,Restoration,Stereo image pairs,Stereo image processing,Stereo image restoration,stereo images restoration,Stereoimages,transformer,Transformer}
}

@article{yaoEndtoendAdaptiveObject2024,
  title = {End-to-End Adaptive Object Detection with Learnable {{Retinex}} for Low-Light City Environment},
  author = {Yao, Miao and Lu, Yijing and Mou, Jinteng and Yan, Chen and Liu, Dongjingdian},
  year = 2024,
  month = jan,
  journal = {Nondestructive Testing and Evaluation},
  volume = {39},
  number = {1},
  pages = {142--163},
  publisher = {Taylor \& Francis},
  issn = {1058-9759},
  doi = {10.1080/10589759.2023.2274011},
  urldate = {2025-02-27},
  abstract = {In the smart city context, efficient urban surveillance under low-light conditions is crucial. Accurate object detection in dimly lit areas is vital for safety and nighttime driving. However, subpar, poorly lit images due to environmental or equipment limitations pose a challenge, affecting precision in tasks like object detection and segmentation. Existing solutions often involve time-consuming, inefficient image preprocessing and lack strong theoretical support for low-light city image enhancement. To address these issue, we propose an end-to-end pipeline named LAR-YOLO that leverages convolutional network to extract a set of image transformation parameters, and implements the Retinex theory to proficiently elevate the quality of the image. Unlike conventional approaches, this innovative method eliminates the need for hand-crafted parameters and can adaptively enhance each low-light image. Additionally, due to a restricted quantity of training data, the detection model may not achieve an adequate level of expertise to enhance detection accuracy. To tackle this challenge, we introduce a cross-domain learning approach that supplements the low-light model with knowledge from normal light scenarios. Our proof-of-principle experiments and ablation studies utilising ExDark and VOC datasets demonstrate that our proposed method outperforms similar low-light object detection algorithms by approximately 13\% in terms of accuracy.},
  keywords = {/unread,cross-domain learning,low-light image processing,Object detection,Retinex theory,smart city},
  file = {C:\Users\theun\Zotero\storage\9TAEQSQT\Yao et al. - 2024 - End-to-end adaptive object detection with learnable Retinex for low-light city environment.pdf}
}

@article{yeSurveyLearningbasedLowlight2024,
  title = {A Survey on Learning-Based Low-Light Image and Video Enhancement},
  author = {Ye, Jing and Qiu, Changzhen and Zhang, Zhiyong},
  year = 2024,
  month = jan,
  journal = {Displays},
  volume = {81},
  pages = {102614},
  issn = {01419382},
  doi = {10.1016/j.displa.2023.102614},
  urldate = {2024-12-04},
  abstract = {Low-light enhancement (LLE) is a fundamental technique for improving the visual perception and interpretability of images and videos that suffer from low light degradation. In recent years, learning-based low-light image and video enhancement has made significant strides. Low-light image enhancement (LLIE) mainly focuses on enhancing images in a spatial-varying manner, while low-light video enhancement (LLVE) emphasizes exploiting temporal information in videos. In this survey, we present a comprehensive review of the research progress of LLE, categorizing LLIE and LLVE solutions according to their task attributes for the first time. We also provide a systematic analysis and discussion of technical details from various aspects. To deepen researchers' understanding of LLE technology development and provide performance benchmarks, we extensively evaluate various LLIE and LLVE models using datasets for low-light image, video, and high-level visual applications. Based on the experimental analysis, we summarize the current limitations and challenges of LLE. Additionally, our study offers insights into potential future research directions for LLE.},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\FLWHEX6W\Ye et al. - 2024 - A survey on learning-based low-light image and video enhancement.pdf}
}

@article{yeVELIEVehicleBasedEfficient2024,
  title = {{{VELIE}}: {{A Vehicle-Based Efficient Low-Light Image Enhancement Method}} for {{Intelligent Vehicles}}},
  shorttitle = {{{VELIE}}},
  author = {Ye, Linwei and Wang, Dong and Yang, Dongyi and Ma, Zhiyuan and Zhang, Quan},
  year = 2024,
  month = jan,
  journal = {Sensors},
  volume = {24},
  number = {4},
  pages = {1345},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s24041345},
  urldate = {2025-02-17},
  abstract = {In Advanced Driving Assistance Systems (ADAS), Automated Driving Systems (ADS), and Driver Assistance Systems (DAS), RGB camera sensors are extensively utilized for object detection, semantic segmentation, and object tracking. Despite their popularity due to low costs, RGB cameras exhibit weak robustness in complex environments, particularly underperforming in low-light conditions, which raises a significant concern. To address these challenges, multi-sensor fusion systems or specialized low-light cameras have been proposed, but their high costs render them unsuitable for widespread deployment. On the other hand, improvements in post-processing algorithms offer a more economical and effective solution. However, current research in low-light image enhancement still shows substantial gaps in detail enhancement on nighttime driving datasets and is characterized by high deployment costs, failing to achieve real-time inference and edge deployment. Therefore, this paper leverages the Swin Vision Transformer combined with a gamma transformation integrated U-Net for the decoupled enhancement of initial low-light inputs, proposing a deep learning enhancement network named Vehicle-based Efficient Low-light Image Enhancement (VELIE). VELIE achieves state-of-the-art performance on various driving datasets with a processing time of only 0.19 s, significantly enhancing high-dimensional environmental perception tasks in low-light conditions.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {autonomous driving,low-light image enhancement,RGB sensor image processing},
  file = {C:\Users\theun\Zotero\storage\Z8U3ZMLN\Ye et al. - 2024 - VELIE A Vehicle-Based Efficient Low-Light Image Enhancement Method for Intelligent Vehicles.pdf}
}

@inproceedings{yiDiffRetinexRethinkingLowlight2023,
  title = {Diff-{{Retinex}}: {{Rethinking Low-light Image Enhancement}} with {{A Generative Diffusion Model}}},
  shorttitle = {Diff-{{Retinex}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Yi, Xunpeng and Xu, Han and Zhang, Hao and Tang, Linfeng and Ma, Jiayi},
  year = 2023,
  pages = {12302--12311},
  urldate = {2025-11-17},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\XHFZLV4Q\Yi et al. - 2023 - Diff-Retinex Rethinking Low-light Image Enhancement with A Generative Diffusion Model.pdf}
}

@inproceedings{yingDelvingLightDarkSemantic2022,
  title = {Delving into {{Light-Dark Semantic Segmentation}} for {{Indoor Scenes Understanding}}},
  booktitle = {Proceedings of the 1st {{Workshop}} on {{Photorealistic Image}} and {{Environment Synthesis}} for {{Multimedia Experiments}}},
  author = {Ying, Xiaowen and Lang, Bo and Zheng, Zhihao and Chuah, Mooi Choo},
  year = 2022,
  month = oct,
  series = {{{PIES-ME}} '22},
  pages = {3--9},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3552482.3556556},
  urldate = {2025-06-15},
  abstract = {State-of-the-art segmentation models are mostly trained with large-scale datasets collected under favorable lighting conditions, and hence directly applying such trained models to dark scenes will result in unsatisfactory performance. In this paper, we present the first benchmark dataset and evaluation methodology to study the problem of semantic segmentation under different lighting conditions for indoor scenes. Our dataset, namely LDIS, consists of samples collected from 87 different indoor scenes under both well-illuminated and low-light conditions. Different from existing work, our benchmark provides a new task setting, namely Light-Dark Semantic Segmentation (LDSS), which adopts four different evaluation metrics that assess the performance of a model from multiple aspects. We perform extensive experiments and ablation studies to compare the effectiveness of different existing techniques with our standardized evaluation protocol. In addition, we propose a new technique, namely DepthAux, that utilizes the consistency of depth images under different lighting conditions to help a model learn a unified and illumination-invariant representation. Our experimental results show that the proposed DepthAux can provide consistent and significant improvements when applied to a variety of different models. Our dataset and other resources are publicly available on our project page: http://mercy.cse.lehigh.edu/LDIS/},
  isbn = {978-1-4503-9500-7},
  keywords = {Benchmarking,Dataset,Evaluation,Large dataset,Large-scale datasets,Lighting,Lighting conditions,Low light,Low-light,Performance,Scene understanding,Segmentation models,Semantic segmentation,Semantic Segmentation,Semantics,State of the art},
  file = {C:\Users\theun\Zotero\storage\V6SFV67W\Ying et al. - 2022 - Delving into Light-Dark Semantic Segmentation for Indoor Scenes Understanding.pdf}
}

@article{yingSCDFSeeingClearly2025,
  title = {{{SCDF}}: {{Seeing Clearly Through Dark}} and {{Fog}}, an {{Adaptive Semantic Segmentation Scheme}} for {{Autonomous Vehicle}}},
  shorttitle = {{{SCDF}}},
  author = {Ying, Zuobin and Lin, Zhengcheng and Li, Zhenyu and Huang, Xiaochun and Ding, Weiping},
  year = 2025,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  pages = {1--14},
  issn = {1558-0016},
  doi = {10.1109/TITS.2025.3565327},
  urldate = {2025-06-15},
  abstract = {Semantic segmentation is a pivotal research area in the advancement of autonomous driving, with a particular focus on addressing adverse weather conditions such as night, rain, and fog. However, a challenging problem that remains is effectively segmenting environments that contain multiple adverse factors within the same scene. To tackle this issue, we present SCDF, an adaptive semantic segmentation approach. SCDF employs two separate image processing modules for denoising and enhancement of low-quality inputs. To enhance the denoising effect, we also optimized the loss function of its defogging module. Furthermore, we proposed a novel weight adaptation module, which enables SCDF to switch between different driving scenarios. To evaluate the effectiveness of SCDF, we conduct experiments on the ACDC and Rainy \& Foggy cityscapes datasets, respectively achieving a higher mean Intersection over Union (mIoU) score of 61.4\%(ACDC) and 79.1\%(Rainy \& Foggy Cityscapes). This promising results demonstrate the potential of SCDF in significantly improving the performance of autonomous driving systems in complex environments with multiple adverse factors.},
  keywords = {Adverse weather,Attention mechanisms,Autonomous driving,Autonomous vehicles,Autonomous Vehicles,Condition,De-noising,image denoising,image enhancement,Image enhancement,Image Processing Module,image segmentation,Image segmentation,Images segmentations,Intelligent transportation systems,Meteorology,Noise reduction,Optical flows,Photointerpretation,Research areas,Salt and pepper noise,Segmentation scheme,Semantic segmentation,Semantic Segmentation,Semantics,Switches},
  file = {C:\Users\theun\Zotero\storage\SX2EVGC5\Ying et al. - 2025 - SCDF Seeing Clearly Through Dark and Fog, an Adaptive Semantic Segmentation Scheme for Autonomous V.pdf}
}

@inproceedings{yinSideWindowFiltering2019,
  title = {Side {{Window Filtering}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yin, Hui and Gong, Yuanhao and Qiu, Guoping},
  year = 2019,
  month = jun,
  pages = {8750--8758},
  publisher = {IEEE},
  address = {Long Beach, CA, USA},
  doi = {10.1109/CVPR.2019.00896},
  urldate = {2025-04-01},
  abstract = {Local windows are routinely used in computer vision and almost without exception the center of the window is aligned with the pixels being processed. We show that this conventional wisdom is not universally applicable. When a pixel is on an edge, placing the center of the window on the pixel is one of the fundamental reasons that cause many filtering algorithms to blur the edges. Based on this insight, we propose a new Side Window Filtering (SWF) technique which aligns the window's side or corner with the pixel being processed. The SWF technique is surprisingly simple yet theoretically rooted and very effective in practice. We show that many traditional linear and nonlinear filters can be easily implemented under the SWF framework. Extensive analysis and experiments show that implementing the SWF principle can significantly improve their edge preserving capabilities and achieve state of the art performances in applications such as image smoothing, denoising, enhancement, structure-preserving texture-removing, mutual-structure extraction, and HDR tone mapping. In addition to image filtering, we further show that the SWF principle can be extended to other applications involving the use of a local window. Using colorization by optimization as an example, we demonstrate that implementing the SWF principle can effectively prevent artifacts such as color leakage associated with the conventional implementation. Given the ubiquity of window based operations in computer vision, the new SWF technique is likely to benefit many more applications.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-7281-3293-8},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\XEXRKB8L\Yin et al. - 2019 - Side Window Filtering.pdf}
}

@article{yuanKeypointsBasedDeepFeature2022,
  title = {Keypoints-{{Based Deep Feature Fusion}} for {{Cooperative Vehicle Detection}} of {{Autonomous Driving}}},
  author = {Yuan, Yunshuang and Cheng, Hao and Sester, Monika},
  year = 2022,
  month = apr,
  journal = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {2},
  pages = {3054--3061},
  issn = {2377-3766},
  doi = {10.1109/LRA.2022.3143299},
  urldate = {2025-01-02},
  abstract = {Sharing collective perception messages (CPM) between vehicles is investigated to decrease occlusions so as to improve the perception accuracy and safety of autonomous driving. However, highly accurate data sharing and low communication overhead is a big challenge for collective perception, especially when real-time communication is required among connected and automated vehicles. In this letter, we propose an efficient and effective keypoints-based deep feature fusion framework built on the 3D object detector PV-RCNN, called Fusion PV-RCNN (FPV-RCNN for short), for collective perception. We introduce a high-performance bounding box proposal matching module and a keypoints selection strategy to compress the CPM size and solve the multi-vehicle data fusion problem. Besides, we also propose an effective localization error correction module based on the maximum consensus principle to increase the robustness of the data fusion. Compared to a bird's-eye view (BEV) keypoints feature fusion, FPV-RCNN achieves improved detection accuracy by about 9\% at a high evaluation criterion (IoU 0.7) on the synthetic dataset COMAP dedicated to collective perception. In addition, its performance is comparable to two raw data fusion baselines that have no data loss in sharing. Moreover, our method also significantly decreases the CPM size to less than 0.3 KB, and is thus about 50 times smaller than the BEV feature map sharing used in previous works. Even with further decreased CPM feature channels, i. e., from 128 to 32, the detection performance does not show apparent drops. The code of our method is available at https://github.com/YuanYunshuang/FPV\_RCNN.},
  keywords = {/unread,Data integration,Feature extraction,Location awareness,object detection,Point cloud compression,Proposals,segmentation and categorization,Sensor fusion,sensor networks,Three-dimensional displays,Vehicle detection},
  file = {C\:\\Users\\theun\\Zotero\\storage\\ZXISN8HM\\Yuan et al. - 2022 - Keypoints-Based Deep Feature Fusion for Cooperative Vehicle Detection of Autonomous Driving.pdf;C\:\\Users\\theun\\Zotero\\storage\\ID5W65BL\\9682601.html}
}

@inproceedings{yuBDD100KDiverseDriving2020,
  title = {{{BDD100K}}: {{A Diverse Driving Dataset}} for {{Heterogeneous Multitask Learning}}},
  shorttitle = {{{BDD100K}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yu, Fisher and Chen, Haofeng and Wang, Xin and Xian, Wenqi and Chen, Yingying and Liu, Fangchen and Madhavan, Vashisht and Darrell, Trevor},
  year = 2020,
  month = jun,
  pages = {2633--2642},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR42600.2020.00271},
  urldate = {2025-05-15},
  abstract = {Datasets drive vision progress, yet existing driving datasets are impoverished in terms of visual content and supported tasks to study multitask learning for autonomous driving. Researchers are usually constrained to study a small set of problems on one dataset, while real-world computer vision applications require performing tasks of various complexities. We construct BDD100K 1, the largest driving video dataset with 100K videos and 10 tasks to evaluate the exciting progress of image recognition algorithms on autonomous driving. The dataset possesses geographic, environmental, and weather diversity, which is useful for training models that are less likely to be surprised by new conditions. Based on this diverse dataset, we build a benchmark for heterogeneous multitask learning and study how to solve the tasks together. Our experiments show that special training strategies are needed for existing models to perform such heterogeneous tasks. BDD100K opens the door for future studies in this important venue.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-7281-7168-5},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\KB9AVKNG\Yu et al. - 2020 - BDD100K A Diverse Driving Dataset for Heterogeneous Multitask Learning.pdf}
}

@article{yuJointCorrectingRefinement2024,
  title = {Joint {{Correcting}} and {{Refinement}} for {{Balanced Low-Light Image Enhancement}}},
  author = {Yu, Nana and Shi, Hong and Han, Yahong},
  year = 2024,
  journal = {IEEE Transactions on Multimedia},
  volume = {26},
  pages = {6310--6324},
  issn = {1941-0077},
  doi = {10.1109/TMM.2023.3348333},
  urldate = {2025-02-27},
  abstract = {Low-light image enhancement tasks demand an appropriate balance among brightness, color, and illumination. While existing methods often focus on one aspect of the image without considering how to pay attention to this balance, which will cause problems of color distortion and overexposure etc. This seriously affects both human visual perception and the performance of high-level visual models. In this work, a novel synergistic structure is proposed which can balance brightness, color, and illumination more effectively. Specifically, the proposed method, so-called Joint Correcting and Refinement Network (JCRNet), which mainly consists of three stages to balance brightness, color, and illumination of enhancement. Stage 1: we utilize a basic encoder-decoder and local supervision mechanism to extract local information and more comprehensive details for enhancement. Stage 2: cross-stage feature transmission and spatial feature transformation further facilitate color correction and feature refinement. Stage 3: we employ a dynamic illumination adjustment approach to embed residuals between predicted and ground truth images into the model, adaptively adjusting illumination balance. Extensive experiments demonstrate that the proposed method exhibits comprehensive performance advantages over 21 state-of-the-art methods on 9 benchmark datasets. Furthermore, a more persuasive experiment has been conducted to validate our approach the effectiveness in downstream visual tasks (e.g., saliency detection). Compared to several enhancement models, the proposed method effectively improves the segmentation results and quantitative metrics of saliency detection.},
  keywords = {back projection,Brightness,color correction,Distortion,Feature extraction,illumination adjustment,Image color analysis,Image enhancement,Lighting,Low-light enhancement,Visualization},
  file = {C\:\\Users\\theun\\Zotero\\storage\\RL9A4J5N\\Yu et al. - 2024 - Joint Correcting and Refinement for Balanced Low-Light Image Enhancement.pdf;C\:\\Users\\theun\\Zotero\\storage\\P6CJ9Y3A\\10376299.html}
}

@misc{yuMultiScaleContextAggregation2015,
  title = {Multi-{{Scale Context Aggregation}} by {{Dilated Convolutions}}},
  author = {Yu, Fisher and Koltun, Vladlen},
  year = 2015,
  month = nov,
  journal = {arXiv.org},
  urldate = {2025-06-09},
  abstract = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.},
  howpublished = {https://arxiv.org/abs/1511.07122v3},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\E59NS78D\Yu and Koltun - 2015 - Multi-Scale Context Aggregation by Dilated Convolutions.pdf}
}

@article{yuNightVisionSelfsupervised2023,
  title = {Night Vision Self-Supervised {{Reflectance-Aware Depth Estimation}} Based on Reflectance},
  author = {Yu, Yao and Pu, Fangling and Chen, Hongjia and Tang, Rui and Li, Jinwen and Xu, Xin},
  year = 2023,
  month = dec,
  journal = {Journal of Visual Communication and Image Representation},
  volume = {97},
  pages = {103962},
  issn = {10473203},
  doi = {10.1016/j.jvcir.2023.103962},
  urldate = {2025-06-30},
  abstract = {The depth estimation of nighttime images is a challenging problem due to the lack of accurate ground-truth depth labels. Although various self-supervised methods leveraging texture information have been proposed to solve the problem, the performance is still not satisfactory due to the imaging limitations of visible cameras. To this end, we propose a self-supervised Reflectance-Aware Depth Estimation approach based on reflectance for nighttime images. Two major factors strengthen the proposed approach: a Reflectance Extraction Network and a feature consistency loss. We introduce the Reflectance Extraction Network to extract texture information based on the finding that the texture is beneficial for depth estimation. Then, we utilize the feature consistency loss to help the baseline network to learn the intrinsic feature rather than the images' light. Experiment results on the challenging Oxford RobotCar dataset confirm the robustness and effectiveness of our approach.},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\MSJMP6PN\Yu et al. - 2023 - Night vision self-supervised Reflectance-Aware Depth Estimation based on reflectance.pdf}
}

@misc{yuxiangPorcoflyMTVMultiviewThermalVisibleImageDataset2025,
  title = {Porcofly/{{MTV-Multi-view-Thermal-Visible-Image-Dataset}}},
  author = {Yuxiang, Liu},
  year = 2025,
  month = may,
  urldate = {2025-05-15},
  keywords = {/unread}
}

@article{zengLearningImageAdaptive3D2022,
  title = {Learning {{Image-Adaptive 3D Lookup Tables}} for {{High Performance Photo Enhancement}} in {{Real-Time}}},
  author = {Zeng, Hui and Cai, Jianrui and Li, Lida and Cao, Zisheng and Zhang, Lei},
  year = 2022,
  month = apr,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {4},
  pages = {2058--2073},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2020.3026740},
  urldate = {2025-03-12},
  abstract = {Recent years have witnessed the increasing popularity of learning based methods to enhance the color and tone of photos. However, many existing photo enhancement methods either deliver unsatisfactory results or consume too much computational and memory resources, hindering their application to high-resolution images (usually with more than 12 megapixels) in practice. In this paper, we learn image-adaptive 3-dimensional lookup tables (3D LUTs) to achieve fast and robust photo enhancement. 3D LUTs are widely used for manipulating color and tone of photos, but they are usually manually tuned and fixed in camera imaging pipeline or photo editing tools. We, for the first time to our best knowledge, propose to learn 3D LUTs from annotated data using pairwise or unpaired learning. More importantly, our learned 3D LUT is image-adaptive for flexible photo enhancement. We learn multiple basis 3D LUTs and a small convolutional neural network (CNN) simultaneously in an end-to-end manner. The small CNN works on the down-sampled version of the input image to predict content-dependent weights to fuse the multiple basis 3D LUTs into an image-adaptive one, which is employed to transform the color and tone of source images efficiently. Our model contains less than 600K parameters and takes less than 2 ms to process an image of 4K resolution using one Titan RTX GPU. While being highly efficient, our model also outperforms the state-of-the-art photo enhancement methods by a large margin in terms of PSNR, SSIM and a color difference metric on two publically available benchmark datasets. Code will be released at https://github.com/HuiZeng/Image-Adaptive-3DLUT.},
  keywords = {/unread,3D lookup table,Cameras,color enhancement,Image color analysis,Photo enhancement,photo retouching,Pipelines,Table lookup,Three-dimensional displays,tone enhncement,Tools},
  file = {C\:\\Users\\theun\\Zotero\\storage\\NR8IPNJ7\\Zeng et al. - 2022 - Learning Image-Adaptive 3D Lookup Tables for High Performance Photo Enhancement in Real-Time.pdf;C\:\\Users\\theun\\Zotero\\storage\\SCH6AD5I\\9206076.html}
}

@inproceedings{zhaBeyondlineofsightPerceptionEnhancement2022,
  title = {Beyond-Line-of-Sight {{Perception Enhancement}} via {{Information Interaction}} in {{Connected Autonomous Driving Environment}}},
  booktitle = {2022 {{China Automation Congress}} ({{CAC}})},
  author = {Zha, Yuanyuan and Shangguan, Wei},
  year = 2022,
  month = nov,
  pages = {1809--1814},
  issn = {2688-0938},
  doi = {10.1109/CAC57257.2022.10054747},
  urldate = {2025-01-02},
  abstract = {On account of occlusion and limited visual range, the independent perception of the single vehicle is restricted, which cannot meet the requirements of high-level autonomous driving. In view of the characteristics of information interaction in connected environment, a vehicle-vehicle based beyond-line-of-sight fusion perception framework is proposed. Effective data fusion of multi-source heterogeneous sensor is realized based on D-S evidence theory. Precise object detection and recognition is achieved based on lightweight object detection Faster R-CNN algorithm with backbone used MobileNetV2. Finally, the beyond-line-of-sight perception enhancement method in typical scenes is verified and analyzed on Prescan. Results show that the presented method helps autonomous vehicles make full use of sensory data effectively, expand perception scope, avoid blind fields, which plays a supporting role in the safe and efficient operation of autonomous vehicles.},
  keywords = {/unread,Automation,Autonomous vehicles,beyond-line-of-sight perception,connected autonomous driving,data fusion,Data integration,Decision making,Evidence theory,lightweight object detection,Object detection,Visualization},
  file = {C\:\\Users\\theun\\Zotero\\storage\\XRMIWWGB\\Zha and Shangguan - 2022 - Beyond-line-of-sight Perception Enhancement via Information Interaction in Connected Autonomous Driv.pdf;C\:\\Users\\theun\\Zotero\\storage\\R8MID8C5\\10054747.html}
}

@article{zhaHierarchicalPerceptionEnhancement2024,
  title = {Hierarchical {{Perception Enhancement}} for {{Different Levels}} of {{Autonomous Driving}}: {{A Review}}},
  shorttitle = {Hierarchical {{Perception Enhancement}} for {{Different Levels}} of {{Autonomous Driving}}},
  author = {Zha, Yuanyuan and Shangguan, Wei and Chai, Linguo and Chen, Jingjing},
  year = 2024,
  month = jun,
  journal = {IEEE Sensors Journal},
  volume = {24},
  number = {11},
  pages = {17366--17386},
  issn = {1530-437X, 1558-1748, 2379-9153},
  doi = {10.1109/JSEN.2024.3388503},
  urldate = {2024-12-04},
  abstract = {High precision, strong reliability, and wide range of perception can provide essential data support for the safe and efficient driving of autonomous vehicles. Based on the technology and method of autonomous driving perception, and taking into account the background and current research, a three-hierarchy architecture is developed to achieve perception enhancement: autonomous perception including self-localization and visual perception, fusion perception, and cooperative perception. Depending on the perception needs and other features of different levels of autonomous driving, different enhancement methods can be applied. First, perception is enhanced by algorithm optimization mainly for driving assistance. Next, perception is enhanced by multimodal and heterogeneous data fusion for conditional autonomous driving. Then, perception is enhanced by connected and interactive data cooperation for high-level autonomous driving. If the level of autonomous driving changes, the main and primary modes of perception enhancement can also be switched. The development revealed that cooperative perception enhancement of swarm intelligence based on vehicle-to-vehicle (V2V), vehicle-to-infrastructure (V2I), and other connected technologies is an inevitable trend. Future studies will focus on processing multisource and heterogeneous sensor data. The core is to break through the technical bottleneck of fusion and cooperation, and improve the perception ability and adaptability in complex traffic environments. Perception enhancement will accelerate the development and practical implementation of autonomous driving and ultimately improve traffic safety and efficiency.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\ZDDHRK8A\Zha et al. - 2024 - Hierarchical Perception Enhancement for Different Levels of Autonomous Driving A Review.pdf}
}

@article{zhangBrighteningLowlightImages2021,
  title = {Beyond {{Brightening Low-light Images}}},
  author = {Zhang, Yonghua and Guo, Xiaojie and Ma, Jiayi and Liu, Wei and Zhang, Jiawan},
  year = 2021,
  month = apr,
  journal = {International Journal of Computer Vision},
  volume = {129},
  number = {4},
  pages = {1013--1037},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-020-01407-x},
  urldate = {2025-02-27},
  abstract = {Images captured under low-light conditions often suffer from (partially) poor visibility. Besides unsatisfactory lightings, multiple types of degradation, such as noise and color distortion due to the limited quality of cameras, hide in the dark. In other words, solely turning up the brightness of dark regions will inevitably amplify pollution. Thus, low-light image enhancement should not only brighten dark regions, but also remove hidden artifacts. To achieve the goal, this work builds a simple yet effective network, which, inspired by Retinex theory, decomposes images into two components. Following a divide-and-conquer principle, one component (illumination) is responsible for light adjustment, while the other (reflectance) for degradation removal. In such a way, the original space is decoupled into two smaller subspaces, expecting for better regularization/learning. It is worth noticing that our network is trained with paired images shot under different exposure conditions, instead of using any ground-truth reflectance and illumination information. Extensive experiments are conducted to demonstrate the efficacy of our design and its superiority over the state-of-the-art alternatives, especially in terms of the robustness against severe visual defects and the flexibility in adjusting light levels. Our code is made publicly available at https://github.com/zhangyhuaee/KinD\_plus.},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\KW749UJV\Zhang et al. - 2021 - Beyond Brightening Low-light Images.pdf}
}

@article{zhangDeepLearningLane2022,
  title = {Deep {{Learning}} in {{Lane Marking Detection}}: {{A Survey}}},
  shorttitle = {Deep {{Learning}} in {{Lane Marking Detection}}},
  author = {Zhang, Youcheng and Lu, Zongqing and Zhang, Xuechen and Xue, Jing-Hao and Liao, Qingmin},
  year = 2022,
  month = jul,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {23},
  number = {7},
  pages = {5976--5992},
  issn = {1524-9050, 1558-0016},
  doi = {10.1109/TITS.2021.3070111},
  urldate = {2025-01-02},
  abstract = {Lane marking detection is a fundamental but crucial step in intelligent driving systems. It can not only provide relevant road condition information to prevent lane departure but also assist vehicle positioning and forehead car detection. However, lane marking detection faces many challenges, including extreme lighting, missing lane markings, and obstacle obstructions. Recently, deep learning-based algorithms draw much attention in intelligent driving society because of their excellent performance. In this paper, we review deep learning methods for lane marking detection, focusing on their network structures and optimization objectives, the two key determinants of their success. Besides, we summarize existing lane-related datasets, evaluation criteria, and common data processing techniques. We also compare the detection performance and running time of various methods, and conclude with some current challenges and future trends for deep learning-based lane marking detection algorithm.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\CBFT5N38\Zhang et al. - 2022 - Deep Learning in Lane Marking Detection A Survey.pdf}
}

@inproceedings{zhangDistillingSemanticPriors2024,
  title = {Distilling {{Semantic Priors}} from {{SAM}} to {{Efficient Image Restoration Models}}},
  booktitle = {2024 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhang, Quan and Liu, Xiaoyu and Li, Wei and Chen, Hanting and Liu, Junchao and Hu, Jie and Xiong, Zhiwei and Yuan, Chun and Wang, Yunhe},
  year = 2024,
  month = jun,
  pages = {25409--25419},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR52733.2024.02401},
  urldate = {2025-06-19},
  abstract = {In image restoration (IR), leveraging semantic priors from segmentation models has been a common approach to improve performance. The recent segment anything model (SAM) has emerged as a powerful tool for extracting advanced semantic priors to enhance IR tasks. However, the computational cost of SAM is prohibitive for IR, compared to existing smaller IR models. The incorporation of SAM for extracting semantic priors considerably hampers the model inference efficiency. To address this issue, we propose a general framework to distill SAM's semantic knowledge to boost exiting IR models without interfering with their inference process. Specifically, our proposed framework consists of the semantic priors fusion (SPF) scheme and the semantic priors distillation (SPD) scheme. SPF fuses two kinds of information between the restored image predicted by the original IR model and the semantic mask predicted by SAM for the refined restored image. SPD leverages a selfdistillation manner to distill the fused semantic priors to boost the performance of original IR models. Additionally, we design a semantic-guided relation (SGR) module for SPD, which ensures semantic feature representation space consistency to fully distill the priors. We demonstrate the effectiveness of our framework across multiple IR models and tasks, including deraining, deblurring, and denoising.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-5300-6},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\KI7F3HZ7\Zhang et al. - 2024 - Distilling Semantic Priors from SAM to Efficient Image Restoration Models.pdf}
}

@inproceedings{zhangKindlingDarknessPractical2019,
  title = {Kindling the {{Darkness}}: {{A Practical Low-light Image Enhancer}}},
  shorttitle = {Kindling the {{Darkness}}},
  booktitle = {Proceedings of the 27th {{ACM International Conference}} on {{Multimedia}}},
  author = {Zhang, Yonghua and Zhang, Jiawan and Guo, Xiaojie},
  year = 2019,
  month = oct,
  series = {{{MM}} '19},
  pages = {1632--1640},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3343031.3350926},
  urldate = {2025-08-07},
  abstract = {Images captured under low-light conditions often suffer from (partially) poor visibility. Besides unsatisfactory lightings, multiple types of degradations, such as noise and color distortion due to the limited quality of cameras, hide in the dark. In other words, solely turning up the brightness of dark regions will inevitably amplify hidden artifacts. This work builds a simple yet effective network for Kindling the Darkness (denoted as KinD), which, inspired by Retinex theory, decomposes images into two components. One component (illumination) is responsible for light adjustment, while the other (reflectance) for degradation removal. In such a way, the original space is decoupled into two smaller subspaces, expecting to be better regularized/learned. It is worth to note that our network is trained with paired images shot under different exposure conditions, instead of using any ground-truth reflectance and illumination information. Extensive experiments are conducted to demonstrate the efficacy of our design and its superiority over state-of-the-art alternatives. Our KinD is robust against severe visual defects, and user-friendly to arbitrarily adjust light levels. In addition, our model spends less than 50ms to process an image in VGA resolution on a 2080Ti GPU. All the above merits make our KinD attractive for practical use.},
  isbn = {978-1-4503-6889-6},
  file = {C:\Users\theun\Zotero\storage\5H4IEYCI\Zhang et al. - 2019 - Kindling the Darkness A Practical Low-light Image Enhancer.pdf}
}

@inproceedings{zhangLearningLowLightIndoor2021,
  title = {Towards {{Learning Low-Light Indoor Semantic Segmentation}} with {{Illumination-Invariant Features}}},
  booktitle = {The {{International Archives}} of the {{Photogrammetry}}, {{Remote Sensing}} and {{Spatial Information Sciences}}},
  author = {Zhang, N. and Nex, F. C. and Kerle, N. and Vosselman, G.},
  year = 2021,
  month = jun,
  pages = {427--432},
  publisher = {{International Society for Photogrammetry and Remote Sensing (ISPRS)}},
  doi = {10.5194/isprs-archives-XLIII-B2-2021-427-2021},
  urldate = {2025-06-13},
  langid = {english},
  keywords = {Dataset,Deep learning,Deep Learning,Illumination changes,Illumination invariant,Image decomposition,Image Decomposition,Image segmentation,Invariant features,Low light,Low-light,Reflection,Scene understanding,Scene Understanding,Segmentation models,Semantic segmentation,Semantic Segmentation,Semantics},
  file = {C:\Users\theun\Zotero\storage\8DXP3I7S\Zhang et al. - 2021 - Towards Learning Low-Light Indoor Semantic Segmentation with Illumination-Invariant Features.pdf}
}

@inproceedings{zhangLightweightSelfSupervisedMonocular2025,
  title = {Lightweight {{Self-Supervised Monocular Depth Estimation}} for {{All-Day Scenes Using Generative Adversarial Network}}},
  booktitle = {{{ICASSP}} 2025 - 2025 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Zhang, Junding and Rao, Di and Akoudad, Youssef and Gao, Wei and Chen, Jie},
  year = 2025,
  month = apr,
  pages = {1--5},
  issn = {2379-190X},
  doi = {10.1109/ICASSP49660.2025.10887989},
  urldate = {2025-06-29},
  abstract = {Self-supervised monocular depth estimation (MDE) has achieved performance levels comparable to supervised methods in well-lit environments. However, current methods struggle particularly with challenging nighttime scenes. Existing all-day self-supervised MDE methods often rely on specialized nighttime datasets, which require extensive data collection and annotation, adding complexity and resource demands to the training process. To overcome this limitation, we propose ADDepth, a novel lightweight all-day self-supervised MDE network. ADDepth leverages CoMoGAN to transform daytime images into nighttime scenes, thereby circumventing the need for a separate nighttime dataset. Additionally, we introduce a low-scale consistency loss to enhance depth map quality by mitigating the issue of blurred depth predictions, a common challenge caused by the reduced number of convolutional kernels in decoder layers. Our approach retains the network's lightweight design while significantly improving its generalization across different lighting conditions. Experimental results on public benchmarks validate the superiority of the proposed ADDepth. The source code is available at https://github.com/zjdzhou/ADDepth.},
  keywords = {all-day scenes,Complexity theory,Decoding,Depth measurement,Generative adversarial networks,Kernel,Lighting,lightweight,monocular depth estimation,Self-supervised learning,Source coding,Speech processing,Training,Transforms},
  file = {C:\Users\theun\Zotero\storage\E9SZRGZF\Zhang et al. - 2025 - Lightweight Self-Supervised Monocular Depth Estimation for All-Day Scenes Using Generative Adversari.pdf}
}

@article{zhangLISULowlightIndoor2022,
  title = {{{LISU}}: {{Low-light}} Indoor Scene Understanding with Joint Learning of Reflectance Restoration},
  shorttitle = {{{LISU}}},
  author = {Zhang, Ning and Nex, Francesco and Kerle, Norman and Vosselman, George},
  year = 2022,
  month = jan,
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {183},
  pages = {470--481},
  issn = {0924-2716},
  doi = {10.1016/j.isprsjprs.2021.11.010},
  urldate = {2025-02-27},
  abstract = {Semantic segmentation using convolutional neural networks (CNNs) achieves higher accuracy than traditional methods, but it fails to yield satisfactory results under illumination variants when the training set is limited. In this paper we present a new data set containing both real and rendered images and a novel cascade network to study semantic segmentation in low-light indoor environments. Specifically, the network decomposes a low-light image into illumination and reflectance components, and then a multi-tasking learning scheme is built. One branch learns to reduce noise and restore information on the reflectance (reflectance restoration branch). Another branch learns to segment the reflectance map (semantic segmentation branch). The CNN features from two tasks are concatenated together so as to improve the segmentation accuracy by embedding the illumination-invariant features. We compare our approach with other CNN-based segmentation frameworks, including the state-of-the-art DeepLab v3+, on the proposed real data set, and our approach achieves the highest mIoU (47.6\%). The experimental results also show that the semantic information supports the restoration of a sharper reflectance map, thus further improving the segmentation. Besides, we pre-train a model with the proposed large-scale rendered images and then fine-tune it on the real images. The pre-training results in an improvement of mIoU by 7.2\%. Our models and data set are publicly available for research. This research is part of the EU project INGENIOUS11https://ingenious-first-responders.eu/. Our data sets and models are available on our website22https://github.com/noahzn/LISU.},
  keywords = {accuracy assessment,algorithm,artificial neural network,Convolutional neural network,Convolutional neural networks,data set,Data set,Deep learning,Image decomposition,Intrinsic image decomposition,Intrinsic images,Low light,Low-light,Real images,Reflection,Rendered images,Rendering (computer graphics),Restoration,segmentation,Semantic segmentation,Semantic Segmentation,Semantics},
  file = {C\:\\Users\\theun\\Zotero\\storage\\5W92BVCZ\\Zhang et al. - 2022 - LISU Low-light indoor scene understanding with joint learning of reflectance restoration.pdf;C\:\\Users\\theun\\Zotero\\storage\\UEFHGNUC\\S0924271621003087.html}
}

@inproceedings{zhangLiteMonoLightweightCNN2023,
  title = {Lite-{{Mono}}: {{A Lightweight CNN}} and {{Transformer Architecture}} for {{Self-Supervised Monocular Depth Estimation}}},
  shorttitle = {Lite-{{Mono}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhang, Ning and Nex, Francesco and Vosselman, George and Kerle, Norman},
  year = 2023,
  pages = {18537--18546},
  urldate = {2025-07-03},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\EEVASCJ3\Zhang et al. - 2023 - Lite-Mono A Lightweight CNN and Transformer Architecture for Self-Supervised Monocular Depth Estima.pdf}
}

@article{zhangPerceptionSensingAutonomous2023,
  title = {Perception and Sensing for Autonomous Vehicles under Adverse Weather Conditions: {{A}} Survey},
  shorttitle = {Perception and Sensing for Autonomous Vehicles under Adverse Weather Conditions},
  author = {Zhang, Yuxiao and Carballo, Alexander and Yang, Hanting and Takeda, Kazuya},
  year = 2023,
  month = feb,
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {196},
  pages = {146--177},
  issn = {0924-2716},
  doi = {10.1016/j.isprsjprs.2022.12.021},
  urldate = {2025-01-09},
  abstract = {Automated Driving Systems (ADS) open up a new domain for the automotive industry and offer new possibilities for future transportation with higher efficiency and comfortable experiences. However, perception and sensing for autonomous driving under adverse weather conditions have been the problem that keeps autonomous vehicles (AVs) from going to higher autonomy for a long time. This paper assesses the influences and challenges that weather brings to ADS sensors in a systematic way, and surveys the solutions against inclement weather conditions. State-of-the-art algorithms and deep learning methods on perception enhancement with regard to each kind of weather, weather status classification, and remote sensing are thoroughly reported. Sensor fusion solutions, weather conditions coverage in currently available datasets, simulators, and experimental facilities are categorized. Additionally, potential ADS sensor candidates and developing research directions such as V2X (Vehicle to Everything) technologies are discussed. By looking into all kinds of major weather problems, and reviewing both sensor and computer science solutions in recent years, this survey points out the main moving trends of adverse weather problems in perception and sensing, i.e., advanced sensor fusion and more sophisticated machine learning techniques; and also the limitations brought by emerging 1550~nm LiDARs. In general, this work contributes a holistic overview of the obstacles and directions of perception and sensing research development in terms of adverse weather conditions.},
  keywords = {Adverse weather conditions,Autonomous driving,Deep learning,LiDAR,Perception and sensing,Sensor fusion},
  file = {C\:\\Users\\theun\\Zotero\\storage\\8XDC83FL\\Zhang et al. - 2023 - Perception and sensing for autonomous vehicles under adverse weather conditions A survey.pdf;C\:\\Users\\theun\\Zotero\\storage\\HFD8ZNT7\\S0924271622003367.html}
}

@inproceedings{zhangRetinexStructurebasedLowlight2024,
  title = {A {{Retinex Structure-based Low-light Enhancement Model Guided}} by {{Spatial Consistency}}},
  booktitle = {2024 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Zhang, Miao and Shen, Yiqing and Li, Zhuowei and Pan, Guofeng and Lu, Shuai},
  year = 2024,
  month = may,
  pages = {2154--2161},
  doi = {10.1109/ICRA57147.2024.10610021},
  urldate = {2025-02-27},
  abstract = {Images captured by robotics under low-light conditions are often plagued by several challenges, including diminished contrast, increased noise, loss of fine details, and unnatural color reproduction. These factors can significantly hinder the performance of computer vision tasks such as object detection and image segmentation. As a result, improving the quality of low-light images is of paramount importance for practical applications in the computer vision domain. To effectively address these challenges, we present a novel low-light image enhancement model, termed Spatial Consistency Retinex Network (SCRNet), which leverages the Retinex-based structure and is guided by the principle of spatial consistency. Specifically, our proposed model incorporates three levels of consistency: channel level, semantic level, and texture level, inspired by the principle of spatial consistency. These levels of consistency enable our model to adaptively enhance image features, ensuring more accurate and visually pleasing results. Extensive experimental evaluations on various low-light image datasets demonstrate that our proposed SCRNet outshines existing state-of-the-art methods, highlighting the potential of SCRNet as an effective solution for enhancing low-light images.},
  keywords = {Adaptation models,Computational modeling,Computer vision,Image color analysis,Image segmentation,Lighting,Semantics},
  file = {C\:\\Users\\theun\\Zotero\\storage\\UV7LJT6I\\Zhang et al. - 2024 - A Retinex Structure-based Low-light Enhancement Model Guided by Spatial Consistency.pdf;C\:\\Users\\theun\\Zotero\\storage\\DB9GBFLM\\10610021.html}
}

@article{zhangRobotNavigationSystem2023,
  title = {A {{Robot Navigation System}} in {{Complex Terrain Based}} on {{Statistical Features}} of {{Point Clouds}}},
  author = {Zhang, Yifei and Wang, Shiyuan},
  year = 2023,
  journal = {IEEE Transactions on Intelligent Vehicles},
  pages = {1--17},
  issn = {2379-8904},
  doi = {10.1109/TIV.2023.3297998},
  urldate = {2025-01-03},
  abstract = {The robot navigation system is mature in 2D flat terrain, however, in 3D complex terrain scenarios the computational burden increases dramatically because an additional dimension is added to the mapping and path planning problems. To this end, a hierarchical controlled robot navigation system in complex terrain (HCRNS-CT) is proposed for mapping and path planning by using a 2D grid map based on statistical features of the height of point clouds to replace the ordinary 3D map and implemented in a mobile platform in this paper. In HCRNS-CT, a 2D grid map that can reflect the curve of the ground is proposed from the viewpoint of statistical features. These point cloud features from the depth camera can be used to represent the traversability of each grid, even if the grid size is large, and thus can reduce the time consumption on mapping and path planning dramatically. And Kalman filter is used to update the global map with these features by local point clouds perceived by the depth camera. In addition, to match the introduced map, we propose a path planning algorithm fully utilizing those features and considering the mechanical structure of the robot. In HCRNS-CT, a hierarchical control structure is adopted as its backbone to reduce the frequency existing in the high computational burden module, and thus maintains the real-time standard of control. Therefore, HCRNS-CT can handle slopes and obstacles effectively in complex environments. A series of real-world experiments validate the robustness and effectiveness of HCRNS-CT.},
  keywords = {/unread,Cameras,control system and data fusion,mobile robotics,Navigation,navigation and control,Path planning,Point cloud compression,Robot kinematics,robot vision,Robot vision systems,Robotics,sensor fusion,Three-dimensional displays},
  file = {C\:\\Users\\theun\\Zotero\\storage\\96TCS9UY\\Zhang and Wang - 2023 - A Robot Navigation System in Complex Terrain Based on Statistical Features of Point Clouds.pdf;C\:\\Users\\theun\\Zotero\\storage\\PJV8FEBE\\10191025.html}
}

@inproceedings{zhangSemanticCombinedLightweight2024,
  title = {Semantic {{Combined Lightweight Transformer Low Light Image Enhancement Method}}},
  booktitle = {Proceedings of {{SPIE}} - {{The International Society}} for {{Optical Engineering}}},
  author = {Zhang, H. and Jin, H. and Li, Z.},
  year = 2024,
  volume = {13089},
  publisher = {SPIE},
  doi = {10.1117/12.3020944},
  abstract = {In this paper, we propose a lightweight approach to address the problems of noise amplification, detail loss, and edge blurring encountered in low-light image enhancement, which typically hinder subsequent computer vision tasks. Our proposed method utilizes transformers and depth-separated convolution, integrates the ISP image processing pipeline, and incorporates semantic information. First, we introduce an enhancement compensation extraction module that utilizes a transformer-based two-branch structure. Notably, depth-separated convolution replaces the original multi-attention transformer to achieve a lightweight design. The local branch estimates pixel-level illumination defects in low-light images, and the global branch enhances global structural information. Subsequently, we design a progressive enhancement module to receive enhancement compensation and reconstruct the enhanced image using the ISP pipeline. Together, these two modules form an enhancement network. Finally, we design a VGG16-based semantic segmentation module to preserve semantic information during the enhancement process and complete the image enhancement. Evaluations on benchmark datasets and extensive experiments with other algorithms demonstrate the effectiveness of our proposed low-light image enhancement method. The reconstructed enhanced images are improved in terms of brightness, contrast and detail sharpness, while effectively mitigating the problems associated with noise amplification and edge blurring. \copyright{} 2024 SPIE. All rights reserved.},
  keywords = {/unread}
}

@article{zhangSurveyEvaluationRGBD2021,
  title = {Survey and {{Evaluation}} of {{RGB-D SLAM}}},
  author = {Zhang, Shishun and Zheng, Longyu and Tao, Wenbing},
  year = 2021,
  journal = {IEEE Access},
  volume = {9},
  pages = {21367--21387},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3053188},
  urldate = {2025-03-19},
  abstract = {The traditional visual SLAM systems take the monocular or stereo camera as input sensor, with complex map initialization and map point triangulation steps needed for 3D map reconstruction, which are easy to fail, computationally complex and can cause noisy measurements. The emergence of RGB-D camera which provides RGB image together with depth information breaks this situation. While a number of RGB-D SLAM systems have been proposed in recent years, the current classification research on RGB-D SLAM is very lacking, and their advantages and shortcomings remain unclear regarding different applications and perturbations, such as illumination transformation, noise and rolling shutter effect of sensors. In this paper, we mainly introduced the basic concept and structure of the RGB-D SLAM system, and then introduced the differences between the various RGB-D SLAM systems in the three aspects of tracking, mapping, and loop detection, and we make a classification study on different RGB-D SLAM algorithms according to the three aspect. Furthermore, we discuss some advanced topics and open problems of RGB-D SLAM, hoping that it will help for future exploring. In the end, we conducted a large number of evaluation experiments on multiple RGB-D SLAM systems, and analyzed their advantages and disadvantages, as well as performance differences in different application scenarios, and provided references for researchers and developers.},
  keywords = {/unread,Cameras,Computer vision,evaluation,Image reconstruction,Mathematical model,RGB-D SLAM,Robot vision systems,robotics,Simultaneous localization and mapping,survey,Three-dimensional displays,Two dimensional displays},
  file = {C\:\\Users\\theun\\Zotero\\storage\\8VUUUN2A\\Zhang et al. - 2021 - Survey and Evaluation of RGB-D SLAM.pdf;C\:\\Users\\theun\\Zotero\\storage\\VQV6CFEE\\9330596.html}
}

@article{zhangThermalvisibleStereoMatching2024,
  title = {Thermal-Visible Stereo Matching at Night Based on {{Multi-Modal Autoencoder}}},
  author = {Zhang, Quan and Li, Yiran and Yang, Le and Zhang, Yi and Li, Zechao and Chen, Xiaoyu and Han, Jing},
  year = 2024,
  month = jan,
  journal = {Infrared Physics \& Technology},
  volume = {136},
  pages = {105010},
  issn = {1350-4495},
  doi = {10.1016/j.infrared.2023.105010},
  urldate = {2025-05-12},
  abstract = {Compared to RGB cameras, thermal sensors are more reliable at night. However, in the stereo matching task, it is difficult to estimate reliable depth simply depending on the binocular thermal cameras, due to their monochrome and low-contrast images. This paper proposed a thermal-visible stereo matching method based on multi-modal autoencoder (MANet), the key to which is to extract the modal-invariant features between thermal and visible images, and meanwhile, retain the characteristic features in terms of the optical properties. Besides, we designed a cross-reconstruction constraint to improve the scene feature extraction capability of MANet. In the stereo matching process, we analyze the thermal and visible properties and adopt optical attention mechanism to improve night-scene stereo matching. We established a stereo matching system with thermal-visible camera based on LiDAR calibration, and provided the dataset for evaluation. The experiment demonstrates that the proposed MANet can effectively extract the modal-invariant features and can be applied on thermal-visible stereo matching.},
  keywords = {3D imaging,Image processing,Multi-modal autoencoder,Night vision ADAS,Stereopsis,Thermal-visible},
  file = {C\:\\Users\\theun\\Zotero\\storage\\2BN9FAX7\\Zhang et al. - 2024 - Thermal-visible stereo matching at night based on Multi-Modal Autoencoder.pdf;C\:\\Users\\theun\\Zotero\\storage\\7J9G6ZA8\\S1350449523004681.html}
}

@inproceedings{zhangVIFBVisibleInfrared2020,
  title = {{{VIFB}}: {{A Visible}} and {{Infrared Image Fusion Benchmark}}},
  shorttitle = {{{VIFB}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Zhang, Xingchen and Ye, Ping and Xiao, Gang},
  year = 2020,
  month = jun,
  pages = {468--478},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPRW50498.2020.00060},
  urldate = {2025-05-15},
  abstract = {Visible and infrared image fusion is an important area in image processing due to its numerous applications. While much progress has been made in recent years with efforts on developing image fusion algorithms, there is a lack of code library and benchmark which can gauge the state-of-theart. In this paper, after briefly reviewing recent advances of visible and infrared image fusion, we present a visible and infrared image fusion benchmark (VIFB) which consists of 21 image pairs, a code library of 20 fusion algorithms and 13 evaluation metrics. We also carry out extensive experiments within the benchmark to understand the performance of these algorithms. By analyzing qualitative and quantitative results, we identify effective algorithms for robust image fusion and give some observations on the status and future prospects of this field.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-7281-9360-1},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\S394LT3N\Zhang et al. - 2020 - VIFB A Visible and Infrared Image Fusion Benchmark.pdf}
}

@incollection{zhangVisionBasedMethodUAV2025,
  title = {A {{Vision-Based Method}} for {{UAV Autonomous Landing Area Detection}}},
  booktitle = {Intelligence {{Science V}}},
  author = {Zhang, Qiutong and Xia, Qingyuan and Wei, Lisheng and Deng, Bohai},
  editor = {Shi, Zhongzhi and Witbrock, Michael and Tian, Qi},
  year = 2025,
  volume = {720},
  pages = {204--213},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-71253-1_15},
  urldate = {2024-12-04},
  abstract = {Automatic identification of the landing area is crucial for UAV (Unmanned Aerial Vehicles) to land correctly and safely. Using passive vision sensors to achieve this objective is a very promising avenue due to their low cost and the potential they provide for performing simultaneous terrain analysis. In this paper, a computer vision method is proposed using an improved U-Net based architecture on UAV imagery to assess the safe landing area. Contrary to past methods, which little attention has been paid to the whole landing process with multiple descending altitude, experiment involves evaluating the landing area by analyzing visual images obtained from different descending heights. In the initial stage of landing, separation was made between water and land, guiding the flight to the land. As descending, classification was applied on images captured near ground into safe/unsafe landing areas and then mapped to the safety score for selection. Experiments on public datasets have shown promising results.},
  isbn = {978-3-031-71252-4 978-3-031-71253-1},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\theun\Zotero\storage\7QF9JC7J\Zhang et al. - 2025 - A Vision-Based Method for UAV Autonomous Landing Area Detection.pdf}
}

@article{zhaoDeepLearningLowLight2025,
  title = {Deep {{Learning}} for {{Low-Light Vision}}: {{A Comprehensive Survey}}},
  shorttitle = {Deep {{Learning}} for {{Low-Light Vision}}},
  author = {Zhao, Qian and Li, Gang and He, Bin and Shen, Runjie},
  year = 2025,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--21},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2025.3566647},
  urldate = {2025-06-18},
  abstract = {Visual recognition in low-light environments is a challenging problem since degraded images are the stacking of multiple degradations (noise, low light and blur, etc.). It has received extensive attention from academia and industry in the era of deep learning. Existing surveys focus on low-light image enhancement (LLIE) methods and normal-light visual recognition methods, while few comprehensive surveys of low-light-related vision tasks. This article provides a comprehensive survey of the latest advancements in low-light vision, including methods, datasets, and evaluation metrics, in two aspects: visual quality-driven and recognition quality-driven. On the visual quality-driven aspect, we survey a large number of very recent LLIE methods. On the recognition quality-driven aspect, we survey low-light object detection techniques in the deep learning era using more intuitive categorization method. Furthermore, a quantitative benchmarking of different methods is conducted on several widely adopted low-light vision-related datasets. Finally, we discuss the challenges that exist in low-light vision and future directions worth exploring. We provide a public website that will continue to track developments in this promising field.},
  keywords = {Deep learning,Degradation,image enhancement,Image enhancement,Image recognition,Image segmentation,Lighting,low-light vision,object detection,Object detection,Reviews,Surveys,Taxonomy,Visualization},
  file = {C:\Users\theun\Zotero\storage\ECEPD9WA\Zhao et al. - 2025 - Deep Learning for Low-Light Vision A Comprehensive Survey.pdf}
}

@article{zhaoLowlightStereoImage2025,
  title = {Low-Light Stereo Image Enhancement and de-Noising in the Low-Frequency Information Enhanced Image Space},
  author = {Zhao, Minghua and Qin, Xiangdong and Du, Shuangli and Bai, Xuefei and Lyu, Jiahao and Liu, Yiguang},
  year = 2025,
  month = mar,
  journal = {Expert Systems with Applications},
  volume = {265},
  pages = {125803},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2024.125803},
  urldate = {2025-03-24},
  abstract = {Unlike single image task, stereo image enhancement can use another view information, and its key stage is how to perform cross-view feature interaction to extract useful information from another view. However, complex noise in low-light image and its impact on subsequent feature encoding and interaction are ignored by the existing methods. In this paper, a method is proposed to perform enhancement and de-noising simultaneously. First, to reduce unwanted noise interference, a low-frequency information enhanced module (IEM) is proposed to suppress noise and produce a new image space. Additionally, a cross-channel and spatial context information mining module (CSM) is proposed to encode long-range spatial dependencies and to enhance inter-channel feature interaction. Relying on CSM, an encoder--decoder structure is constructed, incorporating cross-view and cross-scale feature interactions to perform enhancement in the new image space. Finally, the network is trained with the constraints of both spatial and frequency domain losses. Extensive experiments on both synthesized and real datasets show that our method obtains better detail recovery and noise removal compared with state-of-the-art methods. In addition, a real stereo image enhancement dataset is captured with stereo camera ZED2. The code and dataset are publicly available at: https://www.github.com/noportraits/LFENet.},
  keywords = {Channel coding,Context information,Cross-channel and spatial context information mining,Cross-view feature interaction,Encoding (symbols),Feature interactions,Frequency information,Image denoising,Image enhancement,Information mining,Low-frequency information enhance,Lower frequencies,Spatial context,Stereo image enhancement,Stereo image processing,Stereoimages},
  file = {C\:\\Users\\theun\\Zotero\\storage\\QH3DAXLH\\Zhao et al. - 2025 - Low-light stereo image enhancement and de-noising in the low-frequency information enhanced image sp.pdf;C\:\\Users\\theun\\Zotero\\storage\\YXTYCCW7\\S0957417424026708.html}
}

@inproceedings{zhaoMonoViTSelfSupervisedMonocular2022,
  title = {{{MonoViT}}: {{Self-Supervised Monocular Depth Estimation}} with a {{Vision Transformer}}},
  shorttitle = {{{MonoViT}}},
  booktitle = {2022 {{International Conference}} on {{3D Vision}} ({{3DV}})},
  author = {Zhao, Chaoqiang and Zhang, Youmin and Poggi, Matteo and Tosi, Fabio and Guo, Xianda and Zhu, Zheng and Huang, Guan and Tang, Yang and Mattoccia, Stefano},
  year = 2022,
  month = sep,
  pages = {668--678},
  issn = {2475-7888},
  doi = {10.1109/3DV57658.2022.00077},
  urldate = {2025-07-04},
  abstract = {Self-supervised monocular depth estimation is an attractive solution that does not require hard-to-source depth la-bels for training. Convolutional neural networks (CNNs) have recently achieved great success in this task. However, their limited receptive field constrains existing network architectures to reason only locally, dampening the effectiveness of the self-supervised paradigm. In the light of the recent successes achieved by Vision Transformers (ViTs), we propose MonoViT, a brand-new framework combining the global reasoning enabled by ViT models with the flexibility of self-supervised monocular depth estimation. By combining plain convolutions with Transformer blocks, our model can reason locally and globally, yielding depth prediction at a higher level of detail and accuracy, allowing MonoViT to achieve state-of-the-art performance on the established KITTI dataset. Moreover, MonoViT proves its superior generalization capacities on other datasets such as Make3D and DrivingStereo. Source code available at https://github.com/zxcqlf/MonoViT},
  keywords = {Convolutional codes,Estimation,Network architecture,Predictive models,Source coding,Three-dimensional displays,Training},
  file = {C:\Users\theun\Zotero\storage\TRLCXDR5\Zhao et al. - 2022 - MonoViT Self-Supervised Monocular Depth Estimation with a Vision Transformer.pdf}
}

@inproceedings{zhaoPyramidSceneParsing2017,
  title = {Pyramid {{Scene Parsing Network}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
  year = 2017,
  month = jul,
  pages = {6230--6239},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2017.660},
  urldate = {2025-06-03},
  abstract = {Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4\% on PASCAL VOC 2012 and accuracy 80.2\% on Cityscapes.},
  keywords = {Automobiles,Convolution,Feature extraction,Image segmentation,Neural networks,Semantics},
  file = {C:\Users\theun\Zotero\storage\2JIPUQV4\Zhao et al. - 2017 - Pyramid Scene Parsing Network.pdf}
}

@article{zhaoRetinexDIPUnifiedDeep2022,
  title = {{{RetinexDIP}}: {{A Unified Deep Framework}} for {{Low-Light Image Enhancement}}},
  shorttitle = {{{RetinexDIP}}},
  author = {Zhao, Zunjin and Xiong, Bangshu and Wang, Lei and Ou, Qiaofeng and Yu, Lei and Kuang, Fa},
  year = 2022,
  month = mar,
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {32},
  number = {3},
  pages = {1076--1088},
  issn = {1558-2205},
  doi = {10.1109/TCSVT.2021.3073371},
  urldate = {2025-06-20},
  abstract = {Low-light images suffer from low contrast and unclear details, which not only reduces the available information for humans but limits the application of computer vision algorithms. Among the existing enhancement techniques, Retinex-based and learning-based methods are under the spotlight today. In this paper, we bridge the gap between the two methods. First, we propose a novel ``generative'' strategy for Retinex decomposition, by which the decomposition is cast as a generative problem. Second, based on the strategy, a unified deep framework is proposed to estimate the latent components and perform low-light image enhancement. Third, our method can weaken the coupling relationship between the two components while performing Retinex decomposition. Finally, the RetinexDIP performs Retinex decomposition without any external images, and the estimated illumination can be easily adjusted and is used to perform enhancement. The proposed method is compared with ten state-of-the-art algorithms on seven public datasets, and the experimental results demonstrate the superiority of our method. Code is available at: https://github.com/zhaozunjin/RetinexDIP.},
  keywords = {/unread,Cameras,Couplings,deep prior,Electronics packaging,Histograms,Image enhancement,Lighting,Low-light image enhancement,retinex decomposition,Task analysis,zero-reference},
  file = {C:\Users\theun\Zotero\storage\Q9VMN7Y3\Zhao et al. - 2022 - RetinexDIP A Unified Deep Framework for Low-Light Image Enhancement.pdf}
}

@article{zhaoUnsupervisedMonocularDepth2022,
  title = {Unsupervised {{Monocular Depth Estimation}} in {{Highly Complex Environments}}},
  author = {Zhao, C. and Tang, Y. and Sun, Q.},
  year = 2022,
  journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
  volume = {6},
  number = {5},
  pages = {1237--1246},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {2471285X (ISSN)},
  doi = {10.1109/TETCI.2022.3182360},
  abstract = {With the development of computational intelligence algorithms, unsupervised monocular depth and pose estimation framework, which is driven by warped photometric consistency, has shown great performance in the day-time scenario. While in some challenging environments, like night and rainy night, the essential photometric consistency hypothesis is untenable because of the complex lighting and reflection, so that the above unsupervised framework cannot be directly applied to these complex scenarios. In this paper, we investigate the problem of unsupervised monocular depth estimation in highly complex scenarios and address this challenging problem by adopting an image transfer-based domain adaptation framework. We adapt the depth model trained on day-time scenarios to be applicable to night-time scenarios, and constraints on both feature space and output space promote the framework to learn the key features for depth decoding. Meanwhile, we further tackle the effects of unstable image transfer quality on domain adaptation, and an image adaptation approach is proposed to evaluate the quality of transferred images and re-weight the corresponding losses, so as to improve the performance of the adapted depth model. Extensive experiments show the effectiveness of the proposed unsupervised framework in estimating the dense depth map from highly complex images. \copyright{} 2017 IEEE.},
  langid = {english},
  keywords = {Adaptation models,Computational intelligence,Decoding,domain adaptation,Estimation,Lighting,monocular depth estimation,night,rainy night,Sun,Training,Unsupervised estimation},
  file = {C:\Users\theun\Zotero\storage\NF9FGCY7\Zhao et al. - 2022 - Unsupervised Monocular Depth Estimation in Highly Complex Environments.pdf}
}

@inproceedings{zhengDecoupledCrossScaleCrossView2023,
  title = {Decoupled {{Cross-Scale Cross-View Interaction}} for {{Stereo Image Enhancement}} in the {{Dark}}},
  booktitle = {Proceedings of the 31st {{ACM International Conference}} on {{Multimedia}}},
  author = {Zheng, Huan and Zhang, Zhao and Fan, Jicong and Hong, Richang and Yang, Yi and Yan, Shuicheng},
  year = 2023,
  month = oct,
  pages = {1475--1484},
  publisher = {ACM},
  address = {Ottawa ON Canada},
  doi = {10.1145/3581783.3611962},
  urldate = {2025-03-28},
  isbn = {979-8-4007-0108-5},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\Q2MZLV83\Zheng et al. - 2023 - Decoupled Cross-Scale Cross-View Interaction for Stereo Image Enhancement in the Dark.pdf}
}

@article{zhengMultivehicleMultisensorOccupancy2022,
  title = {Multivehicle {{Multisensor Occupancy Grid Maps}} ({{MVMS-OGM}}) for {{Autonomous Driving}}},
  author = {Zheng, Xinhu and Li, Yuru and Duan, Dongliang and Yang, Liuqing and Chen, Chen and Cheng, Xiang},
  year = 2022,
  month = nov,
  journal = {IEEE Internet of Things Journal},
  volume = {9},
  number = {22},
  pages = {22944--22957},
  issn = {2327-4662},
  doi = {10.1109/JIOT.2022.3187827},
  urldate = {2025-01-02},
  abstract = {In autonomous driving, environment perception is the fundamental task for intelligent vehicles which provides the necessary environment information for other applications. The main issues in existing environment perception can be categorized into two aspects. On the one hand, all sensors are prone to measurement errors and failures. On the other hand, in complex driving environments, vehicles may encounter a variety of blind spots caused by vehicle occlusions, overlaps, and harsh weather conditions, which will cause sensors to experience low-quality data or to miss crucial environmental information. To cope with these issues, a multivehicle and multisensor (MVMS) cooperative perception method is presented to construct the occupancy grid map (OGM) of vehicles in a global view for the environment perception of autonomous driving. Distinct from existing environment perception methods, our proposed MVMS-OGM not only provides continuous geographical information but also captures and fuses continuous information with soft occupancy probabilities, resulting in more comprehensive and raw environmental information. Simulations and real-world experiments demonstrate that the proposed approach not only expands the perception range in comparison with single-vehicle sensing but also better captures the uncertainty of sensor data by fusing the occupancy probabilities with soft information.},
  keywords = {/unread,Autonomous driving,Autonomous vehicles,Cameras,cooperative sensing,Data integration,Laser radar,multisensor data fusion,occupancy grid map (OGM) fusion,occupancy probability assignment,Sensor fusion,Sensors,Task analysis},
  file = {C\:\\Users\\theun\\Zotero\\storage\\G4MG77GK\\Zheng et al. - 2022 - Multivehicle Multisensor Occupancy Grid Maps (MVMS-OGM) for Autonomous Driving.pdf;C\:\\Users\\theun\\Zotero\\storage\\UKT2Y2KL\\9813502.html}
}

@inproceedings{zhengSemanticGuidedZeroShotLearning2022,
  title = {Semantic-{{Guided Zero-Shot Learning}} for {{Low-Light Image}}/{{Video Enhancement}}},
  booktitle = {2022 {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision Workshops}} ({{WACVW}})},
  author = {Zheng, Shen and Gupta, Gaurav},
  year = 2022,
  month = jan,
  pages = {581--590},
  publisher = {IEEE},
  address = {Waikoloa, HI, USA},
  doi = {10.1109/WACVW54805.2022.00064},
  urldate = {2025-02-27},
  abstract = {Low-light images challenge both human perceptions and computer vision algorithms. It is crucial to make algorithms robust to enlighten low-light images for computational photography and computer vision applications such as realtime detection and segmentation. This paper proposes a semantic-guided zero-shot low-light enhancement network (SGZ) which is trained in the absence of paired images, unpaired datasets, and segmentation annotation. Firstly, we design an enhancement factor extraction network using depthwise separable convolution for an efficient estimate of the pixel-wise light deficiency of an low-light image. Secondly, we propose a recurrent image enhancement network to progressively enhance the low-light image with affordable model size. Finally, we introduce an unsupervised semantic segmentation network for preserving the semantic information during intensive enhancement. Extensive experiments on benchmark datasets and a low-light video demonstrate that our model outperforms the previous stateof-the-art. We further discuss the benefits of the proposed method for low-light detection and segmentation. Code is available at https://github.com/ShenZheng2000/SemanticGuided-Low-Light-Image-Enhancement.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-5824-5},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\N4RMYKUI\Zheng and Gupta - 2022 - Semantic-Guided Zero-Shot Learning for Low-Light ImageVideo Enhancement.pdf}
}

@misc{zhengSTEPSJointSelfsupervised2023,
  title = {{{STEPS}}: {{Joint Self-supervised Nighttime Image Enhancement}} and {{Depth Estimation}}},
  shorttitle = {{{STEPS}}},
  author = {Zheng, Yupeng and Zhong, Chengliang and Li, Pengfei and Gao, Huan-ang and Zheng, Yuhang and Jin, Bu and Wang, Ling and Zhao, Hao and Zhou, Guyue and Zhang, Qichao and Zhao, Dongbin},
  year = 2023,
  month = feb,
  number = {arXiv:2302.01334},
  eprint = {2302.01334},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.01334},
  urldate = {2025-07-03},
  abstract = {Self-supervised depth estimation draws a lot of attention recently as it can promote the 3D sensing capabilities of self-driving vehicles. However, it intrinsically relies upon the photometric consistency assumption, which hardly holds during nighttime. Although various supervised nighttime image enhancement methods have been proposed, their generalization performance in challenging driving scenarios is not satisfactory. To this end, we propose the first method that jointly learns a nighttime image enhancer and a depth estimator, without using ground truth for either task. Our method tightly entangles two self-supervised tasks using a newly proposed uncertain pixel masking strategy. This strategy originates from the observation that nighttime images not only suffer from underexposed regions but also from overexposed regions. By fitting a bridge-shaped curve to the illumination map distribution, both regions are suppressed and two tasks are bridged naturally. We benchmark the method on two established datasets: nuScenes and RobotCar and demonstrate state-of-the-art performance on both of them. Detailed ablations also reveal the mechanism of our proposal. Last but not least, to mitigate the problem of sparse ground truth of existing datasets, we provide a new photo-realistically enhanced nighttime dataset based upon CARLA. It brings meaningful new challenges to the community. Codes, data, and models are available at https://github.com/ucaszyp/STEPS.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\theun\\Zotero\\storage\\LDNYBSRW\\Zheng et al. - 2023 - STEPS Joint Self-supervised Nighttime Image Enhancement and Depth Estimation.pdf;C\:\\Users\\theun\\Zotero\\storage\\MP7KSQUK\\2302.html}
}

@article{zhouDarkLightEnhancement2023,
  title = {Dark Light Enhancement for Dark Scene Urban Object Recognition},
  author = {Zhou, Xiao and Du, Xiaobiao and Ru, Peizhe},
  year = 2023,
  journal = {IET Image Processing},
  volume = {17},
  number = {7},
  pages = {2043--2055},
  issn = {1751-9667},
  doi = {10.1049/ipr2.12771},
  urldate = {2025-06-18},
  abstract = {This paper presents a low-light image enhancement method to improve the performance of autonomous piloting tasks based on deep learning methods. In the low light environment, camera sensors cannot capture enough effective photon signals causing poor performance in vision-based autonomous piloting. Moreover, the lack of training data makes single-frame low-light enhancement and denoising algorithms hard to generalize in real-world scenarios. By analyzing the noise patterns of real-world cameras under low-light environments, a noise generation method is proposed to mimic real-world dark-light noise and generate noisy-clean data pairs for training. A method that can calibrate the camera shot noise parameters is then designed to learn low-light enhancement from the synthesized noisy data pairs. The neural network trained on a synthesized dataset can effectively enhance the dark light image quality. Consequently, the network improves the downstream auto-piloting applications such as object detection and semantic segmentation. Qualitative and quantitative experiments have been conducted to demonstrate that the method outperforms previous methods in low-light enhancement.},
  copyright = {\copyright{} 2023 The Authors. IET Image Processing published by John Wiley \& Sons Ltd on behalf of The Institution of Engineering and Technology.},
  langid = {english},
  keywords = {Cameras,dark light image enhancement,Dark light image enhancement,Dark scenes,Data pairs,Deep learning,Image enhancement,Image reconstruction,image restoration,Learning systems,Light enhancement,Light environment,Low light,Object detection,object recognition,Object recognition,Objects recognition,Real-world,Semantic Segmentation,Semantics,Shot noise,Synthesised,Urban objects,Zero-shot learning},
  file = {C:\Users\theun\Zotero\storage\L3KC2YFZ\Zhou et al. - 2023 - Dark light enhancement for dark scene urban object recognition.pdf}
}

@article{zhouHLGNetHighLightGuided2025,
  title = {{{HLGNet}}: {{High-Light Guided Network}} for Low-Light Instance Segmentation with Spatial-Frequency Domain Enhancement},
  shorttitle = {{{HLGNet}}},
  author = {Zhou, Huaping and Wu, Tao and Sun, Kelei and Wu, Jin and Deng, Bin and Zhang, Xueseng},
  year = 2025,
  month = oct,
  journal = {Neural Networks},
  volume = {190},
  pages = {107613},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2025.107613},
  urldate = {2025-06-15},
  abstract = {Instance segmentation models generally perform well under typical lighting conditions but struggle in low-light environments due to insufficient fine-grained detail. To address this, frequency domain enhancement has shown promise. However, the lack of spatial domain processing in existing frequency domain based methods often results in poor boundary delineation and inadequate local perception. To address these challenges, we propose HLGNet (High-Light Guided Network). By leveraging high-light image masks, our approach integrates enhancements in both the frequency and spatial domains, thereby improving the feature representation of low-light images. Specifically, we propose the SPE (Spatial-Frequency Enhancement) Block, which effectively combines and complements local spatial features with global frequency domain information. Additionally, we design the DAF (Dynamic Affine Fusion) module to inject frequency domain information into semantically significant features, thereby enhancing the model's ability to capture both detailed target information and global semantic context. Finally, we propose the HLG Decoder, which dynamically adjusts the attention distribution by utilizing mutual information and entropy, guided by high-light image masks. This ensures improved focus on both local details and global semantics. Extensive quantitative and qualitative evaluations on two widely used low-light instance segmentation datasets demonstrate that HLGNet outperforms current state-of-the-art methods.},
  keywords = {Affine transformations,Affine transforms,Dynamic affine transformation,Frequency domain analysis,Frequency domains,Frequency-spatial information,High lights,High-light image guided,Image enhancement,Image-guided,Instance segmentation,Mutual information,Mutual informations,Photointerpretation,Semantic Segmentation,Spatial informations},
  file = {C\:\\Users\\theun\\Zotero\\storage\\RN69HBNL\\Zhou et al. - 2025 - HLGNet High-Light Guided Network for low-light instance segmentation with spatial-frequency domain.pdf;C\:\\Users\\theun\\Zotero\\storage\\LKD3PME8\\S0893608025004939.html}
}

@inproceedings{zhouLEDNetJointLowLight2022,
  title = {{{LEDNet}}: {{Joint Low-Light Enhancement}} and~{{Deblurring}} in~the~{{Dark}}},
  shorttitle = {{{LEDNet}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2022},
  author = {Zhou, Shangchen and Li, Chongyi and Change Loy, Chen},
  editor = {Avidan, Shai and Brostow, Gabriel and Ciss{\'e}, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  year = 2022,
  pages = {573--589},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-20068-7_33},
  abstract = {Night photography typically suffers from both low light and blurring issues due to the dim environment and the common use of long exposure. While existing light enhancement and deblurring methods could deal with each problem individually, a cascade of such methods cannot work harmoniously to cope well with joint degradation of visibility and sharpness. Training an end-to-end network is also infeasible as no paired data is available to characterize the coexistence of low light and blurs. We address the problem by introducing a novel data synthesis pipeline that models realistic low-light blurring degradations, especially for blurs in saturated regions, e.g., light streaks, that often appear in the night images. With the pipeline, we present the first large-scale dataset for joint low-light enhancement and deblurring. The dataset, LOL-Blur, contains 12,000 low-blur/normal-sharp pairs with diverse darkness and blurs in different scenarios. We further present an effective network, named LEDNet, to perform joint low-light enhancement and deblurring. Our network is unique as it is specially designed to consider the synergy between the two inter-connected tasks. Both the proposed dataset and network provide a foundation for this challenging joint task. Extensive experiments demonstrate the effectiveness of our method on both synthetic and real-world datasets.},
  isbn = {978-3-031-20068-7},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\IV6MP8RT\Zhou et al. - 2022 - LEDNet Joint Low-Light Enhancement and Deblurring in the Dark.pdf}
}

@article{zhouRealtimeLowlightVideo2024,
  title = {Real-Time Low-Light Video Enhancement on Smartphones},
  author = {Zhou, Yiming and MacPhee, Callen and Gunawan, Wesley and Farahani, Ali and Jalali, Bahram},
  year = 2024,
  month = oct,
  journal = {Journal of Real-Time Image Processing},
  volume = {21},
  number = {5},
  pages = {155},
  issn = {1861-8200, 1861-8219},
  doi = {10.1007/s11554-024-01532-7},
  urldate = {2024-12-04},
  abstract = {Real-time low-light video enhancement on smartphones remains an open challenge due to hardware constraints such as limited sensor size and processing power. While night mode cameras have been introduced in smartphones to acquire highquality images in light-constrained environments, their usability is restricted to static scenes as the camera must remain stationary for an extended period to leverage long exposure times or burst imaging techniques. Concurrently, significant process has been made in low-light enhancement on images coming out from the camera's image signal processor (ISP), particularly through neural networks. These methods do not improve the image capture process itself; instead, they function as post-processing techniques to enhance the perceptual brightness and quality of captured imagery for display to human viewers. However, most neural networks are computationally intensive, making their mobile deployment either impractical or requiring considerable engineering efforts. This paper introduces VLight, a novel single-parameter low-light enhancement algorithm that enables real-time video enhancement on smartphones, along with real-time adaptation to changing lighting conditions and user-friendly fine-tuning. Operating as a custom brightness-booster on digital images, VLight provides realtime and device-agnostic enhancement directly on users' devices. Notably, it delivers real-time low-light enhancement at up to 67 frames per second (FPS) for 4K videos locally on the smartphone.},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\T5KDTP6I\Zhou et al. - 2024 - Real-time low-light video enhancement on smartphones.pdf}
}

@article{zhouRobustMonocularDepth2020,
  title = {A {{Robust Monocular Depth Estimation Framework Based}} on {{Light-Weight ERF-Pspnet}} for {{Day-Night Driving Scenes}}},
  author = {Zhou, Keyang and Wang, Kaiwei and Yang, Kailun},
  year = 2020,
  month = apr,
  journal = {Journal of Physics: Conference Series},
  volume = {1518},
  number = {1},
  pages = {012051},
  publisher = {IOP Publishing},
  issn = {1742-6596},
  doi = {10.1088/1742-6596/1518/1/012051},
  urldate = {2025-06-30},
  abstract = {With the development of deep learning, various fields of computer vision have made huge progress. Among them, depth estimation is an important part of scene perception, therefore receives much interest and is widely used in daily life with the assistance of GPUs. Besides, the ways to obtain depth maps have also been improved, from using multiple images to a single image to obtain depth, which is called monocular depth estimation task. In this paper, we design a convolutional neural network called ERF-PSPNet to perform the task. We prove that by using unsupervised training, monocular depth estimation's result learned from large-scale dataset is close to the result of stereo matching. We also show that the monocular depth estimation model proposed in this paper can achieve a satisfying precision while maintaining a certain real-time frame rate for day-night driving scenes, which confirms the practical applicability of our design and result.},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\TJUGANBS\Zhou et al. - 2020 - A Robust Monocular Depth Estimation Framework Based on Light-Weight ERF-Pspnet for Day-Night Driving.pdf}
}

@article{zhouSemanticUnderstandingScenes2019,
  title = {Semantic {{Understanding}} of {{Scenes Through}} the {{ADE20K Dataset}}},
  author = {Zhou, Bolei and Zhao, Hang and Puig, Xavier and Xiao, Tete and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},
  year = 2019,
  month = mar,
  journal = {International Journal of Computer Vision},
  volume = {127},
  number = {3},
  pages = {302--321},
  publisher = {Springer US},
  issn = {1573-1405},
  doi = {10.1007/s11263-018-1140-0},
  urldate = {2025-06-05},
  abstract = {Semantic understanding of visual scenes is one of the holy grails of computer vision. Despite efforts of the community in data collection, there are still few image datasets covering a wide range of scenes and object categories with pixel-wise annotations for scene understanding. In this work, we present a densely annotated dataset ADE20K, which spans diverse annotations of scenes, objects, parts of objects, and in some cases even parts of parts. Totally there are 25k images of the complex everyday scenes containing a variety of objects in their natural spatial context. On average there are 19.5 instances and 10.5 object classes per image. Based on ADE20K, we construct benchmarks for scene parsing and instance segmentation. We provide baseline performances on both of the benchmarks and re-implement state-of-the-art models for open source. We further evaluate the effect of synchronized batch normalization and find that a reasonably large batch size is crucial for the semantic segmentation performance. We show that the networks trained on ADE20K are able to segment a wide variety of scenes and objects.},
  copyright = {2018 Springer Science+Business Media, LLC, part of Springer Nature},
  langid = {english},
  file = {C:\Users\theun\Zotero\storage\R63SBY7D\Zhou et al. - 2019 - Semantic Understanding of Scenes Through the ADE20K Dataset.pdf}
}

@inproceedings{zhouUnsupervisedLearningDepth2017,
  title = {Unsupervised {{Learning}} of {{Depth}} and {{Ego-Motion}} from {{Video}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhou, Tinghui and Brown, Matthew and Snavely, Noah and Lowe, David G.},
  year = 2017,
  month = jul,
  pages = {6612--6619},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2017.700},
  urldate = {2025-06-30},
  abstract = {We present an unsupervised learning framework for the task of monocular depth and camera motion estimation from unstructured video sequences. In common with recent work [10, 14, 16], we use an end-to-end learning approach with view synthesis as the supervisory signal. In contrast to the previous work, our method is completely unsupervised, requiring only monocular video sequences for training. Our method uses single-view depth and multiview pose networks, with a loss based on warping nearby views to the target using the computed depth and pose. The networks are thus coupled by the loss during training, but can be applied independently at test time. Empirical evaluation on the KITTI dataset demonstrates the effectiveness of our approach: 1) monocular depth performs comparably with supervised methods that use either ground-truth pose or depth for training, and 2) pose estimation performs favorably compared to established SLAM systems under comparable input settings.},
  keywords = {Cameras,Geometry,Pipelines,Pose estimation,Three-dimensional displays,Training},
  file = {C:\Users\theun\Zotero\storage\3K97ZQJU\Zhou et al. - 2017 - Unsupervised Learning of Depth and Ego-Motion from Video.pdf}
}

@article{zhouUsingAreaWeightedLoss2025,
  title = {Using an {{Area-Weighted Loss Function}} to {{Address Class Imbalance}} in {{Deep Learning-Based Mapping}} of {{Small Water Bodies}} in a {{Low-Latitude Region}}},
  author = {Zhou, Pu and Foody, Giles and Zhang, Yihang and Wang, Yalan and Wang, Xia and Li, Sisi and Shen, Laiyin and Du, Yun and Li, Xiaodong},
  year = 2025,
  month = jan,
  journal = {Remote Sensing},
  volume = {17},
  number = {11},
  pages = {1868},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2072-4292},
  doi = {10.3390/rs17111868},
  urldate = {2025-10-17},
  abstract = {Recent advances in very high resolution PlanetScope imagery and deep-learning techniques have enabled effective mapping of small water bodies (SWBs), including ponds and ditches. SWBs typically occupy a minor proportion of remote-sensing imagery. This creates significant class imbalance that introduces bias in trained models. Most existing deep-learning approaches fail to adequately address this imbalance. Such an imbalance introduces bias in trained models. Most existing deep-learning approaches fail to adequately address the inter-class (water vs. non-water) and intra-class (SWBs vs. large water bodies) simultaneously. Consequently, they show poor detection of SWBs. To address these challenges, we propose an area-based weighted binary cross-entropy (AWBCE) loss function. AWBCE dynamically weights water bodies according to their size during model training. We evaluated our approach through large-scale SWB mapping in the middle and east of Hubei Province, China. The models were trained on 14,509 manually annotated PlanetScope image patches (512 \texttimes{} 512 pixels each). We implemented the AWBCE loss function in State-of-the-Art segmentation models (UNet, DeepLabV3+, HRNet, LANet, UNetFormer, and LETNet) and evaluated them using overall accuracy, F1-score, intersection over union, and Matthews correlation coefficient as accuracy metrics. The AWBCE loss function consistently improved performance, achieving better boundary accuracy and higher scores across all metrics. Quantitative and visual comparisons demonstrated AWBCE's superiority over other imbalance-focused loss functions (weighted BCE, Dice, and Focal losses). These findings emphasize the importance of specialized approaches for comprehensive SWB mapping using high-resolution PlanetScope imagery in low-latitude regions.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {/unread,area-based weighted binary cross-entropy (AWBCE),class imbalance problem,deep learning,loss function,PlanetScope,small water bodies},
  file = {C:\Users\theun\Zotero\storage\QNZ2UCR5\Zhou et al. - 2025 - Using an Area-Weighted Loss Function to Address Class Imbalance in Deep Learning-Based Mapping of Sm.pdf}
}

@misc{zhuRePLAyRemoveProjective2024,
  title = {{{RePLAy}}: {{Remove Projective LiDAR Depthmap Artifacts}} via {{Exploiting Epipolar Geometry}}},
  shorttitle = {{{RePLAy}}},
  author = {Zhu, Shengjie and Ganesan, Girish Chandar and Kumar, Abhinav and Liu, Xiaoming},
  year = 2024,
  month = jul,
  number = {arXiv:2407.19154},
  eprint = {2407.19154},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.19154},
  urldate = {2024-12-20},
  abstract = {3D sensing is a fundamental task for Autonomous Vehicles. Its deployment often relies on aligned RGB cameras and LiDAR. Despite meticulous synchronization and calibration, systematic misalignment persists in LiDAR projected depthmap. This is due to the physical baseline distance between the two sensors. The artifact is often reflected as background LiDAR incorrectly projected onto the foreground, such as cars and pedestrians. The KITTI dataset uses stereo cameras as a heuristic solution to remove artifacts. However most AV datasets, including nuScenes, Waymo, and DDAD, lack stereo images, making the KITTI solution inapplicable. We propose RePLAy, a parameter-free analytical solution to remove the projective artifacts. We construct a binocular vision system between a hypothesized virtual LiDAR camera and the RGB camera. We then remove the projective artifacts by determining the epipolar occlusion with the proposed analytical solution. We show unanimous improvement in the State-of-The-Art (SoTA) monocular depth estimators and 3D object detectors with the artifacts-free depthmaps.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {/unread,Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\theun\Zotero\storage\7D7GCHCM\Zhu et al. - 2024 - RePLAy Remove Projective LiDAR Depthmap Artifacts via Exploiting Epipolar Geometry.pdf}
}

@inproceedings{zhuUnpairedImageToImageTranslation2017,
  title = {Unpaired {{Image-To-Image Translation Using Cycle-Consistent Adversarial Networks}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  year = 2017,
  pages = {2223--2232},
  urldate = {2025-05-13},
  file = {C:\Users\theun\Zotero\storage\ZI8WMBIB\Zhu et al. - 2017 - Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks.pdf}
}

@article{zuiderveldContrastLimitedAdaptive1994,
  title = {Contrast Limited Adaptive Histogram Equalization.},
  author = {Zuiderveld, Karel J.},
  year = 1994,
  journal = {Graphics gems},
  volume = {4},
  number = {1},
  pages = {474--485},
  publisher = {Academic Press, Boston, MA, USA},
  urldate = {2025-05-27}
}
