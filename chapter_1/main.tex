%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/


\documentclass[journal]{IEEEtran}

\usepackage{hyperref}
\bibliographystyle{IEEEtran}

\usepackage{cite}
\usepackage{color}


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  \graphicspath{{C:/Users/Samuel Gan/OneDrive - Deakin University/PhD/Chapters/method-1/Paper/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Insert my\\ fancy title here}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Samuel Gan,~\IEEEmembership{Member,~IEEE,}
    John~Doe,~\IEEEmembership{Fellow,~OSA,}
    and~Jane~Doe,~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space
    \thanks{Insert thanks in the footnote for whoever about the .}% <-this % stops a space
    \thanks{J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
    \thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
    The abstract goes here.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
    IEEE, IEEEtran, journal, \LaTeX, paper, template.
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}

\IEEEPARstart{L}{ow-light} semantic segmentation is a crucial task in computer vision with significant applications in autonomous driving. State-of-the-art segmentation methods such as SegFormer \cite{xieSegFormerSimpleEfficient2021} and DeepLabV3+ \cite{chenEncoderDecoderAtrousSeparable2018} have shown impressive performance in well-lit conditions but often struggle in low-light environments due to challenges such as poor illumination, noise, and reduced contrast. These methods also face difficulties arising from domain shifts between day and night images, as well as the limited availability of annotated low-light datasets for training.

\textcolor{red}{I can't just write like the above without claims. But leave it as placeholder for a basic introduction}

The goal of semantic segmentation is to take a pixel from an image and classify it into one of several categories, such as a pedestrian or a building \cite{zhaoPyramidSceneParsing2017}.

Why do we care about low-light?
Low-light conditions are prevalent in real-world scenarios. Current semantic segmentation models such as SegFormer \cite{xieSegFormerSimpleEfficient2021}, DeepLabV3+ \cite{chenEncoderDecoderAtrousSeparable2018}, OneFormer \cite{jainOneFormerOneTransformer2022} have shown impressive performance in well-lit conditions but struggle in low-light environments as their evaluation datasets are not designed for testing low-light performance. As a result, these models often fail to generalise to low-light conditions, leading to poor segmentation results.



Introduction structure:
1. Short 2-3 introduction of the high-level problem area -> low-light semantic segmentation for AVs
2. What is semantic segmentation
3. Why do we care about low-light?
4. What are the existing works in low-light semantic segmentation?
5. What are the challenges with low-light semantic segmentation?
6. What motivates our solution to the challenge?
7. Briefly summarise our approach and contributions.
8. Paper outline.

Related works structure:
1. Talk about semantic segmentation works
2. Talk about low-light image enhancement
3. Talk about low-light semantic segmentation
4. Talk about closest works and how we differ. (informs, nightlab)

Methodology structure:
1. Photo of the architecture
2. Overview of model's method. The conceptual idea of hard regions and how we enhance them.
3. Model architecture details.
3a. LLIE module details
3b. Hard-region identification details
3c. Segmentation model details
4. Loss functions

Experiments structure:
1. Datasets
2. Evaluation metrics
3. Training details
4. Results discussions and quantitative and qualitative analysis against baseline
5. Ablation studies + analysis of the studies

Conclusion:
1. Conclusion

Appendix:
1. More implementation details
2. More experimental results

Why is this important?
- Particularly for autonomous driving, low-light conditions are common and challenging for perception systems. Due to financial constraints, it is not always possible to equip vehicles with high-end sensors that perform well in low-light. Therefore, improving low-light semantic segmentation using standard cameras is crucial for safety and reliability.
- Cameras serve a huge range of purposes on autonomous vehicles. They are used for object detection, lane detection, traffic sign recognition, and more. Semantic segmentation is an important task that massively assists the vehicle in understanding its surroundings and making informed safe decisions.

How do we address the problem?
- Existing works either focus on supervised or unsupervised methods. Unsupervised methods tend to underperform supervised ones due to the lack of labeled data. However, obtaining labeled data for low-light conditions is expensive and time-consuming.
- Thus in this paper, we investigate supervised methods as they tend to perform better.
- We propose a novel architecture with a novel training strategy to improve low-light semantic segmentation performance on standard benchmarks, such as NightCity, ACDC, and Dark Zurich.
- This architecture hones in on the concept of hard regions within images. We hypothesise that what differentiates the low-light and normal-light images are the hard regions. Hard regions are areas within images that are particularly challenging to segment, such as areas with poor illumination, high noise, or complex textures. By focusing on these hard regions during training, we can improve the model's ability to handle low-light conditions effectively. Presently, NightLab \cite{dengNightLabDualLevelArchitecture2022} and InforMS \cite{wangInformativeClassesMatter2023} have explored similar ideas, but NightLab focuses on running two separate branches for easy and hard parts of the image, whilst InforMS focuses on varying the frequency of classes during training based on IoU performance.
- Unlike existing works, we determine hard regions dynamically during training based on the performance of the model. We introduce a hard-region enhancement module that identifies and enhances these challenging areas, by applying




\subsection{Problem statement}

Formally define input and output (e.g., given image I, predict labels / masks / poses). Briefly state the evaluation goal and constraints.

\subsection{Key contributions}
Our contributions are as follows:

\begin{itemize}
    \item We propose a novel architecture that efficiently enhances hard regions in low-light images for improved semantic segmentation.
    \item We introduce a training strategy that prioritises learning from hard regions, leading to better model performance in low-light conditions.
    \item We conduct extensive experiments on standard benchmarks, demonstrating significant improvements over existing methods in low-light semantic segmentation.
    \item Code is open source at: xxxxxxxxx
\end{itemize}

\subsection{Paper outline}
Briefly describe the structure: Section~\ref{sec:related} surveys related work, Section~\ref{sec:method} describes the method, Section~\ref{sec:data} details datasets and metrics, Section~\ref{sec:exp} shows experiments and analysis, and Section~\ref{sec:conclusion} concludes.

\section{Related Work}
\label{sec:related}

\subsection{Semantic segmentation}
Current works in low-light semantic segmentation can be broadly categorised into explicit, implicit, and hybrid adaptation methods. Explicit adaptation methodologies directly alter the input image or isolate low-light correction as a preprocessing stage prior to segmentation. These approaches commonly employ techniques such as frequency-domain enhancement, Retinex decomposition or Histogram Equalization to improve image quality and mitigate low-light artefacts explicitly. \textcolor{red}{add citations here} Implicit adaptation methods learn illumination-invariant features through domain adaptation or architectural design, eliminating the need for explicit image enhancement. Unsupervised domain adaptation, adversarial training, and typical supervised training are commonly used in this category. Hybrid adaptation methods integrate explicit and implicit methodologies into a unified framework, where low-light image enhancement modules are jointly trained with segmentation networks to optimise performance on low-light images.

\textcolor{red}{Need to add the drawbacks of each type of method.}


Early approaches were dominated by explicit adaptation \cite{romeraBridgingDayNight2019, choSemanticSegmentationLow2020}, where these methods focused on extracting as much information from the image as possible without affecting the segmentation network.

\subsection{Closest works}
% Direct comparisons and distinguishing points.

\section{Method}
\label{sec:method}
\subsection{Overview}

The model proposed in this paper is a two-stage low-light image enhancement (LLIE) and semantic segmentation architecture that focuses on identifying and enhancing hard regions within low-light images to improve segmentation performance. Current methods follow the trend of either using a separate LLIE module followed by a segmentation model or training an end-to-end model that combines both tasks. However, these methods often overlook the importance of hard regions within low-light images that are particularly challenging to segment accurately. To address this gap, we introduce a hard-region enhancement module within the LLIE architecture that identifies and enhances these challenging areas, thereby improving the overall segmentation performance.

The key innovation behind our model lies in the hard-region segmentation module, which is trained to predict a hard region mask based on an error map generated from the downstream segmentation network. This mask guides the LLIE module to directly learn from what the segmentation network struggles with, thereby guiding the model to magnify features in these regions.

The LLIE module is designed as a multi-scale parallel attention enhancement module.

% Currently still in testing, but the model can be in two possible ways:
% 1. Train DLV3+ on night images until satisfied. Then in the second stage, use DLV3+'s output to train the hard region enhancement module to identify and enhance harder regions. DLV3+ is either frozen or not frozen during this stage.
% 2. Train DLV3+ and hard region enhancement module end-to-end from the start. The hard region enhancement module identifies hard regions based on DLV3+'s performance during training, and enhances them dynamically.
% 2a. This way is a bit sus because the hard region enhancement module may not have a good idea of what hard regions are at the start of training.

% The essence of the model is the LLIE module is trained to identify hard regions by simultaneously training a hard region segmentation head that predicts a hard region mask based on an error map generated from the downstream segmentation network. This guides the decoder to learn to extract hard regions from the image and this is used to guide the LLIE head to focus on enhancing these regions.


1. Figure of the architecture
2. Overview of model's method. The conceptual idea of hard regions and how we enhance them.
The basis of this method relies on the understanding that certain regions within low-light images are inherently more challenging to segment accurately. These hard regions can correspond with areas of low-contrast, high noise, or complex textures that are difficult to interpret under poor illumination. By identifying and guiding the model to pay attention to these hard regions during image enhancement, the overall segmentation performance can be significantly improved.

The main concept behind how this can be achieved is inspired by the dual-branch architecture of NightLab \cite{dengNightLabDualLevelArchitecture2022}, where two separate branches are used to process easy and hard regions of the image. However, instead of using two separate branches, our method integrates a hard-region enhancement module that identifies and enhances these challenging areas to produce an enhanced image for segmentation.

The LLIE module is designed as an encoder-decoder architecture that takes a low-light image as input and produces an enhanced image and a hard region mask as outputs. The hard region mask is a binary mask that indicates which regions the model considers hard to segment. This mask is generated by calculating an error map based on the prediction of the segmentation network against the ground truth. The error map is used to supervise the hard-region segmentation head of the LLIE module.

The feature maps from the decoders of the hard-region segmentation head are fused with the skip connection features for the enhancement head. This fusion guides the enhancement head to focus on enhancing the hard regions of the image. The enhanced image is finally passed to a segmentation model to produce the final segmentation map.

4. Loss functions
\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\textwidth]{Figures/Methodology/training.png}
    \caption{Training overview of the proposed method. The hard-region enhancement module identifies and enhances challenging areas in low-light images, which are then processed by the segmentation backbone to produce accurate semantic segmentation maps.}
    \label{fig:training_overview}
\end{figure*}

\subsection{Low-light image enhancement module}

\subsubsection{Encoder}
With regards to the encoder portion, we adopt a standard CNN architecture, such as EfficientNet \cite{tanEfficientNetRethinkingModel2020}. The encoder extracts multi-scale features from the low-light image which are then fed into the dual encoders.
\subsubsection{Enhancement head}
\subsubsection{Hard region segmentation head}

\subsection{Segmentation model}
% Describe backbone, heads, any novel blocks, with layer sizes if relevant.

\subsection{Loss functions}
% Primary loss, auxiliary terms, weighting, and rationale.
The primary loss used is the cross-entropy loss. The cross-entropy loss is given as:
\begin{equation}
    \mathcal{L}_{CE} = - \sum_{n=1}^{N} t_n \log(\hat{y}_n),
\end{equation}
where \(N\) is the number of classes, \(t_n\) is the ground truth label for class \(n\), and \(\hat{y}_n\) is the predicted probability for class \(n\).

To guide the binary segmentation head, a combination of the cross-entropy loss and focal loss is used. The focal loss helps to address class imbalance by focusing more on hard-to-classify examples, which is particularly useful for the hard region segmentation task.
The focal loss is defined as:
\begin{equation}
    \mathcal{L}_{Focal} = - \sum_{n=1}^{N} \alpha (1 - \hat{y}_n)^\gamma t_n \log(\hat{y}_n),
\end{equation}
where \(\alpha\) is a balancing factor and \(\gamma\) is the focusing parameter.

Finally, for the LLIE module, it receives supervision from the cross-entropy loss computed on the final segmentation output and a weighted combination of the Structural Similarity Index Measure or SSIM \cite {wangImageQualityAssessment2004} and image enhancement losses from Zero-DCE \cite{guoZeroReferenceDeepCurve2020,liLearningEnhanceLowLight2021}.

Following NightLab's \cite{dengNightLabDualLevelArchitecture2022} approach to training the enhancement module, the SSIM loss is utilised to guide the enhancement and penalise structural distortions. The SSIM loss is defined as:
\begin{equation}
    \mathcal{L}_{SSIM} = 1 - SSIM(I_{enhanced}, I_{original}),
\end{equation}
where \(I_{enhanced}\) is the enhanced image output from the LLIE module and \(I_{original}\) is the original image. SSIM is obtained through:
\begin{equation}
    SSIM(x, y) = \frac{(2\mu_x \mu_y + C_1)(2\sigma_{xy} + C_2)}{(\mu_x^2 + \mu_y^2 + C_1)(\sigma_x^2 + \sigma_y^2 + C_2)},
\end{equation}
where

\subsection{Optimization and training}
% Optimizer, learning-rate schedule, augmentations, stop criteria.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\textwidth]{Figures/Methodology/inference.png}
    \caption{Inference overview of the proposed method.}
    \label{fig:inference_overview}
\end{figure*}

\subsection{Implementation details}
% Framework, hardware, batch size, run-time considerations.

\section{Datasets and Evaluation Protocol}
\label{sec:data}
\subsection{Datasets}
% Names, splits, pre-processing, and statistics.

\subsection{Evaluation metrics}
% e.g., mAP, IoU, F1, PSNR, SSIM â€” define precisely.

\subsection{Baselines and protocol}
% How baselines are selected and tuned for fair comparison.

\section{Experiments}
\label{sec:exp}
\subsection{Main quantitative results}
% Table comparing to SOTA and baselines.

\subsection{Ablation studies}
% Component-wise ablations and hyperparameter sensitivity.

\subsection{Qualitative results}
% Representative visual examples and failure cases.

\subsection{Limitations}
% Honest discussion of known limitations.

\section{Analysis}
\subsection{Computational cost}
% Params, FLOPs, inference time.

\subsection{Robustness and generalization}
% Cross-dataset tests, perturbation studies.

\subsection{Interpretability}
% Visualizations (attention, feature maps), where applicable.

\section{Discussion}
Practical implications, deployment considerations, and potential ethical concerns.

\section{Conclusion}
\label{sec:conclusion}
Concise recap of contributions, main empirical findings, and future directions.


\appendices
\section{Additional implementation details}
I.e. hyperparameters, hardware, architecture diagram, training time.

\section{More Experimental Results}
Could spam photos and results here for quantitative and qualitative comparisons


% use section* for acknowledgment
\section*{Acknowledgment}
The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
    \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

\bibliography{IEEEabrv,./mybib.bib}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{IEEEbiography}{Michael Shell}
    Biography text here.
\end{IEEEbiography}

% if you will not have a photo at all:
\begin{IEEEbiographynophoto}{John Doe}
    Biography text here.
\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

\begin{IEEEbiographynophoto}{Jane Doe}
    Biography text here.
\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


